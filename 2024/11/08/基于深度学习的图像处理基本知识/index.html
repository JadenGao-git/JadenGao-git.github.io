<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 7.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">
  <link rel="stylesheet" href="/lib/pace/pace-theme-mac-osx.min.css">
  <script src="/lib/pace/pace.min.js"></script>

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"jadengao-git.github.io","root":"/","scheme":"Mist","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":true,"show_result":false,"style":"mac"},"back2top":{"enable":true,"sidebar":false,"scrollpercent":true},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},"path":"search.xml"};
  </script>

  <meta name="description" content="参考： 《【深度学习】一文搞懂卷积神经网络（CNN）的原理（超详细）》：https:&#x2F;&#x2F;blog.csdn.net&#x2F;AI_dataloads&#x2F;article&#x2F;details&#x2F;133250229">
<meta property="og:type" content="article">
<meta property="og:title" content="基于深度学习的图像处理基本知识">
<meta property="og:url" content="https://jadengao-git.github.io/2024/11/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/index.html">
<meta property="og:site_name" content="米兰小铁匠的博客">
<meta property="og:description" content="参考： 《【深度学习】一文搞懂卷积神经网络（CNN）的原理（超详细）》：https:&#x2F;&#x2F;blog.csdn.net&#x2F;AI_dataloads&#x2F;article&#x2F;details&#x2F;133250229">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E6%88%90-9490606.png">
<meta property="og:image" content="https://jadengao-git.github.io/2024/11/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241021140903850.png">
<meta property="og:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/6baad2f695bc40de2d64b3c212c5a514.png">
<meta property="og:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/764cb2e00594c5b6b2597aec07d02ef3.png">
<meta property="og:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/c33f2f3bbe91dca974d5d1f43f91cfd4.png">
<meta property="og:image" content="https://jadengao-git.github.io/2024/11/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/e06b2e09f4f2221a805e8dd373c1cfe1.jpeg">
<meta property="og:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241024154152036-9755718.png">
<meta property="og:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241024154451934-9755894.png">
<meta property="article:published_time" content="2024-11-08T01:24:08.000Z">
<meta property="article:modified_time" content="2024-11-08T01:28:36.548Z">
<meta property="article:author" content="米兰小铁匠">
<meta property="article:tag" content="图像质量评价">
<meta property="article:tag" content="图像目标检测">
<meta property="article:tag" content="CNN">
<meta property="article:tag" content="YOLO">
<meta property="article:tag" content="GAN">
<meta property="article:tag" content="Transformer">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://jadengao-git.github.io/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E6%88%90-9490606.png">

<link rel="canonical" href="https://jadengao-git.github.io/2024/11/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'zh-CN'
  };
</script>

  <title>基于深度学习的图像处理基本知识 | 米兰小铁匠的博客</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">米兰小铁匠的博客</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
        <i class="fa fa-search fa-fw fa-lg"></i>
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-about">

    <a href="/about/" rel="section"><i class="fa fa-user fa-fw"></i>关于</a>

  </li>
      <li class="menu-item menu-item-search">
        <a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索
        </a>
      </li>
  </ul>
</nav>



  <div class="search-pop-overlay">
    <div class="popup search-popup">
        <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocapitalize="off"
           placeholder="搜索..." spellcheck="false"
           type="search" class="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result">
  <div id="no-result">
    <i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i>
  </div>
</div>

    </div>
  </div>

</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="https://jadengao-git.github.io/2024/11/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.jpg">
      <meta itemprop="name" content="米兰小铁匠">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="米兰小铁匠的博客">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          基于深度学习的图像处理基本知识
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>
              

              <time title="创建时间：2024-11-08 09:24:08 / 修改时间：09:28:36" itemprop="dateCreated datePublished" datetime="2024-11-08T09:24:08+08:00">2024-11-08</time>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <p>参考：</p>
<p><strong>《【深度学习】一文搞懂卷积神经网络（CNN）的原理（超详细）》</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/AI_dataloads/article/details/133250229">https://blog.csdn.net/AI_dataloads/article/details/133250229</a></p>
<span id="more"></span>

<h1 id="一、须知"><a href="#一、须知" class="headerlink" title="一、须知"></a>一、须知</h1><h2 id="1-深度卷积神经网络"><a href="#1-深度卷积神经网络" class="headerlink" title="1. 深度卷积神经网络"></a>1. 深度卷积神经网络</h2><p>深度卷积神经网络（Deep Convolutional Neural Networks，DCNN）是一种深度学习模型，特别适用于图像处理和计算机视觉任务。它是卷积神经网络（Convolutional Neural Networks，CNN）的一个扩展，包含多层卷积和其他操作。</p>
<h3 id="（1）什么是卷积"><a href="#（1）什么是卷积" class="headerlink" title="（1）什么是卷积"></a>（1）什么是卷积</h3><p>在卷积神经网络中，卷积操作是指将一个可移动的小窗口（称为数据窗口）与图像进行逐元素相乘然后相加的操作。这个小窗口其实是一组固定的权重，它可以被看作是一个特定的滤波器（filter）或卷积核。这个操作的名称“卷积”，源自于这种元素级相乘和求和的过程。这一操作是卷积神经网络名字的来源。</p>
<blockquote>
<p><strong>卷积核</strong>：数据窗口中的权重集合称为卷积核（或滤波器），它是网络在训练过程中学习得到的。不同的卷积核可以提取不同类型的特征，比如边缘、纹理等。</p>
<p><strong>特征提取</strong>：通过多个卷积层，CNN可以从简单的特征（如边缘）逐步提取出更复杂的特征（如形状、对象等），这使得CNN在图像处理、计算机视觉等领域表现出色。</p>
</blockquote>
<h3 id="（2）卷积核心问题"><a href="#（2）卷积核心问题" class="headerlink" title="（2）卷积核心问题"></a>（2）卷积核心问题</h3><ol>
<li><strong>步长（stride）</strong>：每次滑动的位置步长。</li>
<li><strong>卷积核的个数</strong>：决定输出的depth厚度。同时代表卷积核的个数。</li>
<li><strong>填充值（zero-padding）</strong>：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。</li>
</ol>
<h3 id="（3）卷积神经网络模型"><a href="#（3）卷积神经网络模型" class="headerlink" title="（3）卷积神经网络模型"></a>（3）卷积神经网络模型</h3><p>卷积神经网络（CNN）通常由多个层组成，每一层负责不同的功能。一个典型的CNN模型结构如下：</p>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E6%88%90-9490606.png" alt="卷积神经网络的构成-9490606"></p>
<h4 id="1-输入层"><a href="#1-输入层" class="headerlink" title="1. 输入层"></a>1. 输入层</h4><p>接收原始输入数据，图像通常由三个颜色通道（红、绿、蓝）组成，形成一个二维矩阵，表示像素的强度值。输入层的形状取决于图像的大小和通道数（例如，224x224x3的图像表示宽224像素，高224像素，3个颜色通道）。</p>
<img src="./基于深度学习的图像处理基本知识.assets/image-20241021140903850.png" alt="image-20241021140903850" style="zoom:50%;" />

<h4 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2. 卷积层"></a>2. 卷积层</h4><p>提取输入图像中的特征。每个卷积层都有多个卷积核（滤波器），这些滤波器在输入上滑动，执行卷积操作。生成特征图（feature maps），每个特征图对应一个卷积核。</p>
<h4 id="3-激活层"><a href="#3-激活层" class="headerlink" title="3. 激活层"></a>3. 激活层</h4><p>通常使用非线性激活函数（如ReLU）应用于卷积层的输出，以引入非线性特性，使网络能够学习更复杂的模式。</p>
<blockquote>
<p><strong>激活函数</strong>是神经网络中的一个重要组成部分，其主要作用是在每一层引入非线性特性，使得网络能够学习和拟合复杂的模式和数据。没有激活函数，神经网络的每一层都只是进行线性变换，无法捕捉到数据中的非线性关系。</p>
<p><strong>常见的激活函数</strong></p>
<p><strong>Sigmoid 函数</strong>：$σ(x)&#x3D; \frac{1}{1 + e^{-x}}$，输出值在 (0, 1) 之间</p>
<ul>
<li><strong>优点</strong>：平滑且可微，适合于二分类任务。</li>
<li><strong>缺点</strong>：在输入较大或较小的时候，梯度会变得非常小，导致梯度消失问题。</li>
</ul>
<p><strong>Tanh 函数（双曲正切函数）</strong>：$tanh(x)&#x3D;\frac{e^{x}-e^{−x}}{e^{x}+e^{−x}}$​，输出值在 (-1, 1) 之间</p>
<ul>
<li><strong>优点</strong>：输出范围更广，相较于Sigmoid具有更好的性能。</li>
<li><strong>缺点</strong>：同样存在梯度消失问题，特别是在深层网络中。</li>
</ul>
<p><strong>ReLU 函数（修正线性单元）</strong>：$ReLU(x)&#x3D;max(0,x)$​，输出值在 [0, ∞) 之间</p>
<ul>
<li><strong>优点</strong>：计算简单且高效，在正区间内有较大的梯度，减少了梯度消失问题，有助于加快收敛速度。</li>
<li><strong>缺点</strong>：在负区间内，输出始终为0，可能导致“神经元死亡”问题，即某些神经元在训练过程中变得永远不激活。</li>
</ul>
<p><strong>Leaky ReLU 函数</strong><br>$$<br>Leaky\ ReLU(x)&#x3D; \begin{cases}<br>x &amp; if\ x&gt;0 \<br>ax &amp; if\ x \leq 0<br>\end{cases}\ （通常取𝛼&#x3D;0.01）<br>$$</p>
<ul>
<li><strong>优点</strong>：在负区间也有一个小的斜率，减少了“神经元死亡”问题。</li>
</ul>
<p><strong>Softmax 函数</strong>，$Softmax(xi)&#x3D;\frac{e^{x_i}}{\sum_{j} e^{x_j}}$​，将输出值转化为概率分布，常用于多分类任务的输出层。</p>
<ul>
<li><strong>优点</strong>：保证输出值在 (0, 1) 之间且和为1，适合于分类任务。</li>
</ul>
</blockquote>
<h4 id="4-池化层（下采样层）"><a href="#4-池化层（下采样层）" class="headerlink" title="4. 池化层（下采样层）"></a>4. 池化层（下采样层）</h4><p>减少特征图的尺寸，从而减小计算量并降低过拟合的风险。常用的池化方法有最大池化（max pooling）和平均池化（average pooling）。池化层的输出是更小的特征图。</p>
<blockquote>
<p><strong>过拟合（Overfitting）</strong>是机器学习和深度学习中的一个常见问题，指的是模型在训练数据上表现很好，但在未见过的测试数据或验证数据上表现较差的现象。过拟合发生在模型学习到了训练数据中的噪声和细节，而不仅仅是数据中的潜在模式。这使得模型在新数据上的泛化能力下降。模型复杂度过高，训练数据量不足，特征过多都会导致过拟合。</p>
<p><strong>最大池化（Max Pooling）</strong> 是一种下采样操作，常用于卷积神经网络（CNN）中，以减少特征图的尺寸和计算量，同时保留重要的特征信息。在最大池化中，特征图被划分为若干个小的区域（通常是 2×2 或 3×3 的窗口），然后对每个区域内的像素值取最大值。这一过程可以理解为在每个窗口中选择最显著的特征，其他信息则被丢弃</p>
</blockquote>
<h4 id="5-全连接层"><a href="#5-全连接层" class="headerlink" title="5. 全连接层"></a>5. 全连接层</h4><p>将前面层提取到的特征进行整合，进行分类或回归。全连接层将所有输入连接到输出，通常在网络的最后部分。输出通常是一个概率分布（使用softmax激活）或连续值（用于回归任务）。</p>
<h4 id="6-输出层"><a href="#6-输出层" class="headerlink" title="6. 输出层"></a>6. 输出层</h4><p>输出最终的预测结果，比如分类标签或回归值。</p>
<h1 id="二、图像目标检测"><a href="#二、图像目标检测" class="headerlink" title="二、图像目标检测"></a>二、图像目标检测</h1><p>参考：</p>
<p><strong>《目标检测（Object Detection）》</strong>：<a target="_blank" rel="noopener" href="https://blog.csdn.net/yegeli/article/details/109861867">https://blog.csdn.net/yegeli/article/details/109861867</a></p>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。</p>
<h3 id="1-核心问题"><a href="#1-核心问题" class="headerlink" title="1. 核心问题"></a>1. 核心问题</h3><ol>
<li>分类问题：即图片（或某个区域）中的图像属于哪个类别。</li>
<li>定位问题：目标可能出现在图像的任何位置。</li>
<li>大小问题：目标有各种不同的大小。</li>
<li>形状问题：目标可能有各种不同的形状。</li>
</ol>
<h3 id="2-目标检测算法分类"><a href="#2-目标检测算法分类" class="headerlink" title="2. 目标检测算法分类"></a>2. 目标检测算法分类</h3><p>基于深度学习的目标检测算法主要分为两类：Two stage, One stage。</p>
<h4 id="Tow-Stage"><a href="#Tow-Stage" class="headerlink" title="Tow Stage"></a>Tow Stage</h4><p>先进行区域生成，该区域称之为region proposal（简称RP，一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。</p>
<p>任务流程：特征提取 –&gt; 生成RP –&gt; 分类&#x2F;定位回归。</p>
<p>常见tow stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。</p>
<h4 id="One-Stage"><a href="#One-Stage" class="headerlink" title="One Stage"></a>One Stage</h4><p>不用RP，直接在网络中提取特征来预测物体分类和位置。</p>
<p>任务流程：特征提取–&gt; 分类&#x2F;定位回归。</p>
<p>常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD和RetinaNet等。</p>
<h2 id="2-目标检测原理"><a href="#2-目标检测原理" class="headerlink" title="2. 目标检测原理"></a>2. 目标检测原理</h2><h3 id="1-候选区域产生"><a href="#1-候选区域产生" class="headerlink" title="1. 候选区域产生"></a>1. 候选区域产生</h3><p>很多目标检测技术都会涉及候选框（bounding boxes）的生成，物体候选框获取当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。目标识别与图像分割技术的发展进一步推动有效提取图像中信息。</p>
<h4 id="（1）滑动窗口"><a href="#（1）滑动窗口" class="headerlink" title="（1）滑动窗口"></a>（1）滑动窗口</h4><p>首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选。最终，经过NMS筛选后获得检测到的物体。</p>
<h4 id="（2）选择性选择"><a href="#（2）选择性选择" class="headerlink" title="（2）选择性选择"></a>（2）选择性选择</h4><p>选择搜索算法的主要思想：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于上面这一想法采用子区域合并的方法进行提取bounding boxes。首先，对输入图像进行分割算法产生许多小的子区域。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</p>
<h3 id="2-数据表示"><a href="#2-数据表示" class="headerlink" title="2. 数据表示"></a>2. 数据表示</h3><h5 id="输入图像"><a href="#输入图像" class="headerlink" title="输入图像"></a><strong>输入图像</strong></h5><ul>
<li><strong>图像格式</strong>：输入通常是RGB或灰度图像，可能具有不同的分辨率和尺寸。</li>
<li><strong>预处理</strong>：在将图像输入到模型之前，通常需要进行预处理，例如缩放、归一化、数据增强等。</li>
</ul>
<h5 id="边界框（Bounding-Box）"><a href="#边界框（Bounding-Box）" class="headerlink" title="边界框（Bounding Box）"></a><strong>边界框（Bounding Box）</strong></h5><p>边界框用于表示图像中目标的位置，通常用一个矩形框表示。每个边界框通常由以下信息表示：</p>
<ul>
<li>坐标：<ul>
<li>中心坐标 (x,y)(x, y)(x,y)：框中心的横纵坐标。</li>
<li>宽度（width）和高度（height）：边界框的尺寸。</li>
<li>左上角坐标 (xmin,ymin)(x_{min}, y_{min})(xmin,ymin) 和右下角坐标 (xmax,ymax)(x_{max}, y_{max})(xmax,ymax)：表示框的四个边界。</li>
</ul>
</li>
<li><strong>置信度（Confidence）</strong>：边界框的置信度表示该框中存在目标的概率，通常是一个介于0和1之间的值。</li>
</ul>
<h5 id="类别标签（Class-Label）"><a href="#类别标签（Class-Label）" class="headerlink" title="类别标签（Class Label）"></a><strong>类别标签（Class Label）</strong></h5><p>每个检测到的目标都有一个对应的类别标签，用于标识目标的类型。例如，常见的类别标签包括“人”、“车”、“动物”等。</p>
<h5 id="网格（Grid）"><a href="#网格（Grid）" class="headerlink" title="网格（Grid）"></a><strong>网格（Grid）</strong></h5><p>在一些目标检测算法（如YOLO）中，输入图像被划分为一个SxS的网格。每个网格负责预测它所包含的目标的边界框和类别概率。这种方式能够将整个图像的处理并行化，提高检测速度。</p>
<h5 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a><strong>输出格式</strong></h5><p>目标检测模型的输出通常是一个集合，包含多个边界框及其对应的类别标签和置信度。常见的输出格式包括：</p>
<ul>
<li><strong>边界框集合</strong>：每个边界框由坐标、置信度和类别标签组成。</li>
<li><strong>非极大抑制（Non-Maximum Suppression, NMS）</strong>：在后处理阶段，NMS用于去除重复检测（即多个边界框检测到同一目标），保留最佳的边界框。</li>
</ul>
<h5 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a><strong>数据集格式</strong></h5><p>训练目标检测模型的数据集通常包含图像文件及其对应的标注文件。常见的标注格式包括：</p>
<ul>
<li><strong>Pascal VOC</strong>：使用XML格式，包含图像的边界框坐标和类别信息。</li>
<li><strong>COCO</strong>：使用JSON格式，包含丰富的目标信息，包括边界框、类别、分割掩码等。</li>
<li><strong>YOLO格式</strong>：每个图像有一个对应的文本文件，列出每个目标的类别和边界框坐标（相对于图像尺寸的比例）。</li>
</ul>
<h3 id="3-效果评估"><a href="#3-效果评估" class="headerlink" title="3. 效果评估"></a>3. 效果评估</h3><p>使用<strong>交并比（Intersection over Union, IoU）</strong>来判断模型的好坏。所谓交并比，是指预测边框、实际边框交集和并集的比率，一般约定0.5为一个可以接收的值</p>
<blockquote>
<p>一些名词解释：</p>
<p><strong>特征图（Feature Map）</strong> 是卷积神经网络（CNN）中的重要概念，表示网络在处理图像时提取的特征信息。具体而言，输入图像经过一层或多层卷积和池化操作后，生成的矩阵就是特征图。特征图包含了输入图像的空间信息（如边缘、纹理、物体轮廓等）和语义信息（如某类物体的特征），它浓缩了原始图像的信息并且具有更小的尺寸。特征图的每个通道反映不同的特征，这些特征有助于后续的分类、检测或分割任务。</p>
<p><strong>双线性插值（Bilinear Interpolation）</strong> 是一种在二维空间中进行数据插值的算法，用于在已知点之间估算未知点的值。这种方法通常用于图像处理、图像缩放和图像变形等场景。</p>
<p><strong>非极大值抑制（Non-Maximum Suppression, NMS）</strong> 是一种后处理技术，通常用于目标检测任务中，以减少重叠的候选框（bounding boxes），从而保留最优的检测结果。NMS 的主要目的是通过选择最具代表性的边界框，来提高检测的精确性和清晰度。</p>
</blockquote>
<h2 id="3-目标检测模型"><a href="#3-目标检测模型" class="headerlink" title="3. 目标检测模型"></a>3. 目标检测模型</h2><h3 id="1-R-CNN系列"><a href="#1-R-CNN系列" class="headerlink" title="1. R-CNN系列"></a>1. R-CNN系列</h3><h4 id="（1）R-CNN"><a href="#（1）R-CNN" class="headerlink" title="（1）R-CNN"></a>（1）R-CNN</h4><p><strong>R-CNN（Regions with Convolutional Neural Networks）</strong>是目标检测领域的重要算法，最早由Ross Girshick等人在2013年提出。R-CNN的核心思想是将目标检测问题分解为两个部分：</p>
<ol>
<li><strong>候选区域生成</strong>：使用“选择性搜索”方法从输入图像中生成约2000个候选区域（region proposals），这些区域可能包含目标。</li>
<li><strong>特征提取与分类</strong>：对于每个候选区域，应用卷积神经网络（CNN）提取特征，随后使用支持向量机（SVM）进行目标分类。</li>
</ol>
<h5 id="详细步骤："><a href="#详细步骤：" class="headerlink" title="详细步骤："></a>详细步骤：</h5><ol>
<li><strong>候选区域生成</strong>：R-CNN使用选择性搜索算法对输入图像进行分割，生成一系列潜在的候选框，每个框表示一个可能包含物体的区域。选择性搜索通过利用图像的颜色、纹理、大小等信息生成这些区域。这些区域可能会包含冗余，数量通常为数千个，但可以减少处理时间相对于处理整个图像。</li>
<li><strong>特征提取</strong>：R-CNN使用卷积神经网络（CNN）来处理每个候选区域。为了匹配CNN输入的固定大小，R-CNN首先将每个候选区域进行缩放（warp）至固定尺寸（例如224×224），然后将这些缩放后的区域输入到一个经过预训练的CNN中（如AlexNet）提取特征。</li>
<li><strong>分类</strong>：对于每个候选区域提取出的特征，R-CNN使用支持向量机（SVM）进行分类，判断区域是否属于某个特定的类别。同时还会通过边界框回归（bounding box regression）来微调候选框的位置，使其与实际物体更匹配。</li>
</ol>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间</li>
<li>selective search方法生成region proposal，对一帧图像，需要花费2秒</li>
<li>三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大</li>
</ul>
<h4 id="（2）Fast-R-CNN"><a href="#（2）Fast-R-CNN" class="headerlink" title="（2）Fast R-CNN"></a>（2）Fast R-CNN</h4><p><strong>Fast R-CNN</strong> 是对 R-CNN 的改进，旨在解决 R-CNN 计算效率低的问题。其主要优化点包括：</p>
<ul>
<li><strong>共享特征提取</strong>：输入图像只通过一次 CNN 处理，生成特征图，而不是对每个候选区域重复计算。随后在这些特征图上进行区域处理。</li>
<li><strong>ROI Pooling</strong>：从特征图中提取候选区域，并通过 ROI Pooling 层将候选区域统一为固定大小，方便送入全连接层进行分类和回归。</li>
<li><strong>分类与回归同时进行</strong>：Fast R-CNN 使用 Softmax 层进行目标分类，并在同一个网络中执行边界框回归，大大提高了检测速度和精度。</li>
</ul>
<h5 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h5><ol>
<li><p><strong>特征提取</strong>：输入图像通过一个深度卷积神经网络（如 VGG16）进行特征提取，生成一个特征图。</p>
</li>
<li><p><strong>候选区域生成</strong>：使用选择性搜索（Selective Search）生成约 2000 个候选区域（ROI）。</p>
</li>
<li><p><strong>ROI Pooling</strong>：对于每个候选区域，通过 ROI Pooling 操作从特征图中提取对应的特征。将每个 ROI 的特征映射到固定的大小（如 7x7）。</p>
</li>
<li><p><strong>全连接层</strong>：将 ROI Pooling 的输出通过两个全连接层，得到每个候选区域的特征表示。</p>
</li>
<li><p><strong>分类和边界框回归</strong>：在全连接层的输出上，分别进行物体分类（使用 softmax 函数）和边界框回归（用于优化每个候选框的位置）。</p>
</li>
<li><p><strong>非极大值抑制（NMS）</strong>：对候选框进行 NMS 处理，去除重叠度高的候选框，保留每个类别中得分最高的框。</p>
</li>
<li><p><strong>输出结果</strong>：最终输出每个物体的类别及其对应的边界框和回归修正。</p>
</li>
</ol>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒）</li>
<li>无法满足实时应用，没有真正实现端到端训练测试</li>
<li>利用了GPU，但是region proposal方法是在CPU上实现的</li>
</ul>
<h4 id="（3）Faster-R-CNN"><a href="#（3）Faster-R-CNN" class="headerlink" title="（3）Faster R-CNN"></a>（3）Faster R-CNN</h4><p><strong>Faster R-CNN</strong> 进一步优化了 Fast R-CNN，消除了对选择性搜索的依赖，提出了区域提议网络（RPN）直接从 CNN 的特征图中生成候选区域。</p>
<ul>
<li><strong>RPN（Region Proposal Network）</strong>：与 Fast R-CNN 不同，Faster R-CNN 使用 RPN 代替选择性搜索，直接从 CNN 的特征图中生成候选区域。RPN 网络通过滑动窗口在特征图上生成候选框，并预测每个框是否包含目标。</li>
<li><strong>共享特征图</strong>：RPN 和 Fast R-CNN 共享 CNN 提取的特征图，这使得检测速度更快。</li>
</ul>
<blockquote>
<p><strong>RPN（Region Proposal Network）</strong> 是一种用于目标检测的神经网络模块，最初由 <strong>Faster R-CNN</strong> 提出。RPN 的主要目的是从输入的特征图中生成候选区域（Region Proposals），这些候选区域将用于后续的目标分类和边界框回归。</p>
<h3 id="RPN-的工作原理："><a href="#RPN-的工作原理：" class="headerlink" title="RPN 的工作原理："></a>RPN 的工作原理：</h3><ol>
<li><strong>特征图输入</strong>：<ul>
<li>RPN 接收从卷积神经网络（CNN）提取的特征图，通常是经过一系列卷积和池化操作后的输出。</li>
</ul>
</li>
<li><strong>滑动窗口</strong>：<ul>
<li>RPN 在特征图上以滑动窗口的方式进行处理。通常使用一个小的窗口（例如 3×3），在特征图上逐步滑动，并对每个位置生成多个候选区域。</li>
</ul>
</li>
<li><strong>Anchor Boxes</strong>：<ul>
<li>对于每个滑动窗口位置，RPN 会生成多个称为 <strong>anchor boxes</strong> 的候选框，anchor boxes 是预定义的固定尺寸和长宽比的矩形框。这些框的位置会围绕滑动窗口的中心进行调整，以便覆盖特征图中可能存在的目标。</li>
</ul>
</li>
<li><strong>分类与回归</strong>：<ul>
<li>RPN 使用一个小的神经网络（通常由卷积层和全连接层构成）对每个 anchor box 进行分类（是否包含目标）和边界框回归（调整框的位置和大小）。具体来说：<ul>
<li><strong>分类任务</strong>：输出每个 anchor box 是否包含目标的概率（前景&#x2F;背景）。</li>
<li><strong>回归任务</strong>：输出每个 anchor box 的坐标调整值，以优化目标的边界框。</li>
</ul>
</li>
</ul>
</li>
<li><strong>非极大值抑制（NMS）</strong>：<ul>
<li>通过 NMS，对生成的候选区域进行处理，去除重叠度过高的框，最终保留最优的候选区域，传递给后续的目标检测网络（如 Fast R-CNN）。</li>
</ul>
</li>
</ol>
</blockquote>
<h4 id="（4）Mask-R-CNN"><a href="#（4）Mask-R-CNN" class="headerlink" title="（4）Mask R-CNN"></a>（4）Mask R-CNN</h4><p><strong>Mask R-CNN</strong> 是 Faster R-CNN 的扩展版本，加入了实例分割功能。除了检测物体和回归边界框外，Mask R-CNN 还能预测每个目标的像素级掩码，实现精细的目标分割。</p>
<ul>
<li><strong>实例分割</strong>：通过在 Faster R-CNN 的基础上增加一个并行的分支，用于预测每个候选区域内的像素掩码。这样不仅可以进行目标检测，还可以实现像素级分割。</li>
<li><strong>ROI Align</strong>：为了解决 ROI Pooling 中的量化误差，Mask R-CNN 引入了 ROI Align，将候选区域精确对齐到特征图上，提高了掩码预测的精度。</li>
</ul>
<h3 id="2-YOLO系列"><a href="#2-YOLO系列" class="headerlink" title="2. YOLO系列"></a>2. YOLO系列</h3><h4 id="（1）YOLOv1"><a href="#（1）YOLOv1" class="headerlink" title="（1）YOLOv1"></a>（1）YOLOv1</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><p>将目标检测问题转化为一个回归问题，将候选区和检测两个阶段合二为一，从而实现实时检测。实际上，YOLO并没有真正去掉候选区，而是采用了预定义候选区的方法，也就是将图片划分为7<em>7个网格，每个网格允许预测出2个边框，总共49</em>2个bounding box，可以理解为98个候选区域，它们很粗略地覆盖了图片的整个区域。YOLO以降低mAP为代价，大幅提升了时间效率。</p>
<p>YOLOv1的<strong>损失函数</strong>主要由以下几个部分组成：</p>
<ul>
<li>边界框位置的损失（位置误差）</li>
<li>边界框大小的损失</li>
<li>置信度损失（边界框是否包含目标）</li>
<li>类别概率损失</li>
</ul>
<h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><ul>
<li><p><strong>输入层</strong>：输入尺寸为448x448的RGB图像。</p>
</li>
<li><p><strong>卷积层</strong>：使用了一系列的卷积层来提取图像特征。YOLOv1共使用了24个卷积层和2个全连接层。</p>
<ul>
<li>第一层为7x7的卷积，步幅为2，使用了64个滤波器。</li>
<li>随后是多个3x3的卷积层（例如64、128、256等），并配有批量归一化和激活函数（如Leaky ReLU）</li>
</ul>
</li>
<li><p><strong>池化层</strong>：使用最大池化层进行下采样。每经过几层卷积后，通常会有一个池化层，降低特征图的空间维度，从而保留主要特征并减小计算量。</p>
</li>
<li><p><strong>第一个全连接层</strong>：将卷积层的输出展平，并映射到一个较小的维度。</p>
</li>
<li><p><strong>第二个全连接层</strong>：最终输出一个大小为$S\times S\times (B\times 5+C)$的张量，其中：</p>
<ul>
<li>$S\times S$ 是网格的数量。</li>
<li>$B$ 是每个网格预测的边界框数量。</li>
<li>每个边界框包含5个信息（中心坐标、宽度、高度、置信度）。</li>
<li>$C$ 是目标类别的数量，每个网格还需要预测每个类别的概率。</li>
</ul>
</li>
</ul>
<h5 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h5><ol>
<li><strong>数据准备</strong></li>
</ol>
<p>在训练YOLOv1之前，需要准备好数据集，包括：</p>
<ul>
<li><strong>图像数据</strong>：输入的图像，通常是高分辨率的RGB图像。</li>
<li><strong>标注数据</strong>：与每幅图像对应的标注信息，通常包含目标的类别和边界框（bounding box）信息。标注格式可能使用Pascal VOC或YOLO格式。</li>
</ul>
<ol start="2">
<li><strong>数据预处理</strong></li>
</ol>
<p>在训练之前，需要对图像和标注进行预处理，包括：</p>
<ul>
<li><strong>图像缩放</strong>：将图像缩放到448x448的尺寸，符合YOLOv1的输入要求。</li>
<li><strong>归一化</strong>：将像素值归一化到0到1之间，以便于模型训练。</li>
</ul>
<blockquote>
<p><strong>归一化（Normalization）</strong>是将数据转换到一个特定的范围或分布，使其适合于后续的模型训练。归一化的主要目的是提高模型的训练效率和预测精度，尤其是在涉及到多维数据的情况下。</p>
<p>$X^′&#x3D;\frac{X}{255}$</p>
</blockquote>
<ul>
<li><strong>数据增强</strong>：通过随机裁剪、旋转、翻转等方法增加训练样本的多样性，提高模型的泛化能力。</li>
</ul>
<blockquote>
<p>采用相对坐标，边界框x 和y 坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间</p>
</blockquote>
<ol start="3">
<li><strong>网络结构</strong></li>
</ol>
<p>YOLOv1的网络结构由卷积层、池化层和全连接层组成。训练时通过前向传播将图像输入网络，生成预测输出，包括边界框位置、大小、置信度和类别概率。</p>
<ol start="4">
<li><strong>损失函数</strong></li>
</ol>
<p>YOLOv1的损失函数是一个关键组成部分，用于评估模型的预测与真实值之间的差距。损失函数通常包括以下几部分：</p>
<ul>
<li><strong>边界框损失</strong>：用于衡量预测的边界框与真实框之间的位置和大小误差。包括中心坐标的平方误差和宽度、高度的平方误差。</li>
<li><strong>置信度损失</strong>：用于衡量预测的边界框是否包含目标。通过计算预测置信度与真实值之间的均方误差。</li>
<li><strong>类别损失</strong>：通过交叉熵损失衡量预测类别概率与真实类别之间的差距。</li>
</ul>
<p>完整的损失函数可以表示为：</p>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/6baad2f695bc40de2d64b3c212c5a514.png" alt="6baad2f695bc40de2d64b3c212c5a514"></p>
<ul>
<li>损失函数由坐标预测、是否包含目标物体置信度、类别预测构成；</li>
<li>其中$1_{i}^{obj}$表示目标是否出现在网格单元i中，表示$1_{ij}^{obj}$​​网格单元i中的第j个边界框预测器“负责”该预测；</li>
<li>如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误；</li>
<li>如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。</li>
</ul>
<ol start="5">
<li><strong>训练过程</strong></li>
</ol>
<ul>
<li><strong>前向传播</strong>：输入图像经过网络，得到预测的边界框和类别概率。</li>
<li><strong>计算损失</strong>：根据预测结果和真实标注计算损失函数的值。</li>
<li><strong>反向传播</strong>：使用反向传播算法计算梯度，更新网络权重。YOLOv1通常使用随机梯度下降（SGD）或Adam优化器进行权重更新。</li>
<li><strong>迭代训练</strong>：重复进行多个epoch，直到损失收敛或达到预设的训练轮数。</li>
</ul>
<p><strong>避免过拟合</strong>的策略：使用dropout和数据增强来避免过拟合。</p>
<blockquote>
<p><strong>Dropout</strong>的核心思想是在训练时以一定的概率随机“关闭”部分神经元，这样在每个前向传播过程中，网络的结构都会有所不同。具体而言：</p>
<ul>
<li>对于给定的一层神经网络，每个神经元以一个预设的概率 ppp （通常为0.5）被“丢弃”，即暂时不参与前向传播和反向传播。</li>
<li>在训练完成后进行推理时，Dropout不再启用，而是使用所有神经元，并将它们的输出值按比例缩放（通常乘以 ppp），以保持推理时的输出期望值与训练时一致。</li>
</ul>
<p><strong>数据增强（Data Augmentation）</strong> 是通过对原始数据进行变换（如旋转、缩放、翻转等）来生成新的训练样本，从而增加训练数据量的方法。</p>
</blockquote>
<ol start="6">
<li><strong>超参数调整</strong></li>
</ol>
<p>训练YOLOv1时，选择合适的超参数（如学习率、批量大小、权重衰减等）非常重要。这些超参数会影响模型的收敛速度和最终性能。</p>
<p><strong>学习率</strong>：第一个迭代周期，慢慢地将学习率从$10^{-3}$提高到$10^{-2}$；然后继续以$10^{-2}$的学习率训练75个迭代周期，用$10^{-3}$的学习率训练30个迭代周期，最后用$10^{-4}$​的学习率训练30个迭代周期。</p>
<ol start="7">
<li><strong>后处理</strong></li>
</ol>
<p>训练完成后，YOLOv1的输出需要进行后处理，以获得最终的检测结果。这包括：</p>
<ul>
<li><strong>非极大抑制（Non-Maximum Suppression, NMS）</strong>：去除重复的边界框，保留置信度最高的边界框。NMS通过设定一个阈值，过滤掉重叠度过高的框。</li>
<li><strong>输出结果</strong>：最终的输出包括每个检测到的对象的类别、边界框坐标和置信度。</li>
</ul>
<ol start="8">
<li><strong>模型评估</strong></li>
</ol>
<p>训练完成后，需要对模型进行评估，通常使用mAP（mean Average Precision）等指标来衡量模型在验证集上的表现，确保模型的准确性和鲁棒性。</p>
<h5 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h5><p><strong>优点</strong></p>
<ul>
<li>YOLO检测物体速度非常快，其增强版GPU中能跑45fps（frame per second），简化版155fps</li>
<li>YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息</li>
<li>YOLO可以学到物体泛化特征</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>精度低于其它state-of-the-art的物体检测系统</li>
<li>容易产生定位错误</li>
<li>对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体</li>
<li>由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体处理上还有待加强</li>
</ul>
<h4 id="（2）YOLOv2"><a href="#（2）YOLOv2" class="headerlink" title="（2）YOLOv2"></a>（2）YOLOv2</h4><p><strong>YOLOv2（You Only Look Once version 2）</strong>，也称为YOLO9000，是YOLOv1的改进版本。它在保持YOLOv1高效的同时，通过一系列创新来提高检测精度和速度，尤其是在小目标检测和检测更复杂对象方面取得了显著进步。</p>
<h5 id="改进策略"><a href="#改进策略" class="headerlink" title="改进策略"></a>改进策略</h5><ol>
<li><strong>Batch Normalization（批量正则化）</strong></li>
</ol>
<p><strong>Batch Normalization</strong> 是一种在神经网络训练中常用的技术，旨在通过规范化每一层的输入来加速训练并提高模型的稳定性。</p>
<ul>
<li><p>在神经网络中，每一层的输入分布可能会随着训练的进行发生变化，这种现象叫做<strong>内部协变量偏移（Internal Covariate Shift）</strong>。随着网络层数加深，这种偏移可能会使得训练变得不稳定。</p>
</li>
<li><p><strong>Batch Normalization</strong> 的作用是在每个批次（batch）上，对每一层的输入进行归一化，使其分布保持稳定。具体来说，BN会将每一层输入的激活值规范化为零均值和单位方差：$\hat x^{(k)}&#x3D; \frac{x^{(k)}−μ_B}{\sqrt{\sigma_B^{2}+\epsilon}}$，</p>
<p>其中，$\mu_B$是当前批次的均值，$\sigma_B$ 是方差，$\epsilon$ 是一个很小的常数，用于防止除零。</p>
</li>
</ul>
<p>归一化之后，BN还会引入两个可学习的参数<strong>缩放</strong>（scale）和<strong>平移</strong>（shift），让模型能够调整归一化后的激活值：$y^{(k)}&#x3D;γx^{(k)}+β$</p>
<p>其中，$\gamma$ 和 $\beta$ 是学习参数，分别用于缩放和平移。</p>
<ul>
<li><p><strong>加速训练</strong>：BN能够规范化每个卷积层的输出，使得后续层的输入具有稳定的分布（接近零均值、单位方差）。这减少了梯度变化的幅度，从而使得模型在反向传播时能够更平稳地更新权重，<strong>加快了收敛速度</strong>。</p>
</li>
<li><p><strong>更高的学习率</strong>：由于数据分布的稳定性，模型可以使用更高的学习率进行训练，而不必担心梯度爆炸或消失的问题。这进一步加速了训练过程。</p>
</li>
<li><p><strong>正则化效果</strong>：BN在一定程度上具有正则化效果，它通过批次归一化使得模型对训练样本中的小扰动不敏感，从而<strong>减少了模型对训练数据的过拟合</strong>。由于每个批次的数据都会被归一化，模型每次看到的数据分布有所变化，这类似于数据增强，也起到了一定的正则化作用。</p>
</li>
<li><p><strong>替代Dropout</strong>：在YOLOv2中，BN层的引入使得模型不再需要使用Dropout层来防止过拟合。YOLOv1使用了Dropout来随机关闭部分神经元，防止模型过拟合，而YOLOv2发现BN本身就能够提供足够的正则化效果，因此移除了Dropout层。</p>
</li>
</ul>
<ol start="2">
<li><strong>High Resolution Classifier（高分辨率分类器）</strong></li>
</ol>
<p>YOLOv2在训练过程中使用了更高分辨率的输入（448x448）。模型最初以低分辨率（224x224）进行训练，在训练的后期将分辨率切换到448x448。高分辨率的输入有助于检测小目标，使得模型在训练的后期能够更精细地学习目标的细节。</p>
<p>YOLOv2将预训练分成两步：先用224x224的输入从头开始训练网络，大概160个epoch，然后再将输入调整到448x448，再训练10个epoch。</p>
<blockquote>
<ul>
<li><strong>batchsize</strong>：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；</li>
<li><strong>iteration</strong>：1个iteration等于使用batchsize个样本训练一次；</li>
<li><strong>epoch</strong>：1个epoch等于使用训练集中的全部样本训练一次；</li>
</ul>
</blockquote>
<p><strong>低分辨率训练的优势</strong></p>
<ul>
<li><strong>加快早期训练阶段</strong>：在模型的初期训练阶段，使用低分辨率输入可以减少计算量，从而加快训练速度。低分辨率的图像包含的细节较少，模型可以更快地从数据中学到大体的特征。</li>
<li><strong>粗略学习特征</strong>：模型在早期训练时主要学习一些粗略的、全局的特征，比如大的形状和轮廓。这个阶段不需要非常高的分辨率。</li>
</ul>
<p><strong>高分辨率输入的引入</strong></p>
<p>在训练的后期阶段，YOLOv2将输入图像的分辨率提高到448x448。这种高分辨率输入的引入有以下好处：</p>
<ul>
<li><strong>更好的细节捕捉</strong>：高分辨率的图像包含更多的像素细节，有助于模型在后期更好地捕捉图像中的小物体和细微特征。比如，在低分辨率图像中，某些小目标可能由于像素较少而变得模糊或者不可见，而在高分辨率图像中，这些目标可以清晰地被模型识别。</li>
<li><strong>提高模型的精确度</strong>：通过在后期使用高分辨率输入，模型可以在粗略特征的基础上进一步精细化，从而提高检测的精度。</li>
</ul>
<ol start="3">
<li><strong>Convolutional With Anchor Boxes（带Anchor Boxes的卷积）</strong></li>
</ol>
<p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。<strong>YOLOv2去掉了YOLOv1中的全连接层，使用Anchor Boxes预测边界框</strong>，同时为了得到更高分辨率的特征图，<strong>YOLOv2还去掉了一个池化层</strong>。</p>
<blockquote>
<p><strong>Anchor Boxes</strong> 是一组具有预定义形状和大小的边界框，模型在这些预定义框的基础上进行调整（通过回归预测），以匹配实际的物体。这种方法允许模型预测不同形状和大小的物体，而不需要直接预测绝对的边界框位置和尺寸。在YOLOv2中，特征图的每个cell（即每个网格单元）都负责预测多个Anchor Boxes。具体来说，YOLOv2在每个cell上预测5个Anchor Boxes，这意味着每个cell可以为场景中的多个物体进行检测，特别是当物体大小和形状不同时。</p>
<p>YOLOv2完全依靠<strong>卷积层</strong>进行边界框预测</p>
</blockquote>
<p>由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416<em>416的输入，模型下采样的总步长为32，最后得到13</em>13的特征图，然后对13<em>13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor boxes之后，YOLOv2可以预测13</em>13*5&#x3D;845个边界框。</p>
<ol start="4">
<li><strong>Dimension Clusters（维度聚类）</strong></li>
</ol>
<p>在Faster R-CNN和SSD中，先验框（Anchor Boxes）都是手动设定的，带有一定的主观性。</p>
<p>YOLOv2采用<strong>k-means聚类算法</strong>对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。</p>
<p>模型复杂度过高可能会导致训练困难，而召回率过低则可能漏检目标。YOLOv2综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练学习。</p>
<blockquote>
<p><strong>k-means聚类算法</strong>是一种广泛使用的无监督学习算法，主要用于将数据集划分为K个簇（clusters）。它的目标是将相似的数据点聚集在一起，同时使得不同簇之间的差异尽可能大。</p>
<p>k-means聚类的<strong>核心思想</strong>是通过迭代过程将数据点分配到最近的簇心（centroid）并更新簇心位置，从而达到最优的簇划分。它使用欧几里得距离（或其他距离度量）来评估数据点与簇心之间的相似性。</p>
<p><strong>召回率</strong>（Recall）是一个重要的评估指标，特别是在二分类和多分类任务中，用于衡量模型在正类（即感兴趣的类别）识别方面的性能。召回率反映了模型能够正确识别出多少实际存在的正类样本。</p>
</blockquote>
<ol start="5">
<li><strong>New Network（新的网络）</strong></li>
</ol>
<p>YOLOv2引入了<strong>Darknet-19</strong>，这是一种新设计的卷积神经网络，用于替代YOLOv1的特征提取网络。Darknet-19具有19个卷积层和5个池化层，且使用了大量的3x3卷积核以及1x1卷积核。</p>
<ol start="6">
<li><strong>直接定位预测（Direct location Prediction）</strong></li>
</ol>
<p>Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2沿用YOLOv1的方法，根据所在网格单元的位置来预测坐标,则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。</p>
<p>直接定位预测的核心思想是直接通过卷积层输出特征图的每个单元来预测边界框的坐标，而不是通过全连接层。</p>
<ol start="7">
<li><strong>细粒度特征（Fine-Grained Features）</strong></li>
</ol>
<p>YOLOv2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2提取Darknet-19最后一个max pool层的输入，得到26<em>26</em>512的特征图。经过1<em>1</em>64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2*2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%。</p>
<ol start="8">
<li><strong>多尺度训练（Multi-Scale Training）</strong></li>
</ol>
<p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}（最小的选项是320×320，最大的是608×608。我们调整网络的尺寸并继续训练）。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<h5 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a>训练过程</h5><ul>
<li>第一阶段：现在ImageNet分类数据集上训练Darknet-19,此时模型输入为224*224，共训练160轮</li>
<li>第二阶段：将网络输入调整为448*448，继续在ImageNet分类数据集上训练细调模型，共10轮</li>
<li>第三阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续细调网络</li>
</ul>
<h5 id="优点与缺点"><a href="#优点与缺点" class="headerlink" title="优点与缺点"></a>优点与缺点</h5><p><strong>优点</strong></p>
<ul>
<li><p>YOLOv2使用了一个新的分类器作为特征提取部分，较多使用了3x3卷积核，在每次池化后操作后把通道数翻倍。网络使用了全局平均池化，把1x1卷积核置于3x3卷积核之间，用来压缩特征。也用了batch normalization稳定模型训练</p>
</li>
<li><p>最终得出的基础模型就是Darknet-19，包含19个卷积层，5个最大池化层，运算次数55.8亿次</p>
</li>
<li><p>YOLOv2比VGG16更快，精度略低于VGG16</p>
</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>YOLOv2检测准确率不够，比SSD稍差</li>
<li>不擅长检测小物体</li>
<li>对近距离物体准确率较低</li>
</ul>
<h4 id="（3）YOLOv3"><a href="#（3）YOLOv3" class="headerlink" title="（3）YOLOv3"></a>（3）YOLOv3</h4><p>YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。</p>
<blockquote>
<p><strong>残差连接（Residual Connection）</strong>是一种在深度神经网络中使用的技术，旨在解决随着网络深度增加而可能出现的梯度消失和过拟合问题。它的基本思想是将输入直接与网络某层的输出相加，从而形成“残差”学习。</p>
<p>这种方式有几个优点：</p>
<ol>
<li><strong>缓解梯度消失</strong>：通过将输入信息直接传递到后续层，帮助保持梯度在反向传播时的流动。</li>
<li><strong>提高训练效率</strong>：允许网络在更深的结构中进行训练，而不会显著增加训练难度。</li>
<li><strong>增强模型性能</strong>：通常能够提高模型的准确性和泛化能力。</li>
</ol>
</blockquote>
<h5 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h5><ul>
<li>新网络结构：<strong>DarkNet-53</strong>，由53层卷积层组成；</li>
<li>用<strong>逻辑回归</strong>替代softmax作为分类器；</li>
<li>融合FPN（特征金字塔网络），实现多尺度检测。</li>
</ul>
<blockquote>
<p><strong>逻辑回归</strong>（Logistic Regression）是一种常用的<strong>分类算法</strong>，虽然名字中带有“回归”，但它实际上用于二元或多元分类任务，特别是处理二元分类问题（即输出为0或1的情况）。它通过学习数据特征与类别标签之间的关系来预测输入数据所属的类别。</p>
<p>逻辑回归的核心思想是使用<strong>逻辑函数（logistic function，也叫sigmoid函数）来将线性回归的输出结果转换为概率</strong>。这种概率值可以表示某个样本属于某个类别的可能性。</p>
<p>逻辑回归模型的训练目标是找到最优的权重 www 和偏置 bbb，使得模型对训练数据的预测尽可能准确。为此，逻辑回归使用<strong>对数损失函数（Log-Loss）</strong>，也称为<strong>二元交叉熵损失（Binary Cross-Entropy Loss）</strong>:</p>
<p>$$Loss(y, \hat{y}) &#x3D; -[y log\hat{y} + (1 - y)log(1 - \hat{y})]$$</p>
<p>其中：</p>
<ul>
<li>$y$ 是真实的标签（0或1），</li>
<li>$\hat{y}$​ 是模型预测的概率。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>FPN</strong>（Feature Pyramid Network，特征金字塔网络）是一种用于目标检测任务的深度学习架构，旨在通过多尺度特征融合来提高模型在不同尺度下对目标物体的检测能力。在卷积神经网络（CNN）中，越深的卷积层，特征图的空间分辨率越低，但其捕捉的语义信息更抽象、更强大；而浅层卷积层的特征图分辨率较高，保留更多的局部细节信息，但语义信息较弱。FPN的作用就是通过结合这些不同层次的特征图，使得模型既能保持高层次语义信息的表达能力，又能保留低层次的细节信息，从而在不同尺度下检测物体时都能有较好的表现。</p>
<p> FPN的工作流程主要包括以下步骤：</p>
<ol>
<li><strong>自顶向下路径（Top-Down Pathway）</strong></li>
</ol>
<ul>
<li>FPN首先通过卷积神经网络主干（如ResNet或VGG）进行前向传播，生成不同层次的特征图。较高层的特征图具有更强的语义信息，但空间分辨率较低。</li>
<li>在自顶向下路径中，FPN通过反向传播将高层的特征图进行逐层上采样（通常是2倍上采样），将高语义信息传播到较浅层次的特征图中。</li>
</ul>
<ol start="2">
<li><strong>横向连接（Lateral Connections）</strong></li>
</ol>
<ul>
<li>在自顶向下传播的过程中，每一个上采样的高层特征图与对应的浅层特征图进行横向连接（通常通过卷积操作进行融合）。这样既保留了高层的抽象语义信息，又结合了浅层的空间细节信息。</li>
<li>这个融合操作的核心在于：高层提供全局上下文信息，浅层提供更细粒度的位置信息，使得FPN对大小目标的检测更加鲁棒。</li>
</ul>
<ol start="3">
<li><strong>特征金字塔（Feature Pyramid）</strong></li>
</ol>
<ul>
<li>通过横向连接和自顶向下的传播，FPN在每一个尺度上都生成了多层次的特征图，这些特征图组合在一起形成一个特征金字塔。</li>
<li>特征金字塔中的每个层次都可以被用于不同尺度的目标检测。例如，较高分辨率的浅层特征图适合检测小物体，较低分辨率的深层特征图适合检测大物体。</li>
</ul>
</blockquote>
<h5 id="多尺度预测"><a href="#多尺度预测" class="headerlink" title="多尺度预测"></a>多尺度预测</h5><p>YOLOv3在基本特征提取器上添加几个卷积层，其中最后一个卷积层预测了一个三维张量——边界框，目标和类别预测。 </p>
<p>在COCO数据集的实验中，YOLOv3的每个网格单元预测<strong>3个边界框</strong>。每个边界框由以下内容组成：</p>
<ul>
<li><strong>4个边界框偏移量</strong>：用于调整预测框的中心坐标、宽度和高度。</li>
<li><strong>1个目标存在的预测值</strong>：表示该边界框内是否有目标（一个概率值）。</li>
<li><strong>80个类别预测</strong>：COCO数据集中有80个类别，所以每个边界框预测该目标属于这80个类别中的哪一个。</li>
</ul>
<p>因此，针对每个网格单元，预测张量的深度为：$3 \times (4+1+80)&#x3D;255$</p>
<p>这个张量的大小$N\times N\times 255$为，其中 $N$ 是特征图的宽和高，表示将输入图像划分为的网格大小（如13×13、26×26等）。</p>
<h4 id="（4）YOLOv4"><a href="#（4）YOLOv4" class="headerlink" title="（4）YOLOv4"></a>（4）YOLOv4</h4><p>YOLOv4 是 YOLO 系列的一个重要更新版本，它在保持 <strong>YOLO（You Only Look Once）</strong> 模型速度和效率的同时，进一步提升了精度。YOLOv4 集成了许多<strong>卷积神经网络（CNN）</strong>领域的最新技术，通过改进网络结构和训练策略，使得它不仅适用于学术研究，同时也适合实际应用。</p>
<h5 id="YOLOv4的创新点"><a href="#YOLOv4的创新点" class="headerlink" title="YOLOv4的创新点"></a>YOLOv4的创新点</h5><p><strong>输入端的创新点</strong>：</p>
<ol>
<li><strong>Mosaic 数据增强</strong></li>
</ol>
<p>Mosaic 数据增强是在训练时将四张图片拼接在一起，这样可以在每次迭代中提供更多的图像上下文和变化，提升模型的泛化能力，并提高对不同尺度目标的检测效果。</p>
<ol start="2">
<li><strong>cmBN（Cross Mini-Batch Normalization）</strong></li>
</ol>
<p>cmBN 解决了在小批量训练时批量归一化效果不佳的问题，通过跨多个 mini-batch 计算均值和方差，确保更稳定的训练过程。</p>
<ol start="3">
<li><strong>SAT（Self-Adversarial Training）</strong></li>
</ol>
<p>SAT 是一种自对抗训练方法，它通过让模型在生成对抗性图像时进行反向传播，使其能够抵抗输入扰动，提升模型的鲁棒性。</p>
<p><strong>BackBone主干网络</strong></p>
<ol>
<li><strong>CSPDarknet53</strong></li>
</ol>
<p>CSPDarknet53 是 YOLOv4 的骨干网络，它在 Darknet53 基础上引入了 <strong>CSPNet（Cross Stage Partial Network）</strong>，通过部分卷积层的分离与特征融合，减少计算量并提升模型的学习能力。</p>
<ol start="2">
<li><strong>Mish 激活函数</strong></li>
</ol>
<p>Mish 替代 ReLU 激活函数，具备更平滑的梯度流动，能够更好地处理负值，提升特征表达能力。</p>
<ol start="3">
<li><strong>Dropblock</strong></li>
</ol>
<p>Dropblock 是一种正则化方法，类似于 Dropout，但它通过屏蔽特征图的连续区域来增强模型的鲁棒性和泛化能力，尤其在处理高维特征时表现更好。</p>
<ul>
<li><p>Neck：目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的SPP模块、FPN+PAN结构</p>
</li>
<li><p>Prediction：输出层的锚框机制和Yolov3相同，主要改进的是训练时的回归框位置损失函数CIOU_Loss，以及预测框筛选的nms变为DIOU_nms</p>
</li>
</ul>
<h5 id="YOLOv4的整体架构"><a href="#YOLOv4的整体架构" class="headerlink" title="YOLOv4的整体架构"></a><strong>YOLOv4的整体架构</strong></h5><p>YOLOv4 的架构主要可以分为三个部分：</p>
<p><strong>第一部分、Backbone（主干网络）</strong>：用于提取图像的特征。</p>
<p>YOLOv4 使用 <strong>CSPDarkNet53</strong> 作为主干网络，这是对 YOLOv3 中的 <strong>DarkNet-53</strong> 的改进。CSPDarkNet53（Cross Stage Partial DarkNet-53）采用了 <strong>CSPNet</strong> 结构，具体改进包括：</p>
<ul>
<li><strong>CSPNet（Cross Stage Partial Network）</strong>：将主干网络的输入特征图分成两部分，一部分通过网络深层提取特征，另一部分通过捷径路径直接与深层特征融合，减少了重复计算，提升了网络的学习能力，同时降低了模型复杂度。</li>
<li><strong>Mish 激活函数</strong>：YOLOv4 使用了 <strong>Mish</strong> 作为激活函数，代替了 YOLOv3 中的 Leaky ReLU。Mish 被证明在一些深度学习任务中比 ReLU 和 Leaky ReLU 更有效，特别是在训练过程中能够获得更好的梯度流动。</li>
<li><strong>CutMix 和 Mosaic 数据增强</strong>：YOLOv4 引入了两种强大的数据增强方法——<strong>CutMix</strong> 和 <strong>Mosaic</strong>，能够在数据增强过程中合成多个图像，生成新的训练样本，从而提升了模型的泛化能力。</li>
</ul>
<blockquote>
<p><strong>CSPNet</strong> 的核心思想是通过<strong>特征分流</strong>和<strong>特征融合</strong>，减少重复计算，降低计算量并提升特征表达的多样性。具体过程如下：</p>
<ol>
<li>将输入特征图 <strong>X</strong> 分为两部分：<strong>X1</strong> 和 <strong>X2</strong>。<ul>
<li><strong>X1</strong>：一部分输入特征图 <strong>X1</strong> 被送入网络的深层结构（如多个卷积层）进行进一步的特征提取。</li>
<li><strong>X2</strong>：另一部分输入特征图 <strong>X2</strong> 没有经过深层处理，而是通过<strong>捷径路径</strong>直接传递到后续层。</li>
</ul>
</li>
<li>在网络的某个位置，<strong>X1</strong> 的深层特征与 <strong>X2</strong>（捷径路径中的特征）进行融合，通常通过<strong>按元素相加</strong>或<strong>拼接</strong>的方式进行。这个融合过程帮助模型将<strong>深层特征的语义信息</strong>与<strong>浅层特征的细节信息</strong>结合在一起，提升了特征的表达能力。</li>
</ol>
<p><strong>Mish激活函数</strong>是光滑的非单调激活函数，$mish(x)&#x3D;x⋅tanh(ln(1+e^x))$</p>
</blockquote>
<p><strong>第二部分、Neck（颈部）</strong>：通过特征金字塔网络（FPN）和路径聚合网络（PAN）实现多尺度特征的融合。</p>
<p>YOLOv4 在颈部使用了两种关键技术来进行特征融合，帮助模型更好地处理多尺度目标检测：</p>
<ul>
<li><strong>FPN（特征金字塔网络）</strong>：FPN 主要用于从网络的不同层次提取特征，并通过<strong>上采样</strong>将高层次的语义信息与浅层次的高分辨率信息进行融合，从而提升多尺度物体检测的效果。</li>
<li><strong>PAN（路径聚合网络）</strong>：PAN 是 YOLOv4 中的另一个特征融合网络，它采用了一个自底向上的路径来将低层特征向上传递，进一步增强浅层特征的传播，提升对小物体的检测效果。</li>
</ul>
<p>通过这两种特征融合策略，YOLOv4 的不同尺度特征图都能够兼顾语义信息和细节信息，从而提升检测精度。</p>
<blockquote>
<p>PAN 是在 FPN 的基础上提出的，用于进一步加强特征的多层次融合，特别是为了改进网络的<strong>信息流动</strong>和<strong>特征传递</strong>。</p>
<p>FPN 采用了<strong>自顶向下</strong>的特征融合机制，但它仅从高层到低层传递信息，无法充分利用从低层到高层的反向信息流动。PAN 则通过引入<strong>自底向上（bottom-up）</strong>的特征聚合路径，使得浅层的高分辨率特征可以向上传递，并与高层次的特征相结合。通过这种双向的信息流动，PAN 提高了多尺度特征的表达能力。</p>
</blockquote>
<p><strong>第三部分、Head（头部）</strong>：用于最终的边界框预测和分类。</p>
<p>YOLOv4 在检测头部仍然采用了与 YOLOv3 类似的机制.</p>
<h2 id="4-视频关键帧处理"><a href="#4-视频关键帧处理" class="headerlink" title="4. 视频关键帧处理"></a>4. 视频关键帧处理</h2><p><strong>关键帧（I-Frame）</strong>：</p>
<ul>
<li><p>关键帧是包含该段视频中主要信息的帧</p>
</li>
<li><p>关键帧在压缩成AVI, MP4, MOV等格式时，该帧会完全保留</p>
</li>
<li><p>视频解码时只需要本帧数据，不需要从前一帧、后一帧获取数据</p>
</li>
</ul>
<p><strong>前向差别帧（P-Frame）</strong></p>
<ul>
<li><p>当前帧与前一个I-Frame或前一个P-Frame之间的差别，可以理解为与前一帧的数据偏移值</p>
</li>
<li><p>P-Frame没有完整数据画面，只有与前一帧的差别信息，解码时需要从前一帧获取数据</p>
</li>
</ul>
<p><strong>双向差别帧（B-Frame）</strong></p>
<ul>
<li>记录本帧与前一帧、后一帧的差别</li>
<li>解码时需要获取前一帧、后一帧的数据</li>
<li>压缩后的视频体积小，但编解码计算较慢</li>
</ul>
<p>可以使用<strong>FFMPEG工具</strong>提取视频中的关键帧和进行视频截取。</p>
<h2 id="5-图像标注工具"><a href="#5-图像标注工具" class="headerlink" title="5. 图像标注工具"></a>5. 图像标注工具</h2><p><strong>Labelme（指定使用）</strong><br>labelme 是一款开源的图像&#x2F;视频标注工具，标签可用于目标检测、分割和分类。灵感是来自于 MIT 开源的一款标注工具 Labelme。Labelme具有的特点是：</p>
<ul>
<li>支持图像的标注的组件有：矩形框，多边形，圆，线，点（rectangle, polygons, circle, lines, points）</li>
<li>支持视频标注</li>
<li>GUI 自定义</li>
<li>支持导出 VOC 格式用于 semantic&#x2F;instance segmentation</li>
<li>支出导出 COCO 格式用于 instance segmentation</li>
</ul>
<h1 id="三、图像质量评价"><a href="#三、图像质量评价" class="headerlink" title="三、图像质量评价"></a>三、图像质量评价</h1><h2 id="1-基本概念-1"><a href="#1-基本概念-1" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>从有没有人参与的角度区分，图像质量评价方法有主观评价和客观评价两个分支。主观评价以人作为观测者，对图像进行主观评价，力求能够真实地反映人的视觉感知；客观评价方法借助于某种数学模型，反映人眼的主观感知，给出基于数字计算的结果。</p>
<h3 id="1-主观评价"><a href="#1-主观评价" class="headerlink" title="1. 主观评价"></a>1. 主观评价</h3><p><strong>主观评价</strong>是通过人类观察者直接评估图像质量。这种方法依赖于人眼对图像的感知，往往更贴近实际的视觉体验。</p>
<h4 id="（1）绝对评价"><a href="#（1）绝对评价" class="headerlink" title="（1）绝对评价"></a>（1）绝对评价</h4><p>由观察者根据自己的知识和理解，按照某些特定评价性能对图像的绝对好坏进行评价。通常，图像质量的绝对评价都是观察者参照原始图像对待定图像采用双刺激连续质量分级法，给出一个直接的质量评价值。</p>
<blockquote>
<p><strong>双刺激连续质量分级法</strong>（<strong>DSCQS</strong>，Double Stimulus Continuous Quality Scale）具体做法是将待评价图像和原始图像按一定规则交替播放持续一定时间给观察者，然后在播放后留出一定的时间间隔供观察者打分，最后将所有给出的分数取平均作为该序列的评价值，即该待评图像的评价值。</p>
</blockquote>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/764cb2e00594c5b6b2597aec07d02ef3.png" alt="img"></p>
<h4 id="（2）相对评价"><a href="#（2）相对评价" class="headerlink" title="（2）相对评价"></a>（2）相对评价</h4><p>相对评价中没有原始图像作为参考，是由观察者对一批待评价图像进行相互比较，从而判断出每个图像的优劣顺序，并给出相应的评价值。通常，相对评价采用单刺激连续质量评价方法。相对于主观绝对评价，主观相对评价也规定了相应的评分制度，称为“群优度尺度”。</p>
<blockquote>
<p><strong>单刺激连续质量评价方法（SSCQE</strong>，Single Stimulus Continuous Quality Evaluation）具体做法是，将一批待评价图像按照一定的序列播放，此时观察者在观看图像的同时给出待评图像相应的评价分值。</p>
</blockquote>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/c33f2f3bbe91dca974d5d1f43f91cfd4.png" alt="img"></p>
<h3 id="2-客观评价"><a href="#2-客观评价" class="headerlink" title="2. 客观评价"></a>2. 客观评价</h3><p>图像质量客观评价的基本目标是设计能精确和自动感知图像质量的计算模型。其终极目标是希望用计算机来代替人类视觉系统去观看和认知图像。在国际上，图像质量客观评价通常是通过测试多个影响影像质量的因素的表现，并通过计算模型获得图像质量量化值与人类主观观测值一致性的好坏来评估的。美国的Imatest和法国的DxO analyzer就是其中比较出名的图像质量客观评价系统。</p>
<img src="./基于深度学习的图像处理基本知识.assets/e06b2e09f4f2221a805e8dd373c1cfe1.jpeg" alt="img" style="zoom: 67%;" />

<p>Imatest和DxO analyzer有异曲同工之处，都是将影像质量评测拆分成多个测试项目，分别对每个项目进行测试、打分。两者相比，DxO analyzer的测试项目会稍微全面一些。</p>
<p>无论是Imatest还是DxO analyzer，两个测试系统都是通过<strong>“测试卡+光源环境+测试软件&#x3D;测试结果”</strong>的模式。通过各种各样的测试卡和光源，在实验室中模拟各种环境，再把成像结果输入软件系统，由系统自动分析，最后得出结果。</p>
<blockquote>
<p><strong>测试卡</strong>是一种专门设计的图像或图形，用于测试和校准显示设备、摄像机、图像处理系统或视频信号质量。测试卡通常具有特定的几何图形、颜色、灰度等级等元素，帮助评估显示设备或图像处理系统的性能。</p>
</blockquote>
<h2 id="2-IQA评估指标"><a href="#2-IQA评估指标" class="headerlink" title="2. IQA评估指标"></a>2. IQA评估指标</h2><h3 id="1-主观评价指标"><a href="#1-主观评价指标" class="headerlink" title="1. 主观评价指标"></a>1. 主观评价指标</h3><p>主观评价依赖于人类观察者的感知，主要通过以下指标进行评价：</p>
<ul>
<li><strong>MOS（Mean Opinion Score，平均主观得分）</strong>：观察者对图像质量打分，通常在1到5的范围内，最终取其平均值。MOS广泛用于图像、视频的质量评估。</li>
<li><strong>DMOS（Differential Mean Opinion Score，平均主观得分差异）</strong>：表示观察者对参考图像和失真图像的评分差异，用于评估失真引入的质量损失。</li>
</ul>
<p><strong>优点</strong>：与人类视觉感知高度相关，评价较为准确。<br><strong>缺点</strong>：耗费人力，费时且不易自动化。</p>
<h3 id="2-客观评价指标"><a href="#2-客观评价指标" class="headerlink" title="2. 客观评价指标"></a>2. 客观评价指标</h3><p>客观评价方法不依赖于人眼，而是通过算法对图像进行定量分析。以下是一些常见的客观评价指标：</p>
<h4 id="与整体图像差异相关的指标："><a href="#与整体图像差异相关的指标：" class="headerlink" title="与整体图像差异相关的指标："></a>与整体图像差异相关的指标：</h4><ul>
<li><strong>MSE（Mean Squared Error，均方误差）</strong>：评估失真图像和参考图像在像素级别上的差异。<br><strong>缺点</strong>：与人眼的视觉感知不符，不能反映局部的细节差异。</li>
<li><strong>RMSE（Root Mean Squared Error，均方根误差）</strong>：均方误差的平方根形式，衡量图像预测误差。<br><strong>优点</strong>：常用于计算算法预测的精度。</li>
<li><strong>PSNR（Peak Signal to Noise Ratio，峰值信噪比）</strong>：通过信号和噪声之间的比例来衡量图像质量。<br><strong>优点</strong>：计算简单，速度快。<br><strong>缺点</strong>：无法反映人类感知到的图像质量差异，特别是对细微的感知变化不敏感。</li>
<li><strong>SSIM（Structural Similarity Index，结构相似性）</strong>：衡量两幅图像在亮度、对比度和结构上的相似性，考虑了人眼对结构信息的敏感度。<br><strong>优点</strong>：比MSE和PSNR更符合人眼视觉感知。<br><strong>缺点</strong>：在位移、旋转、缩放等非结构性失真情况下效果不佳。</li>
</ul>
<h4 id="与主观视觉感知相关的指标："><a href="#与主观视觉感知相关的指标：" class="headerlink" title="与主观视觉感知相关的指标："></a>与主观视觉感知相关的指标：</h4><ul>
<li><strong>UIQI（Universal Image Quality Index，通用图像质量指数）</strong>：从亮度、对比度、结构等方面综合考虑图像质量。<br><strong>优点</strong>：综合反映了人眼感知的多个因素。</li>
<li><strong>VIF（Visual Information Fidelity，视觉信息保真度）</strong>：通过评估图像中的信息损失来衡量质量，注重高频细节。<br><strong>优点</strong>：与人眼感知的图像信息损失一致，尤其适用于图像的高频部分。</li>
<li><strong>IFC（Information Fidelity Criterion，信息保真度准则）</strong>：基于自然场景统计模型，通过高频细节评估图像信息的丢失，与人眼感知高频细节的敏感性相关。<br><strong>优点</strong>：能够很好地匹配人类视觉对高频细节的敏感度，因此在图像超分辨率质量评估中表现优秀。</li>
</ul>
<h4 id="相关性和离出率："><a href="#相关性和离出率：" class="headerlink" title="相关性和离出率："></a>相关性和离出率：</h4><ul>
<li><strong>LCC（Linear Correlation Coefficient，线性相关系数）</strong>：衡量算法评价与人眼评分的线性相关性。</li>
<li><strong>SROCC（Spearman’s Rank-Order Correlation Coefficient，Spearman 秩相关系数）</strong>：衡量评价值与主观打分的单调性。</li>
<li><strong>KROCC（Kendall’s Rank Correlation Coefficient，Kendall 秩相关系数）</strong>：也用于衡量预测结果的单调性，与SROCC类似。</li>
<li><strong>OR（Outlier Ratio，离出率）</strong>：衡量预测值与主观评分的离群点比例。</li>
</ul>
<h3 id="不同方法的优缺点比较："><a href="#不同方法的优缺点比较：" class="headerlink" title="不同方法的优缺点比较："></a>不同方法的优缺点比较：</h3><p>IFC &gt; NQM &gt; WPSNR &gt; MSSSIM &gt; SSIM &gt; UIQI &gt; PSNR &gt;VIF </p>
<ul>
<li><strong>PSNR</strong>：简单快速，但与人类视觉不完全匹配。</li>
<li><strong>SSIM</strong>：比PSNR更符合人眼感知，但对于几何失真（如旋转、缩放）不敏感。</li>
<li><strong>VIF</strong> 和 <strong>IFC</strong>：在高频细节上表现优秀，尤其适用于超分辨率等高精度重建任务。</li>
</ul>
<p>IFC被认为在超分辨率任务中的表现最佳，因为它专注于图像的高频细节，这正是人眼对超分辨率图像最敏感的部分。</p>
<h2 id="3-图像质量检测方式"><a href="#3-图像质量检测方式" class="headerlink" title="3. 图像质量检测方式"></a>3. 图像质量检测方式</h2><h3 id="1-半参考方法"><a href="#1-半参考方法" class="headerlink" title="1. 半参考方法"></a>1. 半参考方法</h3><p>图像的某些特征与原始图像的相同特征进行比较，比如小波变换系数的概率分布、综合多尺度几何分析、对比度敏感函数和可觉察灰度差异特征等。其相应的应用领域包括视频传输中的数字水印验证、利用副通道进行视频质量监控与码流率控制等。</p>
<p><strong>全参考方法</strong>是指在进行图像质量评价时，完全依赖于一张“无失真”或“原始”图像作为参考，通过比较该参考图像和失真图像来量化图像质量的下降程度。这类方法假设原始图像可以被获取，并且它代表了“最佳”质量。</p>
<p><strong>半参考方法</strong>是一种介于全参考和无参考方法之间的质量评价方式，它只需要从参考图像中提取一部分信息（特征），而不需要完整的原始图像。通过这些部分特征来估计图像质量，这在一些受限的场景中非常有用，如网络带宽有限时。</p>
<h3 id="2-盲图像质量"><a href="#2-盲图像质量" class="headerlink" title="2. 盲图像质量"></a>2. 盲图像质量</h3><p>盲图像质量(Blind image quality, BIQ)评价方法完全无需参考图像，根据失真图像的自身特征来估计图像的质量。有些方法是面向特定失真类型的，如针对模糊、噪声、块状效应的严重程度进行评价； 有些方法先进行失真原因分类， 再进行定量评价；而有些方法则试图同时评价不同失真类型的图像。无参考方法最具实用价值, 有着非常广泛的应用范围。 </p>
<p>盲图像质量评价的方法大体上可以分为以下几类：</p>
<h4 id="1-基于失真模型"><a href="#1-基于失真模型" class="headerlink" title="1. 基于失真模型"></a>1. <strong>基于失真模型</strong></h4><p>这类方法假设图像可能会遭受某种特定类型的失真（如噪声、模糊、压缩伪影等），并通过识别这些失真来评估图像质量。</p>
<ul>
<li><strong>典型模型</strong>：通过分析图像的统计特性（如噪声、边缘强度、频域特征等），检测特定类型的失真。例如，针对模糊的图像，可以通过边缘信息和梯度变化来评估质量。</li>
<li><strong>优点</strong>：这种方法适用于失真类型已知的场景，能够提供快速有效的评估。</li>
<li><strong>缺点</strong>：在复杂或混合失真条件下，这些模型可能无法准确评估质量，因为它们假定某种特定失真类型。</li>
</ul>
<h4 id="2-基于自然场景统计"><a href="#2-基于自然场景统计" class="headerlink" title="2. 基于自然场景统计"></a>2. <strong>基于自然场景统计</strong></h4><p>自然场景统计法（Natural Scene Statistics, NSS）认为，正常的自然图像在某些统计特性上具有一致性，而失真图像往往会偏离这些特性。通过评估图像偏离自然场景统计特征的程度，可以判断图像的质量。</p>
<ul>
<li><strong>典型特征</strong>：常使用小波变换、傅里叶变换等方法提取图像的统计特性，然后通过这些特性与标准自然场景的统计特性进行比较。</li>
<li><strong>BRISQUE（Blind&#x2F;Referenceless Image Spatial Quality Evaluator）</strong>：这是一个经典的基于NSS特征的盲图像质量评价模型。它通过分析图像局部对比度、亮度等特征，构建图像的统计模型，从而评估其质量。</li>
<li><strong>优点</strong>：能够应对多种类型的失真，对图像的质量评估具有较强的鲁棒性。</li>
<li><strong>缺点</strong>：计算复杂度较高，且依赖于特征提取方法的准确性。</li>
</ul>
<h4 id="3-基于机器学习"><a href="#3-基于机器学习" class="headerlink" title="3. 基于机器学习"></a>3. <strong>基于机器学习</strong></h4><p>近年来，随着机器学习和深度学习的发展，基于学习的盲图像质量评价方法变得越来越流行。通过使用大量带标签的图像训练模型，机器学习算法能够自动学习图像的质量特征。</p>
<ul>
<li><strong>方法流程</strong>：<ol>
<li><strong>特征提取</strong>：从图像中提取特征（如纹理、边缘、对比度等）。</li>
<li><strong>训练模型</strong>：使用带有质量评分的图像数据库（如LIVE、TID2013等）进行训练，使模型能够根据特征预测图像的质量。</li>
<li><strong>质量预测</strong>：训练好的模型可用于预测新图像的质量得分。</li>
</ol>
</li>
<li><strong>深度学习模型</strong>：使用卷积神经网络（CNN）等深度学习模型，不仅可以自动提取高级特征，还可以直接进行图像质量评分。近年来，基于深度学习的BIQA方法表现优异。</li>
<li><strong>优点</strong>：能够处理各种复杂和混合类型的失真，通过大规模数据训练可以获得高精度的评估。</li>
<li><strong>缺点</strong>：训练过程需要大量带标签的数据，计算资源需求较高。</li>
</ul>
<h4 id="4-基于内容"><a href="#4-基于内容" class="headerlink" title="4. 基于内容"></a>4. <strong>基于内容</strong></h4><p>这类方法根据图像内容对其质量进行评价。不同类型的图像（如自然图像、医学图像、文本图像）可能对失真的敏感性不同，因此这类方法通过识别图像内容，采用不同的策略进行质量评价。</p>
<ul>
<li><strong>场景自适应</strong>：例如，文本图像中边缘信息非常重要，模糊会显著降低质量；而在自然场景图像中，颜色和纹理的失真可能更加重要。</li>
<li><strong>优点</strong>：针对特定场景的质量评价效果好。</li>
<li><strong>缺点</strong>：不具备通用性，需要根据具体图像类型调整评估策略。</li>
</ul>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241024154152036-9755718.png" alt="image-20241024154152036"></p>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241024154451934-9755894.png" alt="image-20241024154451934"></p>
<h3 id="3-基于深度学习的IQA模型"><a href="#3-基于深度学习的IQA模型" class="headerlink" title="3. 基于深度学习的IQA模型"></a>3. 基于深度学习的IQA模型</h3><p>在深度学习领域有不同类型的深度神经网络结构：卷积神经网络（Convolutional Neural Network，CNN）、循环神经网络（Recurrent Neural Network，RNN）、生成对抗网络（Generative Adversarial Network，GAN）和变换器（Transformer）等。CNN和GAN常用在图像质量评价算法中，RNN网络模型多用于对视频的质量评价，Transformer首先在自然语言处理领域取得成功，近几年，开始应用到计算机视觉领域。</p>
<h4 id="（1）基于卷积神经网络"><a href="#（1）基于卷积神经网络" class="headerlink" title="（1）基于卷积神经网络"></a>（1）基于卷积神经网络</h4><p>基于 CNN 的 IQA 方法通过深度学习网络自动学习图像的特征，进而预测图像的质量。其核心思想是训练一个神经网络模型，使其能够从输入图像中提取出与质量相关的特征，然后基于这些特征输出质量评分。这个过程通常分为以下几个步骤：</p>
<ol>
<li><strong>数据集准备</strong></li>
</ol>
<p>基于 CNN 的 IQA 方法需要大量带标签的图像数据集进行训练。通常这些数据集包含失真的图像以及相应的质量评分（如主观质量评分 MOS 或 DMOS）。</p>
<ol start="2">
<li><strong>CNN 模型设计</strong></li>
</ol>
<p>CNN 模型的设计和选择是基于卷积神经网络从图像中提取特征的能力。常用的 CNN 架构包括多层卷积层、池化层、全连接层等。常见的设计思路包括：</p>
<ul>
<li><strong>浅层网络</strong>：浅层网络结构通常只包含几层卷积层，适合特征简单的质量评价任务。</li>
<li><strong>深层网络</strong>：较深的 CNN 结构（如 ResNet、VGG 等）可以提取出更复杂的高级特征，适合处理复杂的失真类型。</li>
</ul>
<ol start="3">
<li><strong>特征提取</strong></li>
</ol>
<p>CNN 自动从图像中提取多层次的特征，低层卷积层提取简单的边缘、纹理等低级特征，高层卷积层则学习到更加复杂的形状和模式等高级特征。</p>
<ol start="4">
<li><strong>质量评分预测</strong></li>
</ol>
<p>CNN 最终会通过全连接层或回归层将提取到的特征映射为图像质量得分，通常输出一个连续值作为图像的质量分数（如 1-5 或 0-100 的范围）。这种得分可以是主观质量评分的近似值，也可以是由具体场景中定义的质量指标。</p>
<h4 id="（2）基于生成对抗网络"><a href="#（2）基于生成对抗网络" class="headerlink" title="（2）基于生成对抗网络"></a>（2）基于生成对抗网络</h4><p>基于生成对抗网络（Generative Adversarial Networks, <strong>GAN</strong>）的图像质量检测是一种新兴的、性能较好的无参考图像质量评价方法。GAN 主要由生成器（Generator）和判别器（Discriminator）两个对抗网络组成，其中判别器可以用于图像质量的预测和评价。基于 GAN 的图像质量检测利用判别器的能力来区分高质量和低质量的图像，从而有效地检测和评价图像质量。</p>
<p>在传统的 GAN 框架中，生成器旨在生成逼真的图像，而判别器负责区分真实图像和生成的假图像。在图像质量检测任务中，这一框架可以被重构用于评估图像质量：</p>
<ol>
<li><p><strong>生成器：</strong></p>
<p>生成器通常用于生成或者恢复高质量图像，但在 IQA 中，它可以用来生成不同质量的失真图像（即尝试生成可能的失真类型）。</p>
</li>
<li><p><strong>判别器：</strong></p>
<p>判别器的任务是对图像的真实性或质量进行判别。它接收真实图像和生成的图像作为输入，判断它们是否是高质量图像。判别器不仅可以区分真假图像，还可以用于输出一个质量评分（通常是基于对图像细节的理解）。</p>
</li>
</ol>
<p>在图像质量检测中，判别器经过训练后可以学习到高质量图像和低质量图像之间的差异，因此能够有效预测失真图像的质量分数。</p>
<p><strong>GAN 工作流程</strong></p>
<ol>
<li><strong>初始化</strong>：生成器和判别器网络参数随机初始化。</li>
<li><strong>生成图像</strong>：生成器接收随机噪声向量，生成一批图像。</li>
<li><strong>分类真假图像</strong>：判别器接收生成图像和真实图像，输出每张图像是真实的概率。</li>
<li><strong>损失计算</strong>：<ul>
<li>判别器的损失：正确分类真实图像和生成图像的能力。</li>
<li>生成器的损失：欺骗判别器，使生成图像被判别为真实图像的能力。</li>
</ul>
</li>
<li><strong>参数更新</strong>：<ul>
<li>生成器通过最小化损失生成更逼真的图像。</li>
<li>判别器通过最大化损失提高分类真假图像的能力。</li>
</ul>
</li>
<li><strong>重复训练</strong>：循环执行上述过程，直到生成器生成的图像足够逼真，判别器难以区分。</li>
</ol>
<h4 id="（3）基于Transformer"><a href="#（3）基于Transformer" class="headerlink" title="（3）基于Transformer"></a>（3）基于Transformer</h4><p>基于 <strong>Transformer</strong> 的图像质量评价（Image Quality Assessment, IQA）方法是近年来随着 Transformer 模型在计算机视觉任务中的成功而发展起来的一类新型方法。这些方法借助 Transformer 的强大建模能力，能够捕捉图像的全局依赖关系和复杂的特征，尤其在处理无参考图像质量评价（NR-IQA）任务中表现突出。</p>
<p>基于 Transformer 的 IQA 模型一般分为两个核心模块：</p>
<ul>
<li><strong>图像分块处理</strong>：将输入图像分成一系列小块（如 ViT 中的 Patch），每个小块作为一个 token 进入 Transformer 网络。这样可以保留图像的局部信息，同时通过 Transformer 捕捉图像块之间的全局依赖。</li>
<li><strong>全局特征提取</strong>：通过<strong>自注意力机制</strong>，Transformer 网络可以学习到图像中局部区域和全局场景之间的复杂关系。这对于衡量图像质量中的全局失真（如模糊、伪影等）非常有效。</li>
</ul>
<blockquote>
<p><strong>自注意力机制（Self-Attention Mechanism）</strong> 是神经网络中的一种关键技术，最早在自然语言处理领域的 Transformer 模型中提出，后来也被广泛应用于图像处理等领域。它通过计算输入序列中每个元素与其他元素之间的关系来提取全局信息。自注意力机制能够灵活地捕捉输入数据的全局依赖关系，这与传统的局部特征提取（如卷积操作）不同，因而在处理复杂数据时具有强大的建模能力</p>
</blockquote>

    </div>

    
    
    

      <footer class="post-footer">
          
          <div class="post-tags">
              <a href="/tags/%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7/" rel="tag"><i class="fa fa-tag"></i> 图像质量评价</a>
              <a href="/tags/%E5%9B%BE%E5%83%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B/" rel="tag"><i class="fa fa-tag"></i> 图像目标检测</a>
              <a href="/tags/CNN/" rel="tag"><i class="fa fa-tag"></i> CNN</a>
              <a href="/tags/YOLO/" rel="tag"><i class="fa fa-tag"></i> YOLO</a>
              <a href="/tags/GAN/" rel="tag"><i class="fa fa-tag"></i> GAN</a>
              <a href="/tags/Transformer/" rel="tag"><i class="fa fa-tag"></i> Transformer</a>
          </div>
          <script type="text/javascript">
            var tagsall=document.getElementsByClassName("post-tags")
            for (var i = tagsall.length - 1; i >= 0; i--){
                var tags=tagsall[i].getElementsByTagName("a");
                for (var j = tags.length - 1; j >= 0; j--) {
                    var golden_ratio = 0.618033988749895;
                    var s = 0.5;
                    var v = 0.999;
                    var h = golden_ratio + Math.random()*0.8 - 0.5;
                    var h_i = parseInt(h * 6);
                    var f = h * 6 - h_i;
                    var p = v * (1 - s);
                    var q = v * (1 - f * s);
                    var t = v * (1 - (1 - f) * s);
                    var r, g, b;
                    switch (h_i) {
                        case 0:
                            r = v;
                            g = t;
                            b = p;
                            break;
                        case 1:
                            r = q;
                            g = v;
                            b = p;
                            break;
                        case 2:
                            r = p;
                            g = v;
                            b = t;
                            break;
                        case 3 :
                            r = p;
                            g = q;
                            b = v;
                            break;
                        case 4:
                            r = t;
                            g = p;
                            b = v;
                            break;
                        case 5:
                            r = v;
                            g = p;
                            b = q;
                            break;
                        default:
                            r = 1;
                            g = 1;
                            b = 1;
                      }
                    tags[j].style.background = "rgba("+parseInt(r*255)+","+parseInt(g*255)+","+parseInt(b*255)+","+0.5+")";
                }
            }                        
            </script>

          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2024/11/07/Quality-aware%20Pre-trained%20Models%20for%20Blind%20Image%20Quality%20Assessment/" rel="prev" title="Quality-Aware Pre-Trained Models for Blind Image Quality Assessment">
      <i class="fa fa-chevron-left"></i> Quality-Aware Pre-Trained Models for Blind Image Quality Assessment
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%80%E3%80%81%E9%A1%BB%E7%9F%A5"><span class="nav-text">一、须知</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E6%B7%B1%E5%BA%A6%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">1. 深度卷积神经网络</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E4%BB%80%E4%B9%88%E6%98%AF%E5%8D%B7%E7%A7%AF"><span class="nav-text">（1）什么是卷积</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%8D%B7%E7%A7%AF%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="nav-text">（2）卷积核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B"><span class="nav-text">（3）卷积神经网络模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E8%BE%93%E5%85%A5%E5%B1%82"><span class="nav-text">1. 输入层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%8D%B7%E7%A7%AF%E5%B1%82"><span class="nav-text">2. 卷积层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E6%BF%80%E6%B4%BB%E5%B1%82"><span class="nav-text">3. 激活层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E6%B1%A0%E5%8C%96%E5%B1%82%EF%BC%88%E4%B8%8B%E9%87%87%E6%A0%B7%E5%B1%82%EF%BC%89"><span class="nav-text">4. 池化层（下采样层）</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#5-%E5%85%A8%E8%BF%9E%E6%8E%A5%E5%B1%82"><span class="nav-text">5. 全连接层</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#6-%E8%BE%93%E5%87%BA%E5%B1%82"><span class="nav-text">6. 输出层</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%BA%8C%E3%80%81%E5%9B%BE%E5%83%8F%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B"><span class="nav-text">二、图像目标检测</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5"><span class="nav-text">1. 基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E6%A0%B8%E5%BF%83%E9%97%AE%E9%A2%98"><span class="nav-text">1. 核心问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E7%AE%97%E6%B3%95%E5%88%86%E7%B1%BB"><span class="nav-text">2. 目标检测算法分类</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Tow-Stage"><span class="nav-text">Tow Stage</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#One-Stage"><span class="nav-text">One Stage</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E5%8E%9F%E7%90%86"><span class="nav-text">2. 目标检测原理</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%80%99%E9%80%89%E5%8C%BA%E5%9F%9F%E4%BA%A7%E7%94%9F"><span class="nav-text">1. 候选区域产生</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E6%BB%91%E5%8A%A8%E7%AA%97%E5%8F%A3"><span class="nav-text">（1）滑动窗口</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E9%80%89%E6%8B%A9%E6%80%A7%E9%80%89%E6%8B%A9"><span class="nav-text">（2）选择性选择</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E6%95%B0%E6%8D%AE%E8%A1%A8%E7%A4%BA"><span class="nav-text">2. 数据表示</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%85%A5%E5%9B%BE%E5%83%8F"><span class="nav-text">输入图像</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%B9%E7%95%8C%E6%A1%86%EF%BC%88Bounding-Box%EF%BC%89"><span class="nav-text">边界框（Bounding Box）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%B1%BB%E5%88%AB%E6%A0%87%E7%AD%BE%EF%BC%88Class-Label%EF%BC%89"><span class="nav-text">类别标签（Class Label）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%91%E6%A0%BC%EF%BC%88Grid%EF%BC%89"><span class="nav-text">网格（Grid）</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%BE%93%E5%87%BA%E6%A0%BC%E5%BC%8F"><span class="nav-text">输出格式</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%95%B0%E6%8D%AE%E9%9B%86%E6%A0%BC%E5%BC%8F"><span class="nav-text">数据集格式</span></a></li></ol></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E6%95%88%E6%9E%9C%E8%AF%84%E4%BC%B0"><span class="nav-text">3. 效果评估</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B"><span class="nav-text">3. 目标检测模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-R-CNN%E7%B3%BB%E5%88%97"><span class="nav-text">1. R-CNN系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89R-CNN"><span class="nav-text">（1）R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4%EF%BC%9A"><span class="nav-text">详细步骤：</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9"><span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89Fast-R-CNN"><span class="nav-text">（2）Fast R-CNN</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AF%A6%E7%BB%86%E6%AD%A5%E9%AA%A4"><span class="nav-text">详细步骤</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BC%BA%E7%82%B9-1"><span class="nav-text">缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89Faster-R-CNN"><span class="nav-text">（3）Faster R-CNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#RPN-%E7%9A%84%E5%B7%A5%E4%BD%9C%E5%8E%9F%E7%90%86%EF%BC%9A"><span class="nav-text">RPN 的工作原理：</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%884%EF%BC%89Mask-R-CNN"><span class="nav-text">（4）Mask R-CNN</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-YOLO%E7%B3%BB%E5%88%97"><span class="nav-text">2. YOLO系列</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89YOLOv1"><span class="nav-text">（1）YOLOv1</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%9F%BA%E6%9C%AC%E6%80%9D%E6%83%B3"><span class="nav-text">基本思想</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E7%BD%91%E7%BB%9C%E7%BB%93%E6%9E%84"><span class="nav-text">网络结构</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B"><span class="nav-text">训练过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%98%E7%BC%BA%E7%82%B9"><span class="nav-text">优缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89YOLOv2"><span class="nav-text">（2）YOLOv2</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B%E7%AD%96%E7%95%A5"><span class="nav-text">改进策略</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E8%AE%AD%E7%BB%83%E8%BF%87%E7%A8%8B-1"><span class="nav-text">训练过程</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E4%BC%98%E7%82%B9%E4%B8%8E%E7%BC%BA%E7%82%B9"><span class="nav-text">优点与缺点</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89YOLOv3"><span class="nav-text">（3）YOLOv3</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#%E6%94%B9%E8%BF%9B"><span class="nav-text">改进</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#%E5%A4%9A%E5%B0%BA%E5%BA%A6%E9%A2%84%E6%B5%8B"><span class="nav-text">多尺度预测</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%884%EF%BC%89YOLOv4"><span class="nav-text">（4）YOLOv4</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#YOLOv4%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9"><span class="nav-text">YOLOv4的创新点</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#YOLOv4%E7%9A%84%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84"><span class="nav-text">YOLOv4的整体架构</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#4-%E8%A7%86%E9%A2%91%E5%85%B3%E9%94%AE%E5%B8%A7%E5%A4%84%E7%90%86"><span class="nav-text">4. 视频关键帧处理</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#5-%E5%9B%BE%E5%83%8F%E6%A0%87%E6%B3%A8%E5%B7%A5%E5%85%B7"><span class="nav-text">5. 图像标注工具</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E4%B8%89%E3%80%81%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E8%AF%84%E4%BB%B7"><span class="nav-text">三、图像质量评价</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-%E5%9F%BA%E6%9C%AC%E6%A6%82%E5%BF%B5-1"><span class="nav-text">1. 基本概念</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%B8%BB%E8%A7%82%E8%AF%84%E4%BB%B7"><span class="nav-text">1. 主观评价</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E7%BB%9D%E5%AF%B9%E8%AF%84%E4%BB%B7"><span class="nav-text">（1）绝对评价</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E7%9B%B8%E5%AF%B9%E8%AF%84%E4%BB%B7"><span class="nav-text">（2）相对评价</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%AE%A2%E8%A7%82%E8%AF%84%E4%BB%B7"><span class="nav-text">2. 客观评价</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-IQA%E8%AF%84%E4%BC%B0%E6%8C%87%E6%A0%87"><span class="nav-text">2. IQA评估指标</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E4%B8%BB%E8%A7%82%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-text">1. 主观评价指标</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E5%AE%A2%E8%A7%82%E8%AF%84%E4%BB%B7%E6%8C%87%E6%A0%87"><span class="nav-text">2. 客观评价指标</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8E%E6%95%B4%E4%BD%93%E5%9B%BE%E5%83%8F%E5%B7%AE%E5%BC%82%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8C%87%E6%A0%87%EF%BC%9A"><span class="nav-text">与整体图像差异相关的指标：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%B8%8E%E4%B8%BB%E8%A7%82%E8%A7%86%E8%A7%89%E6%84%9F%E7%9F%A5%E7%9B%B8%E5%85%B3%E7%9A%84%E6%8C%87%E6%A0%87%EF%BC%9A"><span class="nav-text">与主观视觉感知相关的指标：</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%9B%B8%E5%85%B3%E6%80%A7%E5%92%8C%E7%A6%BB%E5%87%BA%E7%8E%87%EF%BC%9A"><span class="nav-text">相关性和离出率：</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%8D%E5%90%8C%E6%96%B9%E6%B3%95%E7%9A%84%E4%BC%98%E7%BC%BA%E7%82%B9%E6%AF%94%E8%BE%83%EF%BC%9A"><span class="nav-text">不同方法的优缺点比较：</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#3-%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F%E6%A3%80%E6%B5%8B%E6%96%B9%E5%BC%8F"><span class="nav-text">3. 图像质量检测方式</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-%E5%8D%8A%E5%8F%82%E8%80%83%E6%96%B9%E6%B3%95"><span class="nav-text">1. 半参考方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-%E7%9B%B2%E5%9B%BE%E5%83%8F%E8%B4%A8%E9%87%8F"><span class="nav-text">2. 盲图像质量</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#1-%E5%9F%BA%E4%BA%8E%E5%A4%B1%E7%9C%9F%E6%A8%A1%E5%9E%8B"><span class="nav-text">1. 基于失真模型</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#2-%E5%9F%BA%E4%BA%8E%E8%87%AA%E7%84%B6%E5%9C%BA%E6%99%AF%E7%BB%9F%E8%AE%A1"><span class="nav-text">2. 基于自然场景统计</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#3-%E5%9F%BA%E4%BA%8E%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0"><span class="nav-text">3. 基于机器学习</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#4-%E5%9F%BA%E4%BA%8E%E5%86%85%E5%AE%B9"><span class="nav-text">4. 基于内容</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#3-%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84IQA%E6%A8%A1%E5%9E%8B"><span class="nav-text">3. 基于深度学习的IQA模型</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%881%EF%BC%89%E5%9F%BA%E4%BA%8E%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C"><span class="nav-text">（1）基于卷积神经网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%882%EF%BC%89%E5%9F%BA%E4%BA%8E%E7%94%9F%E6%88%90%E5%AF%B9%E6%8A%97%E7%BD%91%E7%BB%9C"><span class="nav-text">（2）基于生成对抗网络</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%EF%BC%883%EF%BC%89%E5%9F%BA%E4%BA%8ETransformer"><span class="nav-text">（3）基于Transformer</span></a></li></ol></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image" alt="米兰小铁匠"
      src="/images/avatar.jpg">
  <p class="site-author-name" itemprop="name">米兰小铁匠</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">2</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">7</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
        <a href="https://github.com/JadenGao-git" title="GitHub → https:&#x2F;&#x2F;github.com&#x2F;JadenGao-git" rel="noopener" target="_blank"><i class="fab fa-github fa-fw"></i>GitHub</a>
      </span>
  </div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2024</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">米兰小铁匠</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://mist.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Mist</a> 强力驱动
  </div>
-->

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  




  
<script src="/js/local-search.js"></script>













  

  

</body>
</html>
