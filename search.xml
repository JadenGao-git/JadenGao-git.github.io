<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>Quality-Aware Pre-Trained Models for Blind Image Quality Assessment</title>
    <url>/2024/11/07/Quality-aware%20Pre-trained%20Models%20for%20Blind%20Image%20Quality%20Assessment/</url>
    <content><![CDATA[<p><strong>作者</strong>：Kai Zhao†, Kun Yuan†, Ming Sun, Mading Li 和 Xing Wen<br>Kuaishou Technology<br>{zhaokai05, yuankun03, sunming03, limading, wenxing}@kuaishou.com</p>
<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>盲图像质量评估（Blind Image Quality Assessment, BIQA）旨在自动评估单张图像的主观质量。近年来，基于深度学习的方法提升了BIQA的表现。然而，由于标注数据的缺乏，基于深度学习的BIQA方法在某种程度上未能完全发挥其潜力。本文提出一种自监督学习方式，通过定制的预训练任务来解决该问题，从数量级上增加了可用于学习表示的数据量。为了限制学习过程，我们基于一个简单假设设计了质量感知的对比损失：来自失真图像的不同区域的质量应该相似，但在不同失真类型或不同图像之间应有所差异。此外，我们改进了现有的退化过程，构建了约2 × 10⁷大小的退化空间。利用我们的方法在ImageNet数据集上进行预训练后，模型对图像质量更加敏感，在后续的BIQA任务中表现显著提升。实验结果表明，我们的方法在多个BIQA数据集上取得了显著的改进。</p>
<span id="more"></span>

<h2 id="基础名词解释"><a href="#基础名词解释" class="headerlink" title="基础名词解释"></a>基础名词解释</h2><h3 id="1-对比学习"><a href="#1-对比学习" class="headerlink" title="1. 对比学习"></a>1. 对比学习</h3><h4 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h4><p>对比学习是无监督学习的一种，着重于学习同类实例之间的共同特征，区分非同类实例之间的不同之处。</p>
<blockquote>
<p>举个例子，从ImageNet中抽出猫、猫、狗、飞机四张图，那么猫和猫的图片肯定是相似的，和狗不相似。但是和飞机比起来，猫和狗是相似的。所以<strong>对比学习就是对比着差异去学习，模型并不需要真的知道图片中代表的是什么，而只需要知道哪些图片是类似的，哪些图片是不一样的就可以了</strong>。</p>
</blockquote>
<h4 id="训练目的"><a href="#训练目的" class="headerlink" title="训练目的"></a>训练目的</h4><p>对比学习，希望相似数据（图片）最终学到的特征是相似的，在特征空间（embedding space）中，特征向量尽量靠近；反之还希望不同的数据学到的特征向量，尽量远离。</p>
<h4 id="pretext-task（代理任务）"><a href="#pretext-task（代理任务）" class="headerlink" title="pretext task（代理任务）"></a>pretext task（代理任务）</h4><p>对比学习是不需要标签的（比如不需要知道图片是哪一类），但模型还是需要知道哪些图片是类似的，哪些是不相似的，才能训练。这就需要通过通过设计一些巧妙的代理任务，人为指定一些任务来实现。</p>
<h4 id="应用最广的代理任务：instance-discrimination"><a href="#应用最广的代理任务：instance-discrimination" class="headerlink" title="应用最广的代理任务：instance discrimination"></a>应用最广的代理任务：instance discrimination</h4><ul>
<li>简单说就是，从一堆图片中调出任意一张图片$x_i$，将其做一次转换（transformation ，比如随机裁剪等数据增广），得到新的图片$x_{i1}$、$x_{i2}$。那么<strong>样本$x_{i1}$叫做基准点（锚点），$x_{i2}$被认为是正样本</strong>（两者都是从$x_{i1}$变化得到的，虽然看起来有差异，但语义信息不应该发生变化），数据集中其它所有图片都是负样本。</li>
<li>有了正负样本的划分，就可以将数据都输入编码器进行编码提取特征了。因为所有的正负样本都是基于锚点来说的，所以$x_{i1}$会单独使用一个编码器$E_{11}$，$x_{i2}$和其它所有负样本使用另外的编码器（可以是同一个编码器，也可以也可以使用不同的编码器。但是不同的编码器之间必须相似，这样编码的特征才有一致性，才有比较的意义）。</li>
<li>对比学习就是要让正样本的编码特征和锚点的编码特征尽可能靠近（相似），让负样本的特征和锚点特征尽量远离。</li>
<li>instance discrimination直译过来就是个体判别，在这个任务中，只有经过这张图片转换的样本才是正样本，其它图片都是负样本，所以每张图都自成一类。对于ImageNet来说，就不是1000类，而是128万个类别。</li>
</ul>
<h4 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h4><p>确定了代理任务，知道如何定义正负样本之后，就需要用一个目标函数，来告诉模型该如何学习，比如常见的对比学习目标函数NCE loss等。</p>
<h4 id="特性"><a href="#特性" class="headerlink" title="特性"></a>特性</h4><p>对比学习最大的特性，是这种方法非常的灵活，可以设置各种不同的代理任务。只要找到一种方式去定义正负样本，剩下的都是一些比较标准化的流程。</p>
<h3 id="2-预训练"><a href="#2-预训练" class="headerlink" title="2. 预训练"></a>2. 预训练</h3><p>在深度学习领域，预训练（Pre-training）通常是指对模型进行<a href="https://so.csdn.net/so/search?q=%E6%97%A0%E7%9B%91%E7%9D%A3&spm=1001.2101.3001.7020">无监督</a>或自监督学习的过程，在大规模未标注数据上先训练模型，以便为后续任务提供一个高质量的初始权重。</p>
<p>预训练模型首先在诸如自然语言理解或者生成的任务上通过解决诸如掩码语言模型（Masked Language Modeling, MLM）、下一个词预测（Next Sentence Prediction, NSP）或其他自定义设计的任务来学习通用的语言表示。然后，针对特定下游任务，可以采用微调（Fine-tuning）的方式，即在预训练模型的基础上进一步训练以适应具体场景，如文本分类、情感分析、问答系统等。</p>
<p>这样做的优势在于：</p>
<ol>
<li>利用大量未标记数据中蕴含的潜在信息和模式。</li>
<li>避免从随机初始化开始训练需要大量标记数据的问题。</li>
<li>能够快速迁移学习到新的、相关但数据有限的任务中去。</li>
</ol>
<h3 id="3-图像质量检测中的正样本与负样本"><a href="#3-图像质量检测中的正样本与负样本" class="headerlink" title="3. 图像质量检测中的正样本与负样本"></a>3. 图像质量检测中的正样本与负样本</h3><p>在图像质量检测（尤其是盲图像质量评估，BIQA）任务中，<strong>正样本</strong>和<strong>负样本</strong>通常是根据图像的质量差异来定义的。具体来说：</p>
<ol>
<li><strong>正样本</strong>：指质量相似的图像对，或是相同图像的不同区域的对比，且这些区域的质量一致。例如，同一张图像中两个局部区域的质量可能相同，或是两个经过相同失真处理的图像片段。正样本的设计目的是让模型学习到相同或相似质量图像的特征相似性。</li>
<li><strong>负样本</strong>：指质量差异较大的图像对。这些可以是同一图像的不同区域但受不同失真类型或程度影响的图像片段，也可以是来自不同图像的片段。负样本的作用是帮助模型区分出质量不同的图像特征，使模型能够学会判断并“推开”质量差异较大的图像。</li>
</ol>
<p>在盲图像质量评估任务中，正负样本的定义通常基于以下几种情况：</p>
<ul>
<li><strong>同一图像不同区域</strong>：如果这些区域的质量相似，则为正样本对；如果这些区域质量差异较大（例如一个区域清晰，另一个区域模糊），则构成负样本对。</li>
<li><strong>不同失真处理</strong>：对于同一张图像，如果两个片段经过了相同或相似的失真处理，则为正样本对；而如果失真类型或程度不同，则为负样本对。</li>
<li><strong>不同图像</strong>：通常情况下，不同图像的内容和质量都有所差异，所以会被视为负样本对。不过在一些特殊情况中，例如图像内容不同但失真处理相同，这样的图像对也可能被用作正样本对，取决于模型的设计。</li>
</ul>
<p><strong>正负样本在模型中的作用</strong></p>
<ul>
<li><strong>正样本对</strong>：模型被鼓励在特征空间中将这些相似质量的样本拉近，学习到类似质量的图像特征。</li>
<li><strong>负样本对</strong>：模型被训练将这些不同质量的图像推远，使得模型在特征空间中更清楚地分离出高质量和低质量图像特征。</li>
</ul>
<p>这种正负样本对比的设计帮助模型捕捉图像质量的细微差别，从而更准确地评估图像的整体质量。</p>
<h2 id="1-引言"><a href="#1-引言" class="headerlink" title="1. 引言"></a>1. 引言</h2><p>在移动互联网时代，每天有数十亿张图像被生成、上传并在Twitter、TikTok等社交媒体平台上共享。图像质量作为一个重要指标，帮助服务提供商筛选和传递高质量图像，从而提高用户体验质量（Quality of Experience）。因此，已有大量研究致力于建立一种与人类观感一致的图像质量评估（IQA）方法。在现实场景中，通常无法获取参考图像，或参考图像的质量本身也可能存在问题。因此，相较于全参考（Full-Reference）IQA已取得预期成果，盲IQA（BIQA）方法因其实用性和适用性更具吸引力。</p>
<p>最近，基于深度学习的BIQA方法在IQA领域的表现取得了巨大进展。然而，由于&#x3D;&#x3D;标注数据的缺乏&#x3D;&#x3D;，问题依然未得到彻底解决。现有最大的BIQA数据集FLIVE包含近40,000张真实世界的失真图像，而著名的CIFAR-100数据集则包含60,000张标注图像。现有的BIQA数据集数量过少，无法有效训练深度学习模型。</p>
<p>研究人员提出了一些方法来应对这一挑战。一个直接的方法是对局部图像区域进行采样，并为这些区域赋予整个图像的均值意见评分（MOS）。然而，局部图像区域的感知评分往往不同于整体图像的评分。另一种常见策略是利用大规模数据集（例如ImageNet）的领域知识。然而，这些预训练模型在BIQA任务中的效果可能不尽如人意，因为相同内容的图像具有相同的语义标签，但其质量可能不同。一些研究人员提出在合成图像上训练模型，然后将其应用于小规模目标BIQA数据集。然而，由简单退化过程生成的图像难以模拟真实的失真效果。</p>
<p>自监督学习（SSL）或无监督学习可以解决标注数据不足的问题，因为它可以利用大量未标记的数据。与其他视觉任务主要关注高层次信息的模型不同，<u>BIQA任务需要学习对各类低层次失真、高层次内容及其之间交互的敏感表示</u>。本文提出一种新颖的SSL机制，<u>通过区分不同感知质量的样本来生成用于后续BIQA任务的质量感知预训练（QPT）模型。</u>具体来说，我们假设来自同一失真图像的区域应具有相似的质量，但与其他图像的不同区域质量应不同。此外，我们引入了图像恢复领域的最新进展，如打乱顺序、高阶退化和跳过操作，以模拟真实的失真效果。通过在ImageNet数据集上进行预训练，我们期望模型能够提取质量感知特征，从而提升BIQA任务的表现。 </p>
<blockquote>
<p>BIQA模型必须对<strong>低层次的失真细节</strong>（如像素级的噪声或模糊）和<strong>高层次的语义内容</strong>（如整个场景或物体）都敏感，并且能够理解这些不同层次信息之间的交互关系。</p>
<p>自监督学习（SSL）机制不依赖人工标注，而是通过设计一种自动区分不同图像质量的任务来训练模型，使其能够更敏感地识别图像的质量差异。本文的方法称为“质量感知预训练（QPT）模型”，这是为了使模型在后续的BIQA任务中更好地评估图像质量。</p>
</blockquote>
<h2 id="2-相关工作"><a href="#2-相关工作" class="headerlink" title="2. 相关工作"></a>2. 相关工作</h2><h3 id="2-1-盲图像质量评估"><a href="#2-1-盲图像质量评估" class="headerlink" title="2.1 盲图像质量评估"></a>2.1 盲图像质量评估</h3><p>在深度学习兴起之前，自然场景统计（NSS）理论主导了BIQA领域，假设完美自然图像服从一定的统计分布，不同类型的失真会打破这种统计规律。基于该理论，不同领域提出了各种手工设计的特征，包括空间、频率和梯度领域的特征。同时，一些基于学习的方法也得到了初步探索，通常使用支持向量回归来预测主观质量。</p>
<p>近年来，各类基于深度学习的BIQA方法显著提升了在真实场景数据集上的表现。最早的研究之一是使用三层的浅层网络来解决BIQA问题。之后，研究自然扩展到通过加深网络深度或使用更有效的构建模块来增强模型的表现。近期，基于Transformer的BIQA方法逐渐增多，基于假设Transformer能够补偿卷积神经网络在捕获非局部信息方面的不足。</p>
<p>除了精细的模型设计外，部分研究致力于解决BIQA中的主要障碍——标注数据的匮乏。一些方法尝试充分利用现有的监督信号，例如排序学习、多任务学习和混合数据集训练。同时，其他研究转向大规模预训练，生成大量失真图像。&#x3D;&#x3D;相比之下，本文提出的方法基于对比学习，无需显式的监督信号，从而充分利用了大量真实世界的图像。&#x3D;&#x3D;</p>
<h3 id="2-2-自监督学习"><a href="#2-2-自监督学习" class="headerlink" title="2.2 自监督学习"></a>2.2 自监督学习</h3><p>SSL是一种无监督学习形式，用于为下游任务学习良好的数据表示。进行此类学习的一个简单思路是最小化模型输出与固定目标之间的差异，例如重建输入像素或预测预定义类别。受到<u>自然语言处理中的掩码语言建模</u>的成功启发，掩码图像建模在计算机视觉领域中逐渐流行。</p>
<blockquote>
<p>掩码语言建模（Masked Language Modeling, MLM）是一种自然语言处理（NLP）任务，常用于自监督学习。它通过让模型在不完全可见的文本上进行预测来学习词汇、语法和上下文等语言特征。MLM是BERT（Bidirectional Encoder Representations from Transformers）等模型的核心训练方式。</p>
<p>在掩码语言建模中，给定一段文本，模型会随机选择其中一部分词语进行掩码，通常用一个特殊的“[MASK]”符号替代。例如，对于句子“我今天去上学”，模型可能会将“今天”掩码掉，即“我[MASK]去上学”。接着，模型的任务是基于上下文预测掩码词“今天”。</p>
</blockquote>
<p>对比学习的目标是在嵌入空间中，聚集相似样本，排斥不相似的样本。具体来说，对比学习通常涉及预训练任务和训练目标。得益于对比学习的灵活性，已提出广泛的预训练任务，例如图像上色、多视角编码和实例辨别。训练目标的主要趋势是从单一正样本和负样本扩展到多对正负样本对比。值得注意的是，上述工作都属于语义感知预训练，因为它们鼓励相同图像的不同视角具有相似的表示，忽略了图像质量的变化。&#x3D;&#x3D;本文为BIQA重新设计了一种质量感知的预训练任务。&#x3D;&#x3D;</p>
<h3 id="2-3-图像退化建模"><a href="#2-3-图像退化建模" class="headerlink" title="2.3 图像退化建模"></a>2.3 图像退化建模</h3><p>大多数现有工作专注于几个经典的失真类型，以提取失真特定的特征。随着BIQA重要性的增加，越来越多的失真类型被开发出来并应用于合成失真图像。DipIQ总结了常见的失真操作，如噪声、模糊和压缩，并进一步细分了每种操作的五个等级。DB-CNN引入了额外的五种失真类型。最近，许多研究指出，上述方法在模拟复杂的真实图像失真方面存在局限性。&#x3D;&#x3D;本文结合图像恢复领域的多种退化手法，形成了一个更大的退化空间，以生成更贴近真实的失真图像。&#x3D;&#x3D;</p>
<h2 id="3-方法"><a href="#3-方法" class="headerlink" title="3. 方法"></a>3. 方法</h2><p>为了解决BIQA任务中“预训练-微调”模式的潜力，我们通过在ImageNet上进行预训练来生成QPT模型。以下是具体的方法步骤：</p>
<h3 id="3-1-自监督学习框架回顾"><a href="#3-1-自监督学习框架回顾" class="headerlink" title="3.1 自监督学习框架回顾"></a>3.1 自监督学习框架回顾</h3><p>SSL通过利用大规模数据来学习表示，从而解决BIQA缺乏大规模标注数据的障碍。SSL方法大致分为生成式和对比式。MoCo作为对比学习的一种方法，通过使用队列和滑动平均编码器构建动态字典，用于对比学习。由于图像质量受到多种复杂因素的影响，对比学习更适合BIQA任务，因为它可以轻松测量样本之间的质量排序。</p>
<blockquote>
<p>MoCo（Momentum Contrast for Unsupervised Visual Representation Learning）是一种无监督对比学习方法，主要用于图像特征的自监督学习。MoCo通过设计一种动态字典和基于动量的更新机制，使模型能够在大规模数据集上进行高效对比学习。MoCo在图像分类、目标检测等任务中表现良好，尤其在无监督学习中取得了重要的进展。</p>
<p>MoCo的核心概念和实现机制如下：</p>
<ol>
<li><p><strong>动态字典</strong>：MoCo构建了一个动态更新的特征字典，以便模型可以在对比学习中从更大的样本池中进行选择。传统的对比学习依赖于同一个小批次的数据来构建正样本和负样本对，这会限制负样本的数量。而MoCo使用了一个大的、基于队列的字典，可以不断地存入新的样本并逐渐替换旧样本，从而提高负样本的多样性和数量。</p>
</li>
<li><p><strong>动量编码器</strong>：MoCo通过两个编码器来提取图像特征：查询编码器（query encoder）和键编码器（key encoder）。查询编码器直接使用主模型的参数进行更新，而键编码器的参数则通过动量更新方式，从查询编码器的参数缓慢复制过来。具体而言，键编码器的参数是通过如下动量更新公式进行更新的：<br>$$<br>\theta_k &#x3D; m \cdot \theta_k + (1 - m) \cdot \theta_q<br>$$<br>其中，$\theta_k$ 和$\theta_q$ 分别表示键编码器和查询编码器的参数，$m$ 是动量系数，通常接近于1（如0.999），以保证键编码器的更新缓慢、稳定。这种设计能够在对比学习中保持编码的一致性，减小由于模型参数快速更新带来的差异性影响。</p>
</li>
<li><p><strong>对比学习目标（InfoNCE损失）</strong>：MoCo的目标是将相似的样本（正样本对）在特征空间中拉近，而将不相似的样本（负样本对）推远。具体地，MoCo使用了对比损失（InfoNCE Loss），通过拉近查询样本和其正样本的距离，推远与负样本的距离。损失函数的计算公式如下：<br>$$<br>L &#x3D; - \log \frac{\exp(q \cdot k_+ &#x2F; \tau)}{\sum_{i&#x3D;0}^{N} \exp(q \cdot k_i &#x2F; \tau)}<br>$$<br>其中，$q$是查询样本的特征向量，$k_+$ 是对应的正样本向量，$k_i$ 是负样本，$\tau$ 是温度参数，用于调节对比损失的平滑程度。</p>
</li>
<li><p><strong>对比学习过程</strong>：在训练过程中，MoCo会不断地从输入数据中生成新的查询样本和键样本，并通过动量编码器构建更新动态字典。由于字典的大小相对于小批次数据来说要大得多，所以MoCo能够提供大量的负样本，从而提升模型的特征表示能力。</p>
</li>
</ol>
<p><strong>MoCo的优势：</strong></p>
<ul>
<li><strong>有效利用负样本</strong>：MoCo的动态字典设计，使其能够有效地利用大量的负样本，提升了特征学习的效果。</li>
<li><strong>保持一致性</strong>：动量更新机制保证了编码器的稳定性，减小了快速更新导致的特征不一致性问题。</li>
<li><strong>拓展性强</strong>：MoCo易于扩展到更大的数据集或更复杂的任务中，能够在没有标注数据的情况下学习到有意义的特征表示。</li>
</ul>
</blockquote>
<h3 id="3-2-退化空间"><a href="#3-2-退化空间" class="headerlink" title="3.2 退化空间"></a>3.2 退化空间</h3><p>为在自监督场景中获得可利用的质量相关信息，一种简单的方法是手动生成包含可控失真的图像对。在设计具体的失真类型之前，有几点观察需要提到：首先，感知质量受多种因素影响，如内容、失真、压缩等。其次，这些因素在实际场景中往往以复杂组合形式存在。为应对上述观察，本文从个体操作及其组合角度设计了退化空间，设计了包括&#x3D;&#x3D;几何变形、色彩变化和纹理调整&#x3D;&#x3D;的多种失真类型。通过随机选择失真类型和顺序，我们引入了打乱、跳过等操作，以进一步扩展退化空间。</p>
<h3 id="3-3-质量感知预训练任务"><a href="#3-3-质量感知预训练任务" class="headerlink" title="3.3 质量感知预训练任务"></a>3.3 质量感知预训练任务</h3><p>不同于经典的语义感知预训练任务，本文提出了一种新的质量感知预训练任务，通过区分不同感知质量的样本来学习质量信息。每张图像经过多种失真操作生成多个视图，每个视图提取不同位置的补丁形成正样本对，而其他对则为负样本。最终，通过设计质量感知对比损失，模型能够有效学习质量相关的信息。</p>
<blockquote>
<p>传统的<strong>语义感知预训练任务</strong>主要关注图像的语义内容，比如物体类别或场景类型，而不关注图像的细节质量（例如清晰度、噪声、对比度等）。在语义感知任务中，模型更关注“这张图片是什么”（如猫、狗、风景），而不是“这张图片的质量如何”。然而，在图像质量评估任务中，仅依赖语义信息是不够的。图像质量评估需要模型能够检测到图像的视觉质量细节，比如是否模糊、噪声程度、压缩失真等。因此，<strong>质量感知预训练任务</strong>专注于让模型学习如何识别图像的质量差异，而不是图像的内容类别。</p>
<ul>
<li><strong>语义感知任务的局限性</strong>：语义感知任务仅让模型学习到物体的类别信息，而不会关注图像的视觉质量。这种预训练模式不适合需要对图像细节敏感的任务，比如盲图像质量评估（BIQA），因为图像质量的好坏和图像的语义类别无关。</li>
<li><strong>质量感知预训练任务的核心</strong>：论文提出了一种“质量感知预训练任务”，通过区分图像的<strong>质量差异</strong>来训练模型。例如，模型会学会将高质量的图像片段和低质量的图像片段区分开，甚至能够理解同一张图像中不同失真类型或不同退化程度的差异。这使得模型可以从图像的细节中识别出质量特征，而不是简单地学习“这是猫还是狗”这样的语义信息。</li>
</ul>
</blockquote>
<h2 id="4-实验"><a href="#4-实验" class="headerlink" title="4. 实验"></a>4. 实验</h2><h3 id="4-1-数据集和评价标准"><a href="#4-1-数据集和评价标准" class="headerlink" title="4.1 数据集和评价标准"></a>4.1 数据集和评价标准</h3><p>实验在五个公共BIQA数据集上进行，包括BID、CLIVE、KonIQ10K、SPAQ和FLIVE数据集。评价指标采用Pearson线性相关系数（PLCC）和Spearman等级相关系数（SRCC），分别用于衡量预测结果与主观评分的准确度和一致性。</p>
<h3 id="4-2-实现细节"><a href="#4-2-实现细节" class="headerlink" title="4.2 实现细节"></a>4.2 实现细节</h3><p>实验使用PyTorch在8个NVIDIA V100 GPU上进行。整个实验分为预训练和微调两个阶段。在预训练阶段，我们在ImageNet数据集上使用ResNet-50模型进行训练。在微调阶段，对不同数据集的实验分别进行100到200个epoch的训练。</p>
<h3 id="4-3-与现有最优方法的比较"><a href="#4-3-与现有最优方法的比较" class="headerlink" title="4.3 与现有最优方法的比较"></a>4.3 与现有最优方法的比较</h3><p>实验结果表明，QPT模型显著提升了当前BIQA任务的表现。与传统方法相比，QPT模型在多个数据集上取得了更高的SRCC和PLCC值，并且该模型可以无缝集成到现有最优方法中，以进一步提升性能。</p>
<h3 id="4-4-消融研究"><a href="#4-4-消融研究" class="headerlink" title="4.4 消融研究"></a>4.4 消融研究</h3><p>通过消融实验，我们研究了数据量、编码器容量和负样本组成对QPT模型性能的影响。结果表明，数据量和编码器容量的增加均有助于提升下游任务的表现，且不同负样本组合对模型性能有显著影响。</p>
<h3 id="4-5-与其他预训练任务的比较"><a href="#4-5-与其他预训练任务的比较" class="headerlink" title="4.5 与其他预训练任务的比较"></a>4.5 与其他预训练任务的比较</h3><p>我们进一步将QPT与其他预训练任务（包括从头训练、监督学习和MoCo）进行对比。结果显示，QPT在BIQA场景中比其他任务表现更优，特别是在线性探测和端到端微调实验中均取得了最高的表现。</p>
<h2 id="5-结论"><a href="#5-结论" class="headerlink" title="5. 结论"></a>5. 结论</h2><p>本文提出了用于下游BIQA任务的QPT模型，通过引入多种退化类型和组合，构建了包含2 × 10⁷种可能退化的退化空间，以模拟更复杂且真实的失真图像。通过质量感知对比损失，模型可以学习质量相关的信息。实验结果表明，QPT在多个BIQA基准数据集上取得显著改进，并且可以轻松集成到现有最优方法中，表现出良好的泛化能力。</p>
<p>了解更多：<br><a href="https://www.nxrte.com/jishu/16352.html">https://www.nxrte.com/jishu/16352.html</a></p>
]]></content>
      <categories>
        <category>基于深度学习的图像处理</category>
      </categories>
      <tags>
        <tag>图像质量评价</tag>
        <tag>QPT</tag>
      </tags>
  </entry>
  <entry>
    <title>基于深度学习的图像处理基本知识</title>
    <url>/2024/11/08/%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86/</url>
    <content><![CDATA[<p>参考：</p>
<p><strong>《【深度学习】一文搞懂卷积神经网络（CNN）的原理（超详细）》</strong>：<a href="https://blog.csdn.net/AI_dataloads/article/details/133250229">https://blog.csdn.net/AI_dataloads/article/details/133250229</a></p>
<span id="more"></span>

<h1 id="一、须知"><a href="#一、须知" class="headerlink" title="一、须知"></a>一、须知</h1><h2 id="1-深度卷积神经网络"><a href="#1-深度卷积神经网络" class="headerlink" title="1. 深度卷积神经网络"></a>1. 深度卷积神经网络</h2><p>深度卷积神经网络（Deep Convolutional Neural Networks，DCNN）是一种深度学习模型，特别适用于图像处理和计算机视觉任务。它是卷积神经网络（Convolutional Neural Networks，CNN）的一个扩展，包含多层卷积和其他操作。</p>
<h3 id="（1）什么是卷积"><a href="#（1）什么是卷积" class="headerlink" title="（1）什么是卷积"></a>（1）什么是卷积</h3><p>在卷积神经网络中，卷积操作是指将一个可移动的小窗口（称为数据窗口）与图像进行逐元素相乘然后相加的操作。这个小窗口其实是一组固定的权重，它可以被看作是一个特定的滤波器（filter）或卷积核。这个操作的名称“卷积”，源自于这种元素级相乘和求和的过程。这一操作是卷积神经网络名字的来源。</p>
<blockquote>
<p><strong>卷积核</strong>：数据窗口中的权重集合称为卷积核（或滤波器），它是网络在训练过程中学习得到的。不同的卷积核可以提取不同类型的特征，比如边缘、纹理等。</p>
<p><strong>特征提取</strong>：通过多个卷积层，CNN可以从简单的特征（如边缘）逐步提取出更复杂的特征（如形状、对象等），这使得CNN在图像处理、计算机视觉等领域表现出色。</p>
</blockquote>
<h3 id="（2）卷积核心问题"><a href="#（2）卷积核心问题" class="headerlink" title="（2）卷积核心问题"></a>（2）卷积核心问题</h3><ol>
<li><strong>步长（stride）</strong>：每次滑动的位置步长。</li>
<li><strong>卷积核的个数</strong>：决定输出的depth厚度。同时代表卷积核的个数。</li>
<li><strong>填充值（zero-padding）</strong>：在外围边缘补充若干圈0，方便从初始位置以步长为单位可以刚好滑倒末尾位置，通俗地讲就是为了总长能被步长整除。</li>
</ol>
<h3 id="（3）卷积神经网络模型"><a href="#（3）卷积神经网络模型" class="headerlink" title="（3）卷积神经网络模型"></a>（3）卷积神经网络模型</h3><p>卷积神经网络（CNN）通常由多个层组成，每一层负责不同的功能。一个典型的CNN模型结构如下：</p>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/%E5%8D%B7%E7%A7%AF%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E6%9E%84%E6%88%90-9490606.png" alt="卷积神经网络的构成-9490606"></p>
<h4 id="1-输入层"><a href="#1-输入层" class="headerlink" title="1. 输入层"></a>1. 输入层</h4><p>接收原始输入数据，图像通常由三个颜色通道（红、绿、蓝）组成，形成一个二维矩阵，表示像素的强度值。输入层的形状取决于图像的大小和通道数（例如，224x224x3的图像表示宽224像素，高224像素，3个颜色通道）。</p>
<img src="./基于深度学习的图像处理基本知识.assets/image-20241021140903850.png" alt="image-20241021140903850" style="zoom:50%;" />

<h4 id="2-卷积层"><a href="#2-卷积层" class="headerlink" title="2. 卷积层"></a>2. 卷积层</h4><p>提取输入图像中的特征。每个卷积层都有多个卷积核（滤波器），这些滤波器在输入上滑动，执行卷积操作。生成特征图（feature maps），每个特征图对应一个卷积核。</p>
<h4 id="3-激活层"><a href="#3-激活层" class="headerlink" title="3. 激活层"></a>3. 激活层</h4><p>通常使用非线性激活函数（如ReLU）应用于卷积层的输出，以引入非线性特性，使网络能够学习更复杂的模式。</p>
<blockquote>
<p><strong>激活函数</strong>是神经网络中的一个重要组成部分，其主要作用是在每一层引入非线性特性，使得网络能够学习和拟合复杂的模式和数据。没有激活函数，神经网络的每一层都只是进行线性变换，无法捕捉到数据中的非线性关系。</p>
<p><strong>常见的激活函数</strong></p>
<p><strong>Sigmoid 函数</strong>：$σ(x)&#x3D; \frac{1}{1 + e^{-x}}$，输出值在 (0, 1) 之间</p>
<ul>
<li><strong>优点</strong>：平滑且可微，适合于二分类任务。</li>
<li><strong>缺点</strong>：在输入较大或较小的时候，梯度会变得非常小，导致梯度消失问题。</li>
</ul>
<p><strong>Tanh 函数（双曲正切函数）</strong>：$tanh(x)&#x3D;\frac{e^{x}-e^{−x}}{e^{x}+e^{−x}}$​，输出值在 (-1, 1) 之间</p>
<ul>
<li><strong>优点</strong>：输出范围更广，相较于Sigmoid具有更好的性能。</li>
<li><strong>缺点</strong>：同样存在梯度消失问题，特别是在深层网络中。</li>
</ul>
<p><strong>ReLU 函数（修正线性单元）</strong>：$ReLU(x)&#x3D;max(0,x)$​，输出值在 [0, ∞) 之间</p>
<ul>
<li><strong>优点</strong>：计算简单且高效，在正区间内有较大的梯度，减少了梯度消失问题，有助于加快收敛速度。</li>
<li><strong>缺点</strong>：在负区间内，输出始终为0，可能导致“神经元死亡”问题，即某些神经元在训练过程中变得永远不激活。</li>
</ul>
<p><strong>Leaky ReLU 函数</strong><br>$$<br>Leaky\ ReLU(x)&#x3D; \begin{cases}<br>x &amp; if\ x&gt;0 \<br>ax &amp; if\ x \leq 0<br>\end{cases}\ （通常取𝛼&#x3D;0.01）<br>$$</p>
<ul>
<li><strong>优点</strong>：在负区间也有一个小的斜率，减少了“神经元死亡”问题。</li>
</ul>
<p><strong>Softmax 函数</strong>，$Softmax(xi)&#x3D;\frac{e^{x_i}}{\sum_{j} e^{x_j}}$​，将输出值转化为概率分布，常用于多分类任务的输出层。</p>
<ul>
<li><strong>优点</strong>：保证输出值在 (0, 1) 之间且和为1，适合于分类任务。</li>
</ul>
</blockquote>
<h4 id="4-池化层（下采样层）"><a href="#4-池化层（下采样层）" class="headerlink" title="4. 池化层（下采样层）"></a>4. 池化层（下采样层）</h4><p>减少特征图的尺寸，从而减小计算量并降低过拟合的风险。常用的池化方法有最大池化（max pooling）和平均池化（average pooling）。池化层的输出是更小的特征图。</p>
<blockquote>
<p><strong>过拟合（Overfitting）</strong>是机器学习和深度学习中的一个常见问题，指的是模型在训练数据上表现很好，但在未见过的测试数据或验证数据上表现较差的现象。过拟合发生在模型学习到了训练数据中的噪声和细节，而不仅仅是数据中的潜在模式。这使得模型在新数据上的泛化能力下降。模型复杂度过高，训练数据量不足，特征过多都会导致过拟合。</p>
<p><strong>最大池化（Max Pooling）</strong> 是一种下采样操作，常用于卷积神经网络（CNN）中，以减少特征图的尺寸和计算量，同时保留重要的特征信息。在最大池化中，特征图被划分为若干个小的区域（通常是 2×2 或 3×3 的窗口），然后对每个区域内的像素值取最大值。这一过程可以理解为在每个窗口中选择最显著的特征，其他信息则被丢弃</p>
</blockquote>
<h4 id="5-全连接层"><a href="#5-全连接层" class="headerlink" title="5. 全连接层"></a>5. 全连接层</h4><p>将前面层提取到的特征进行整合，进行分类或回归。全连接层将所有输入连接到输出，通常在网络的最后部分。输出通常是一个概率分布（使用softmax激活）或连续值（用于回归任务）。</p>
<h4 id="6-输出层"><a href="#6-输出层" class="headerlink" title="6. 输出层"></a>6. 输出层</h4><p>输出最终的预测结果，比如分类标签或回归值。</p>
<h1 id="二、图像目标检测"><a href="#二、图像目标检测" class="headerlink" title="二、图像目标检测"></a>二、图像目标检测</h1><p>参考：</p>
<p><strong>《目标检测（Object Detection）》</strong>：<a href="https://blog.csdn.net/yegeli/article/details/109861867">https://blog.csdn.net/yegeli/article/details/109861867</a></p>
<h2 id="1-基本概念"><a href="#1-基本概念" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>目标检测（Object Detection）的任务是找出图像中所有感兴趣的目标（物体），确定它们的类别和位置，是计算机视觉领域的核心问题之一。由于各类物体有不同的外观、形状和姿态，加上成像时光照、遮挡等因素的干扰，目标检测一直是计算机视觉领域最具有挑战性的问题。</p>
<h3 id="1-核心问题"><a href="#1-核心问题" class="headerlink" title="1. 核心问题"></a>1. 核心问题</h3><ol>
<li>分类问题：即图片（或某个区域）中的图像属于哪个类别。</li>
<li>定位问题：目标可能出现在图像的任何位置。</li>
<li>大小问题：目标有各种不同的大小。</li>
<li>形状问题：目标可能有各种不同的形状。</li>
</ol>
<h3 id="2-目标检测算法分类"><a href="#2-目标检测算法分类" class="headerlink" title="2. 目标检测算法分类"></a>2. 目标检测算法分类</h3><p>基于深度学习的目标检测算法主要分为两类：Two stage, One stage。</p>
<h4 id="Tow-Stage"><a href="#Tow-Stage" class="headerlink" title="Tow Stage"></a>Tow Stage</h4><p>先进行区域生成，该区域称之为region proposal（简称RP，一个有可能包含待检物体的预选框），再通过卷积神经网络进行样本分类。</p>
<p>任务流程：特征提取 –&gt; 生成RP –&gt; 分类&#x2F;定位回归。</p>
<p>常见tow stage目标检测算法有：R-CNN、SPP-Net、Fast R-CNN、Faster R-CNN和R-FCN等。</p>
<h4 id="One-Stage"><a href="#One-Stage" class="headerlink" title="One Stage"></a>One Stage</h4><p>不用RP，直接在网络中提取特征来预测物体分类和位置。</p>
<p>任务流程：特征提取–&gt; 分类&#x2F;定位回归。</p>
<p>常见的one stage目标检测算法有：OverFeat、YOLOv1、YOLOv2、YOLOv3、SSD和RetinaNet等。</p>
<h2 id="2-目标检测原理"><a href="#2-目标检测原理" class="headerlink" title="2. 目标检测原理"></a>2. 目标检测原理</h2><h3 id="1-候选区域产生"><a href="#1-候选区域产生" class="headerlink" title="1. 候选区域产生"></a>1. 候选区域产生</h3><p>很多目标检测技术都会涉及候选框（bounding boxes）的生成，物体候选框获取当前主要使用图像分割与区域生长技术。区域生长(合并)主要由于检测图像中存在的物体具有局部区域相似性(颜色、纹理等)。目标识别与图像分割技术的发展进一步推动有效提取图像中信息。</p>
<h4 id="（1）滑动窗口"><a href="#（1）滑动窗口" class="headerlink" title="（1）滑动窗口"></a>（1）滑动窗口</h4><p>首先对输入图像进行不同窗口大小的滑窗进行从左往右、从上到下的滑动。每次滑动时候对当前窗口执行分类器(分类器是事先训练好的)。如果当前窗口得到较高的分类概率，则认为检测到了物体。对每个不同窗口大小的滑窗都进行检测后，会得到不同窗口检测到的物体标记，这些窗口大小会存在重复较高的部分，最后采用非极大值抑制(Non-Maximum Suppression, NMS)的方法进行筛选。最终，经过NMS筛选后获得检测到的物体。</p>
<h4 id="（2）选择性选择"><a href="#（2）选择性选择" class="headerlink" title="（2）选择性选择"></a>（2）选择性选择</h4><p>选择搜索算法的主要思想：图像中物体可能存在的区域应该是有某些相似性或者连续性区域的。因此，选择搜索基于上面这一想法采用子区域合并的方法进行提取bounding boxes。首先，对输入图像进行分割算法产生许多小的子区域。其次，根据这些子区域之间相似性(相似性标准主要有颜色、纹理、大小等等)进行区域合并，不断的进行区域迭代合并。每次迭代过程中对这些合并的子区域做bounding boxes(外切矩形)，这些子区域外切矩形就是通常所说的候选框。</p>
<h3 id="2-数据表示"><a href="#2-数据表示" class="headerlink" title="2. 数据表示"></a>2. 数据表示</h3><h5 id="输入图像"><a href="#输入图像" class="headerlink" title="输入图像"></a><strong>输入图像</strong></h5><ul>
<li><strong>图像格式</strong>：输入通常是RGB或灰度图像，可能具有不同的分辨率和尺寸。</li>
<li><strong>预处理</strong>：在将图像输入到模型之前，通常需要进行预处理，例如缩放、归一化、数据增强等。</li>
</ul>
<h5 id="边界框（Bounding-Box）"><a href="#边界框（Bounding-Box）" class="headerlink" title="边界框（Bounding Box）"></a><strong>边界框（Bounding Box）</strong></h5><p>边界框用于表示图像中目标的位置，通常用一个矩形框表示。每个边界框通常由以下信息表示：</p>
<ul>
<li>坐标：<ul>
<li>中心坐标 (x,y)(x, y)(x,y)：框中心的横纵坐标。</li>
<li>宽度（width）和高度（height）：边界框的尺寸。</li>
<li>左上角坐标 (xmin,ymin)(x_{min}, y_{min})(xmin,ymin) 和右下角坐标 (xmax,ymax)(x_{max}, y_{max})(xmax,ymax)：表示框的四个边界。</li>
</ul>
</li>
<li><strong>置信度（Confidence）</strong>：边界框的置信度表示该框中存在目标的概率，通常是一个介于0和1之间的值。</li>
</ul>
<h5 id="类别标签（Class-Label）"><a href="#类别标签（Class-Label）" class="headerlink" title="类别标签（Class Label）"></a><strong>类别标签（Class Label）</strong></h5><p>每个检测到的目标都有一个对应的类别标签，用于标识目标的类型。例如，常见的类别标签包括“人”、“车”、“动物”等。</p>
<h5 id="网格（Grid）"><a href="#网格（Grid）" class="headerlink" title="网格（Grid）"></a><strong>网格（Grid）</strong></h5><p>在一些目标检测算法（如YOLO）中，输入图像被划分为一个SxS的网格。每个网格负责预测它所包含的目标的边界框和类别概率。这种方式能够将整个图像的处理并行化，提高检测速度。</p>
<h5 id="输出格式"><a href="#输出格式" class="headerlink" title="输出格式"></a><strong>输出格式</strong></h5><p>目标检测模型的输出通常是一个集合，包含多个边界框及其对应的类别标签和置信度。常见的输出格式包括：</p>
<ul>
<li><strong>边界框集合</strong>：每个边界框由坐标、置信度和类别标签组成。</li>
<li><strong>非极大抑制（Non-Maximum Suppression, NMS）</strong>：在后处理阶段，NMS用于去除重复检测（即多个边界框检测到同一目标），保留最佳的边界框。</li>
</ul>
<h5 id="数据集格式"><a href="#数据集格式" class="headerlink" title="数据集格式"></a><strong>数据集格式</strong></h5><p>训练目标检测模型的数据集通常包含图像文件及其对应的标注文件。常见的标注格式包括：</p>
<ul>
<li><strong>Pascal VOC</strong>：使用XML格式，包含图像的边界框坐标和类别信息。</li>
<li><strong>COCO</strong>：使用JSON格式，包含丰富的目标信息，包括边界框、类别、分割掩码等。</li>
<li><strong>YOLO格式</strong>：每个图像有一个对应的文本文件，列出每个目标的类别和边界框坐标（相对于图像尺寸的比例）。</li>
</ul>
<h3 id="3-效果评估"><a href="#3-效果评估" class="headerlink" title="3. 效果评估"></a>3. 效果评估</h3><p>使用<strong>交并比（Intersection over Union, IoU）</strong>来判断模型的好坏。所谓交并比，是指预测边框、实际边框交集和并集的比率，一般约定0.5为一个可以接收的值</p>
<blockquote>
<p>一些名词解释：</p>
<p><strong>特征图（Feature Map）</strong> 是卷积神经网络（CNN）中的重要概念，表示网络在处理图像时提取的特征信息。具体而言，输入图像经过一层或多层卷积和池化操作后，生成的矩阵就是特征图。特征图包含了输入图像的空间信息（如边缘、纹理、物体轮廓等）和语义信息（如某类物体的特征），它浓缩了原始图像的信息并且具有更小的尺寸。特征图的每个通道反映不同的特征，这些特征有助于后续的分类、检测或分割任务。</p>
<p><strong>双线性插值（Bilinear Interpolation）</strong> 是一种在二维空间中进行数据插值的算法，用于在已知点之间估算未知点的值。这种方法通常用于图像处理、图像缩放和图像变形等场景。</p>
<p><strong>非极大值抑制（Non-Maximum Suppression, NMS）</strong> 是一种后处理技术，通常用于目标检测任务中，以减少重叠的候选框（bounding boxes），从而保留最优的检测结果。NMS 的主要目的是通过选择最具代表性的边界框，来提高检测的精确性和清晰度。</p>
</blockquote>
<h2 id="3-目标检测模型"><a href="#3-目标检测模型" class="headerlink" title="3. 目标检测模型"></a>3. 目标检测模型</h2><h3 id="1-R-CNN系列"><a href="#1-R-CNN系列" class="headerlink" title="1. R-CNN系列"></a>1. R-CNN系列</h3><h4 id="（1）R-CNN"><a href="#（1）R-CNN" class="headerlink" title="（1）R-CNN"></a>（1）R-CNN</h4><p><strong>R-CNN（Regions with Convolutional Neural Networks）</strong>是目标检测领域的重要算法，最早由Ross Girshick等人在2013年提出。R-CNN的核心思想是将目标检测问题分解为两个部分：</p>
<ol>
<li><strong>候选区域生成</strong>：使用“选择性搜索”方法从输入图像中生成约2000个候选区域（region proposals），这些区域可能包含目标。</li>
<li><strong>特征提取与分类</strong>：对于每个候选区域，应用卷积神经网络（CNN）提取特征，随后使用支持向量机（SVM）进行目标分类。</li>
</ol>
<h5 id="详细步骤："><a href="#详细步骤：" class="headerlink" title="详细步骤："></a>详细步骤：</h5><ol>
<li><strong>候选区域生成</strong>：R-CNN使用选择性搜索算法对输入图像进行分割，生成一系列潜在的候选框，每个框表示一个可能包含物体的区域。选择性搜索通过利用图像的颜色、纹理、大小等信息生成这些区域。这些区域可能会包含冗余，数量通常为数千个，但可以减少处理时间相对于处理整个图像。</li>
<li><strong>特征提取</strong>：R-CNN使用卷积神经网络（CNN）来处理每个候选区域。为了匹配CNN输入的固定大小，R-CNN首先将每个候选区域进行缩放（warp）至固定尺寸（例如224×224），然后将这些缩放后的区域输入到一个经过预训练的CNN中（如AlexNet）提取特征。</li>
<li><strong>分类</strong>：对于每个候选区域提取出的特征，R-CNN使用支持向量机（SVM）进行分类，判断区域是否属于某个特定的类别。同时还会通过边界框回归（bounding box regression）来微调候选框的位置，使其与实际物体更匹配。</li>
</ol>
<h5 id="缺点"><a href="#缺点" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>重复计算，每个region proposal，都需要经过一个AlexNet特征提取，为所有的RoI（region of interest）提取特征大约花费47秒，占用空间</li>
<li>selective search方法生成region proposal，对一帧图像，需要花费2秒</li>
<li>三个模块（提取、分类、回归）是分别训练的，并且在训练时候，对于存储空间消耗较大</li>
</ul>
<h4 id="（2）Fast-R-CNN"><a href="#（2）Fast-R-CNN" class="headerlink" title="（2）Fast R-CNN"></a>（2）Fast R-CNN</h4><p><strong>Fast R-CNN</strong> 是对 R-CNN 的改进，旨在解决 R-CNN 计算效率低的问题。其主要优化点包括：</p>
<ul>
<li><strong>共享特征提取</strong>：输入图像只通过一次 CNN 处理，生成特征图，而不是对每个候选区域重复计算。随后在这些特征图上进行区域处理。</li>
<li><strong>ROI Pooling</strong>：从特征图中提取候选区域，并通过 ROI Pooling 层将候选区域统一为固定大小，方便送入全连接层进行分类和回归。</li>
<li><strong>分类与回归同时进行</strong>：Fast R-CNN 使用 Softmax 层进行目标分类，并在同一个网络中执行边界框回归，大大提高了检测速度和精度。</li>
</ul>
<h5 id="详细步骤"><a href="#详细步骤" class="headerlink" title="详细步骤"></a>详细步骤</h5><ol>
<li><p><strong>特征提取</strong>：输入图像通过一个深度卷积神经网络（如 VGG16）进行特征提取，生成一个特征图。</p>
</li>
<li><p><strong>候选区域生成</strong>：使用选择性搜索（Selective Search）生成约 2000 个候选区域（ROI）。</p>
</li>
<li><p><strong>ROI Pooling</strong>：对于每个候选区域，通过 ROI Pooling 操作从特征图中提取对应的特征。将每个 ROI 的特征映射到固定的大小（如 7x7）。</p>
</li>
<li><p><strong>全连接层</strong>：将 ROI Pooling 的输出通过两个全连接层，得到每个候选区域的特征表示。</p>
</li>
<li><p><strong>分类和边界框回归</strong>：在全连接层的输出上，分别进行物体分类（使用 softmax 函数）和边界框回归（用于优化每个候选框的位置）。</p>
</li>
<li><p><strong>非极大值抑制（NMS）</strong>：对候选框进行 NMS 处理，去除重叠度高的候选框，保留每个类别中得分最高的框。</p>
</li>
<li><p><strong>输出结果</strong>：最终输出每个物体的类别及其对应的边界框和回归修正。</p>
</li>
</ol>
<h5 id="缺点-1"><a href="#缺点-1" class="headerlink" title="缺点"></a>缺点</h5><ul>
<li>依旧采用selective search提取region proposal（耗时2~3秒，特征提取耗时0.32秒）</li>
<li>无法满足实时应用，没有真正实现端到端训练测试</li>
<li>利用了GPU，但是region proposal方法是在CPU上实现的</li>
</ul>
<h4 id="（3）Faster-R-CNN"><a href="#（3）Faster-R-CNN" class="headerlink" title="（3）Faster R-CNN"></a>（3）Faster R-CNN</h4><p><strong>Faster R-CNN</strong> 进一步优化了 Fast R-CNN，消除了对选择性搜索的依赖，提出了区域提议网络（RPN）直接从 CNN 的特征图中生成候选区域。</p>
<ul>
<li><strong>RPN（Region Proposal Network）</strong>：与 Fast R-CNN 不同，Faster R-CNN 使用 RPN 代替选择性搜索，直接从 CNN 的特征图中生成候选区域。RPN 网络通过滑动窗口在特征图上生成候选框，并预测每个框是否包含目标。</li>
<li><strong>共享特征图</strong>：RPN 和 Fast R-CNN 共享 CNN 提取的特征图，这使得检测速度更快。</li>
</ul>
<blockquote>
<p><strong>RPN（Region Proposal Network）</strong> 是一种用于目标检测的神经网络模块，最初由 <strong>Faster R-CNN</strong> 提出。RPN 的主要目的是从输入的特征图中生成候选区域（Region Proposals），这些候选区域将用于后续的目标分类和边界框回归。</p>
<h3 id="RPN-的工作原理："><a href="#RPN-的工作原理：" class="headerlink" title="RPN 的工作原理："></a>RPN 的工作原理：</h3><ol>
<li><strong>特征图输入</strong>：<ul>
<li>RPN 接收从卷积神经网络（CNN）提取的特征图，通常是经过一系列卷积和池化操作后的输出。</li>
</ul>
</li>
<li><strong>滑动窗口</strong>：<ul>
<li>RPN 在特征图上以滑动窗口的方式进行处理。通常使用一个小的窗口（例如 3×3），在特征图上逐步滑动，并对每个位置生成多个候选区域。</li>
</ul>
</li>
<li><strong>Anchor Boxes</strong>：<ul>
<li>对于每个滑动窗口位置，RPN 会生成多个称为 <strong>anchor boxes</strong> 的候选框，anchor boxes 是预定义的固定尺寸和长宽比的矩形框。这些框的位置会围绕滑动窗口的中心进行调整，以便覆盖特征图中可能存在的目标。</li>
</ul>
</li>
<li><strong>分类与回归</strong>：<ul>
<li>RPN 使用一个小的神经网络（通常由卷积层和全连接层构成）对每个 anchor box 进行分类（是否包含目标）和边界框回归（调整框的位置和大小）。具体来说：<ul>
<li><strong>分类任务</strong>：输出每个 anchor box 是否包含目标的概率（前景&#x2F;背景）。</li>
<li><strong>回归任务</strong>：输出每个 anchor box 的坐标调整值，以优化目标的边界框。</li>
</ul>
</li>
</ul>
</li>
<li><strong>非极大值抑制（NMS）</strong>：<ul>
<li>通过 NMS，对生成的候选区域进行处理，去除重叠度过高的框，最终保留最优的候选区域，传递给后续的目标检测网络（如 Fast R-CNN）。</li>
</ul>
</li>
</ol>
</blockquote>
<h4 id="（4）Mask-R-CNN"><a href="#（4）Mask-R-CNN" class="headerlink" title="（4）Mask R-CNN"></a>（4）Mask R-CNN</h4><p><strong>Mask R-CNN</strong> 是 Faster R-CNN 的扩展版本，加入了实例分割功能。除了检测物体和回归边界框外，Mask R-CNN 还能预测每个目标的像素级掩码，实现精细的目标分割。</p>
<ul>
<li><strong>实例分割</strong>：通过在 Faster R-CNN 的基础上增加一个并行的分支，用于预测每个候选区域内的像素掩码。这样不仅可以进行目标检测，还可以实现像素级分割。</li>
<li><strong>ROI Align</strong>：为了解决 ROI Pooling 中的量化误差，Mask R-CNN 引入了 ROI Align，将候选区域精确对齐到特征图上，提高了掩码预测的精度。</li>
</ul>
<h3 id="2-YOLO系列"><a href="#2-YOLO系列" class="headerlink" title="2. YOLO系列"></a>2. YOLO系列</h3><h4 id="（1）YOLOv1"><a href="#（1）YOLOv1" class="headerlink" title="（1）YOLOv1"></a>（1）YOLOv1</h4><h5 id="基本思想"><a href="#基本思想" class="headerlink" title="基本思想"></a>基本思想</h5><p>将目标检测问题转化为一个回归问题，将候选区和检测两个阶段合二为一，从而实现实时检测。实际上，YOLO并没有真正去掉候选区，而是采用了预定义候选区的方法，也就是将图片划分为7<em>7个网格，每个网格允许预测出2个边框，总共49</em>2个bounding box，可以理解为98个候选区域，它们很粗略地覆盖了图片的整个区域。YOLO以降低mAP为代价，大幅提升了时间效率。</p>
<p>YOLOv1的<strong>损失函数</strong>主要由以下几个部分组成：</p>
<ul>
<li>边界框位置的损失（位置误差）</li>
<li>边界框大小的损失</li>
<li>置信度损失（边界框是否包含目标）</li>
<li>类别概率损失</li>
</ul>
<h5 id="网络结构"><a href="#网络结构" class="headerlink" title="网络结构"></a>网络结构</h5><ul>
<li><p><strong>输入层</strong>：输入尺寸为448x448的RGB图像。</p>
</li>
<li><p><strong>卷积层</strong>：使用了一系列的卷积层来提取图像特征。YOLOv1共使用了24个卷积层和2个全连接层。</p>
<ul>
<li>第一层为7x7的卷积，步幅为2，使用了64个滤波器。</li>
<li>随后是多个3x3的卷积层（例如64、128、256等），并配有批量归一化和激活函数（如Leaky ReLU）</li>
</ul>
</li>
<li><p><strong>池化层</strong>：使用最大池化层进行下采样。每经过几层卷积后，通常会有一个池化层，降低特征图的空间维度，从而保留主要特征并减小计算量。</p>
</li>
<li><p><strong>第一个全连接层</strong>：将卷积层的输出展平，并映射到一个较小的维度。</p>
</li>
<li><p><strong>第二个全连接层</strong>：最终输出一个大小为$S\times S\times (B\times 5+C)$的张量，其中：</p>
<ul>
<li>$S\times S$ 是网格的数量。</li>
<li>$B$ 是每个网格预测的边界框数量。</li>
<li>每个边界框包含5个信息（中心坐标、宽度、高度、置信度）。</li>
<li>$C$ 是目标类别的数量，每个网格还需要预测每个类别的概率。</li>
</ul>
</li>
</ul>
<h5 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h5><ol>
<li><strong>数据准备</strong></li>
</ol>
<p>在训练YOLOv1之前，需要准备好数据集，包括：</p>
<ul>
<li><strong>图像数据</strong>：输入的图像，通常是高分辨率的RGB图像。</li>
<li><strong>标注数据</strong>：与每幅图像对应的标注信息，通常包含目标的类别和边界框（bounding box）信息。标注格式可能使用Pascal VOC或YOLO格式。</li>
</ul>
<ol start="2">
<li><strong>数据预处理</strong></li>
</ol>
<p>在训练之前，需要对图像和标注进行预处理，包括：</p>
<ul>
<li><strong>图像缩放</strong>：将图像缩放到448x448的尺寸，符合YOLOv1的输入要求。</li>
<li><strong>归一化</strong>：将像素值归一化到0到1之间，以便于模型训练。</li>
</ul>
<blockquote>
<p><strong>归一化（Normalization）</strong>是将数据转换到一个特定的范围或分布，使其适合于后续的模型训练。归一化的主要目的是提高模型的训练效率和预测精度，尤其是在涉及到多维数据的情况下。</p>
<p>$X^′&#x3D;\frac{X}{255}$</p>
</blockquote>
<ul>
<li><strong>数据增强</strong>：通过随机裁剪、旋转、翻转等方法增加训练样本的多样性，提高模型的泛化能力。</li>
</ul>
<blockquote>
<p>采用相对坐标，边界框x 和y 坐标参数化为特定网格单元位置的偏移量，边界也在0和1之间</p>
</blockquote>
<ol start="3">
<li><strong>网络结构</strong></li>
</ol>
<p>YOLOv1的网络结构由卷积层、池化层和全连接层组成。训练时通过前向传播将图像输入网络，生成预测输出，包括边界框位置、大小、置信度和类别概率。</p>
<ol start="4">
<li><strong>损失函数</strong></li>
</ol>
<p>YOLOv1的损失函数是一个关键组成部分，用于评估模型的预测与真实值之间的差距。损失函数通常包括以下几部分：</p>
<ul>
<li><strong>边界框损失</strong>：用于衡量预测的边界框与真实框之间的位置和大小误差。包括中心坐标的平方误差和宽度、高度的平方误差。</li>
<li><strong>置信度损失</strong>：用于衡量预测的边界框是否包含目标。通过计算预测置信度与真实值之间的均方误差。</li>
<li><strong>类别损失</strong>：通过交叉熵损失衡量预测类别概率与真实类别之间的差距。</li>
</ul>
<p>完整的损失函数可以表示为：</p>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/6baad2f695bc40de2d64b3c212c5a514.png" alt="6baad2f695bc40de2d64b3c212c5a514"></p>
<ul>
<li>损失函数由坐标预测、是否包含目标物体置信度、类别预测构成；</li>
<li>其中$1_{i}^{obj}$表示目标是否出现在网格单元i中，表示$1_{ij}^{obj}$​​网格单元i中的第j个边界框预测器“负责”该预测；</li>
<li>如果目标存在于该网格单元中（前面讨论的条件类别概率），则损失函数仅惩罚分类错误；</li>
<li>如果预测器“负责”实际边界框（即该网格单元中具有最高IOU的预测器），则它也仅惩罚边界框坐标错误。</li>
</ul>
<ol start="5">
<li><strong>训练过程</strong></li>
</ol>
<ul>
<li><strong>前向传播</strong>：输入图像经过网络，得到预测的边界框和类别概率。</li>
<li><strong>计算损失</strong>：根据预测结果和真实标注计算损失函数的值。</li>
<li><strong>反向传播</strong>：使用反向传播算法计算梯度，更新网络权重。YOLOv1通常使用随机梯度下降（SGD）或Adam优化器进行权重更新。</li>
<li><strong>迭代训练</strong>：重复进行多个epoch，直到损失收敛或达到预设的训练轮数。</li>
</ul>
<p><strong>避免过拟合</strong>的策略：使用dropout和数据增强来避免过拟合。</p>
<blockquote>
<p><strong>Dropout</strong>的核心思想是在训练时以一定的概率随机“关闭”部分神经元，这样在每个前向传播过程中，网络的结构都会有所不同。具体而言：</p>
<ul>
<li>对于给定的一层神经网络，每个神经元以一个预设的概率 ppp （通常为0.5）被“丢弃”，即暂时不参与前向传播和反向传播。</li>
<li>在训练完成后进行推理时，Dropout不再启用，而是使用所有神经元，并将它们的输出值按比例缩放（通常乘以 ppp），以保持推理时的输出期望值与训练时一致。</li>
</ul>
<p><strong>数据增强（Data Augmentation）</strong> 是通过对原始数据进行变换（如旋转、缩放、翻转等）来生成新的训练样本，从而增加训练数据量的方法。</p>
</blockquote>
<ol start="6">
<li><strong>超参数调整</strong></li>
</ol>
<p>训练YOLOv1时，选择合适的超参数（如学习率、批量大小、权重衰减等）非常重要。这些超参数会影响模型的收敛速度和最终性能。</p>
<p><strong>学习率</strong>：第一个迭代周期，慢慢地将学习率从$10^{-3}$提高到$10^{-2}$；然后继续以$10^{-2}$的学习率训练75个迭代周期，用$10^{-3}$的学习率训练30个迭代周期，最后用$10^{-4}$​的学习率训练30个迭代周期。</p>
<ol start="7">
<li><strong>后处理</strong></li>
</ol>
<p>训练完成后，YOLOv1的输出需要进行后处理，以获得最终的检测结果。这包括：</p>
<ul>
<li><strong>非极大抑制（Non-Maximum Suppression, NMS）</strong>：去除重复的边界框，保留置信度最高的边界框。NMS通过设定一个阈值，过滤掉重叠度过高的框。</li>
<li><strong>输出结果</strong>：最终的输出包括每个检测到的对象的类别、边界框坐标和置信度。</li>
</ul>
<ol start="8">
<li><strong>模型评估</strong></li>
</ol>
<p>训练完成后，需要对模型进行评估，通常使用mAP（mean Average Precision）等指标来衡量模型在验证集上的表现，确保模型的准确性和鲁棒性。</p>
<h5 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h5><p><strong>优点</strong></p>
<ul>
<li>YOLO检测物体速度非常快，其增强版GPU中能跑45fps（frame per second），简化版155fps</li>
<li>YOLO在训练和测试时都能看到一整张图的信息（而不像其它算法看到局部图片信息），因此YOLO在检测物体是能很好利用上下文信息，从而不容易在背景上预测出错误的物体信息</li>
<li>YOLO可以学到物体泛化特征</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>精度低于其它state-of-the-art的物体检测系统</li>
<li>容易产生定位错误</li>
<li>对小物体检测效果不好，尤其是密集的小物体，因为一个栅格只能检测2个物体</li>
<li>由于损失函数的问题，定位误差是影响检测效果的主要原因，尤其是大小物体处理上还有待加强</li>
</ul>
<h4 id="（2）YOLOv2"><a href="#（2）YOLOv2" class="headerlink" title="（2）YOLOv2"></a>（2）YOLOv2</h4><p><strong>YOLOv2（You Only Look Once version 2）</strong>，也称为YOLO9000，是YOLOv1的改进版本。它在保持YOLOv1高效的同时，通过一系列创新来提高检测精度和速度，尤其是在小目标检测和检测更复杂对象方面取得了显著进步。</p>
<h5 id="改进策略"><a href="#改进策略" class="headerlink" title="改进策略"></a>改进策略</h5><ol>
<li><strong>Batch Normalization（批量正则化）</strong></li>
</ol>
<p><strong>Batch Normalization</strong> 是一种在神经网络训练中常用的技术，旨在通过规范化每一层的输入来加速训练并提高模型的稳定性。</p>
<ul>
<li><p>在神经网络中，每一层的输入分布可能会随着训练的进行发生变化，这种现象叫做<strong>内部协变量偏移（Internal Covariate Shift）</strong>。随着网络层数加深，这种偏移可能会使得训练变得不稳定。</p>
</li>
<li><p><strong>Batch Normalization</strong> 的作用是在每个批次（batch）上，对每一层的输入进行归一化，使其分布保持稳定。具体来说，BN会将每一层输入的激活值规范化为零均值和单位方差：$\hat x^{(k)}&#x3D; \frac{x^{(k)}−μ_B}{\sqrt{\sigma_B^{2}+\epsilon}}$，</p>
<p>其中，$\mu_B$是当前批次的均值，$\sigma_B$ 是方差，$\epsilon$ 是一个很小的常数，用于防止除零。</p>
</li>
</ul>
<p>归一化之后，BN还会引入两个可学习的参数<strong>缩放</strong>（scale）和<strong>平移</strong>（shift），让模型能够调整归一化后的激活值：$y^{(k)}&#x3D;γx^{(k)}+β$</p>
<p>其中，$\gamma$ 和 $\beta$ 是学习参数，分别用于缩放和平移。</p>
<ul>
<li><p><strong>加速训练</strong>：BN能够规范化每个卷积层的输出，使得后续层的输入具有稳定的分布（接近零均值、单位方差）。这减少了梯度变化的幅度，从而使得模型在反向传播时能够更平稳地更新权重，<strong>加快了收敛速度</strong>。</p>
</li>
<li><p><strong>更高的学习率</strong>：由于数据分布的稳定性，模型可以使用更高的学习率进行训练，而不必担心梯度爆炸或消失的问题。这进一步加速了训练过程。</p>
</li>
<li><p><strong>正则化效果</strong>：BN在一定程度上具有正则化效果，它通过批次归一化使得模型对训练样本中的小扰动不敏感，从而<strong>减少了模型对训练数据的过拟合</strong>。由于每个批次的数据都会被归一化，模型每次看到的数据分布有所变化，这类似于数据增强，也起到了一定的正则化作用。</p>
</li>
<li><p><strong>替代Dropout</strong>：在YOLOv2中，BN层的引入使得模型不再需要使用Dropout层来防止过拟合。YOLOv1使用了Dropout来随机关闭部分神经元，防止模型过拟合，而YOLOv2发现BN本身就能够提供足够的正则化效果，因此移除了Dropout层。</p>
</li>
</ul>
<ol start="2">
<li><strong>High Resolution Classifier（高分辨率分类器）</strong></li>
</ol>
<p>YOLOv2在训练过程中使用了更高分辨率的输入（448x448）。模型最初以低分辨率（224x224）进行训练，在训练的后期将分辨率切换到448x448。高分辨率的输入有助于检测小目标，使得模型在训练的后期能够更精细地学习目标的细节。</p>
<p>YOLOv2将预训练分成两步：先用224x224的输入从头开始训练网络，大概160个epoch，然后再将输入调整到448x448，再训练10个epoch。</p>
<blockquote>
<ul>
<li><strong>batchsize</strong>：批大小。在深度学习中，一般采用SGD训练，即每次训练在训练集中取batchsize个样本训练；</li>
<li><strong>iteration</strong>：1个iteration等于使用batchsize个样本训练一次；</li>
<li><strong>epoch</strong>：1个epoch等于使用训练集中的全部样本训练一次；</li>
</ul>
</blockquote>
<p><strong>低分辨率训练的优势</strong></p>
<ul>
<li><strong>加快早期训练阶段</strong>：在模型的初期训练阶段，使用低分辨率输入可以减少计算量，从而加快训练速度。低分辨率的图像包含的细节较少，模型可以更快地从数据中学到大体的特征。</li>
<li><strong>粗略学习特征</strong>：模型在早期训练时主要学习一些粗略的、全局的特征，比如大的形状和轮廓。这个阶段不需要非常高的分辨率。</li>
</ul>
<p><strong>高分辨率输入的引入</strong></p>
<p>在训练的后期阶段，YOLOv2将输入图像的分辨率提高到448x448。这种高分辨率输入的引入有以下好处：</p>
<ul>
<li><strong>更好的细节捕捉</strong>：高分辨率的图像包含更多的像素细节，有助于模型在后期更好地捕捉图像中的小物体和细微特征。比如，在低分辨率图像中，某些小目标可能由于像素较少而变得模糊或者不可见，而在高分辨率图像中，这些目标可以清晰地被模型识别。</li>
<li><strong>提高模型的精确度</strong>：通过在后期使用高分辨率输入，模型可以在粗略特征的基础上进一步精细化，从而提高检测的精度。</li>
</ul>
<ol start="3">
<li><strong>Convolutional With Anchor Boxes（带Anchor Boxes的卷积）</strong></li>
</ol>
<p>YOLOv1利用全连接层直接对边界框进行预测，导致丢失较多空间信息，定位不准。<strong>YOLOv2去掉了YOLOv1中的全连接层，使用Anchor Boxes预测边界框</strong>，同时为了得到更高分辨率的特征图，<strong>YOLOv2还去掉了一个池化层</strong>。</p>
<blockquote>
<p><strong>Anchor Boxes</strong> 是一组具有预定义形状和大小的边界框，模型在这些预定义框的基础上进行调整（通过回归预测），以匹配实际的物体。这种方法允许模型预测不同形状和大小的物体，而不需要直接预测绝对的边界框位置和尺寸。在YOLOv2中，特征图的每个cell（即每个网格单元）都负责预测多个Anchor Boxes。具体来说，YOLOv2在每个cell上预测5个Anchor Boxes，这意味着每个cell可以为场景中的多个物体进行检测，特别是当物体大小和形状不同时。</p>
<p>YOLOv2完全依靠<strong>卷积层</strong>进行边界框预测</p>
</blockquote>
<p>由于图片中的物体都倾向于出现在图片的中心位置，若特征图恰好有一个中心位置，利用这个中心位置预测中心点落入该位置的物体，对这些物体的检测会更容易。所以总希望得到的特征图的宽高都为奇数。YOLOv2通过缩减网络，使用416<em>416的输入，模型下采样的总步长为32，最后得到13</em>13的特征图，然后对13<em>13的特征图的每个cell预测5个anchor boxes，对每个anchor box预测边界框的位置信息、置信度和一套分类概率值。使用anchor boxes之后，YOLOv2可以预测13</em>13*5&#x3D;845个边界框。</p>
<ol start="4">
<li><strong>Dimension Clusters（维度聚类）</strong></li>
</ol>
<p>在Faster R-CNN和SSD中，先验框（Anchor Boxes）都是手动设定的，带有一定的主观性。</p>
<p>YOLOv2采用<strong>k-means聚类算法</strong>对训练集中的边界框做了聚类分析，选用boxes之间的IOU值作为聚类指标。</p>
<p>模型复杂度过高可能会导致训练困难，而召回率过低则可能漏检目标。YOLOv2综合考虑模型复杂度和召回率，最终选择5个聚类中心，得到5个先验框，发现其中中扁长的框较少，而瘦高的框更多，更符合行人特征。通过对比实验，发现用聚类分析得到的先验框比手动选择的先验框有更高的平均IOU值，这使得模型更容易训练学习。</p>
<blockquote>
<p><strong>k-means聚类算法</strong>是一种广泛使用的无监督学习算法，主要用于将数据集划分为K个簇（clusters）。它的目标是将相似的数据点聚集在一起，同时使得不同簇之间的差异尽可能大。</p>
<p>k-means聚类的<strong>核心思想</strong>是通过迭代过程将数据点分配到最近的簇心（centroid）并更新簇心位置，从而达到最优的簇划分。它使用欧几里得距离（或其他距离度量）来评估数据点与簇心之间的相似性。</p>
<p><strong>召回率</strong>（Recall）是一个重要的评估指标，特别是在二分类和多分类任务中，用于衡量模型在正类（即感兴趣的类别）识别方面的性能。召回率反映了模型能够正确识别出多少实际存在的正类样本。</p>
</blockquote>
<ol start="5">
<li><strong>New Network（新的网络）</strong></li>
</ol>
<p>YOLOv2引入了<strong>Darknet-19</strong>，这是一种新设计的卷积神经网络，用于替代YOLOv1的特征提取网络。Darknet-19具有19个卷积层和5个池化层，且使用了大量的3x3卷积核以及1x1卷积核。</p>
<ol start="6">
<li><strong>直接定位预测（Direct location Prediction）</strong></li>
</ol>
<p>Faster R-CNN使用anchor boxes预测边界框相对先验框的偏移量，由于没有对偏移量进行约束，每个位置预测的边界框可以落在图片任何位置，会导致模型不稳定，加长训练时间。YOLOv2沿用YOLOv1的方法，根据所在网格单元的位置来预测坐标,则Ground Truth的值介于0到1之间。网络中将得到的网络预测结果再输入sigmoid函数中，让输出结果介于0到1之间。</p>
<p>直接定位预测的核心思想是直接通过卷积层输出特征图的每个单元来预测边界框的坐标，而不是通过全连接层。</p>
<ol start="7">
<li><strong>细粒度特征（Fine-Grained Features）</strong></li>
</ol>
<p>YOLOv2借鉴SSD使用多尺度的特征图做检测，提出pass through层将高分辨率的特征图与低分辨率的特征图联系在一起，从而实现多尺度检测。YOLOv2提取Darknet-19最后一个max pool层的输入，得到26<em>26</em>512的特征图。经过1<em>1</em>64的卷积以降低特征图的维度，得到26x26x64的特征图，然后经过pass through层的处理变成13x13x256的特征图（抽取原特征图每个2*2的局部区域组成新的channel，即原特征图大小降低4倍，channel增加4倍），再与13x13x1024大小的特征图连接，变成13x13x1280的特征图，最后在这些特征图上做预测。使用Fine-Grained Features，YOLOv2的性能提升了1%。</p>
<ol start="8">
<li><strong>多尺度训练（Multi-Scale Training）</strong></li>
</ol>
<p>YOLOv2中使用的Darknet-19网络结构中只有卷积层和池化层，所以其对输入图片的大小没有限制。YOLOv2采用多尺度输入的方式训练，在训练过程中每隔10个batches,重新随机选择输入图片的尺寸，由于Darknet-19下采样总步长为32，输入图片的尺寸一般选择32的倍数{320,352,…,608}（最小的选项是320×320，最大的是608×608。我们调整网络的尺寸并继续训练）。采用Multi-Scale Training, 可以适应不同大小的图片输入，当采用低分辨率的图片输入时，mAP值略有下降，但速度更快，当采用高分辨率的图片输入时，能得到较高mAP值，但速度有所下降。</p>
<h5 id="训练过程-1"><a href="#训练过程-1" class="headerlink" title="训练过程"></a>训练过程</h5><ul>
<li>第一阶段：现在ImageNet分类数据集上训练Darknet-19,此时模型输入为224*224，共训练160轮</li>
<li>第二阶段：将网络输入调整为448*448，继续在ImageNet分类数据集上训练细调模型，共10轮</li>
<li>第三阶段：修改Darknet-19分类模型为检测模型，并在检测数据集上继续细调网络</li>
</ul>
<h5 id="优点与缺点"><a href="#优点与缺点" class="headerlink" title="优点与缺点"></a>优点与缺点</h5><p><strong>优点</strong></p>
<ul>
<li><p>YOLOv2使用了一个新的分类器作为特征提取部分，较多使用了3x3卷积核，在每次池化后操作后把通道数翻倍。网络使用了全局平均池化，把1x1卷积核置于3x3卷积核之间，用来压缩特征。也用了batch normalization稳定模型训练</p>
</li>
<li><p>最终得出的基础模型就是Darknet-19，包含19个卷积层，5个最大池化层，运算次数55.8亿次</p>
</li>
<li><p>YOLOv2比VGG16更快，精度略低于VGG16</p>
</li>
</ul>
<p><strong>缺点</strong></p>
<ul>
<li>YOLOv2检测准确率不够，比SSD稍差</li>
<li>不擅长检测小物体</li>
<li>对近距离物体准确率较低</li>
</ul>
<h4 id="（3）YOLOv3"><a href="#（3）YOLOv3" class="headerlink" title="（3）YOLOv3"></a>（3）YOLOv3</h4><p>YOLOv3总结了自己在YOLOv2的基础上做的一些尝试性改进，有的尝试取得了成功，而有的尝试并没有提升模型性能。其中有两个值得一提的亮点，一个是使用残差模型，进一步加深了网络结构；另一个是使用FPN架构实现多尺度检测。</p>
<blockquote>
<p><strong>残差连接（Residual Connection）</strong>是一种在深度神经网络中使用的技术，旨在解决随着网络深度增加而可能出现的梯度消失和过拟合问题。它的基本思想是将输入直接与网络某层的输出相加，从而形成“残差”学习。</p>
<p>这种方式有几个优点：</p>
<ol>
<li><strong>缓解梯度消失</strong>：通过将输入信息直接传递到后续层，帮助保持梯度在反向传播时的流动。</li>
<li><strong>提高训练效率</strong>：允许网络在更深的结构中进行训练，而不会显著增加训练难度。</li>
<li><strong>增强模型性能</strong>：通常能够提高模型的准确性和泛化能力。</li>
</ol>
</blockquote>
<h5 id="改进"><a href="#改进" class="headerlink" title="改进"></a>改进</h5><ul>
<li>新网络结构：<strong>DarkNet-53</strong>，由53层卷积层组成；</li>
<li>用<strong>逻辑回归</strong>替代softmax作为分类器；</li>
<li>融合FPN（特征金字塔网络），实现多尺度检测。</li>
</ul>
<blockquote>
<p><strong>逻辑回归</strong>（Logistic Regression）是一种常用的<strong>分类算法</strong>，虽然名字中带有“回归”，但它实际上用于二元或多元分类任务，特别是处理二元分类问题（即输出为0或1的情况）。它通过学习数据特征与类别标签之间的关系来预测输入数据所属的类别。</p>
<p>逻辑回归的核心思想是使用<strong>逻辑函数（logistic function，也叫sigmoid函数）来将线性回归的输出结果转换为概率</strong>。这种概率值可以表示某个样本属于某个类别的可能性。</p>
<p>逻辑回归模型的训练目标是找到最优的权重 www 和偏置 bbb，使得模型对训练数据的预测尽可能准确。为此，逻辑回归使用<strong>对数损失函数（Log-Loss）</strong>，也称为<strong>二元交叉熵损失（Binary Cross-Entropy Loss）</strong>:</p>
<p>$$Loss(y, \hat{y}) &#x3D; -[y log\hat{y} + (1 - y)log(1 - \hat{y})]$$</p>
<p>其中：</p>
<ul>
<li>$y$ 是真实的标签（0或1），</li>
<li>$\hat{y}$​ 是模型预测的概率。</li>
</ul>
</blockquote>
<blockquote>
<p><strong>FPN</strong>（Feature Pyramid Network，特征金字塔网络）是一种用于目标检测任务的深度学习架构，旨在通过多尺度特征融合来提高模型在不同尺度下对目标物体的检测能力。在卷积神经网络（CNN）中，越深的卷积层，特征图的空间分辨率越低，但其捕捉的语义信息更抽象、更强大；而浅层卷积层的特征图分辨率较高，保留更多的局部细节信息，但语义信息较弱。FPN的作用就是通过结合这些不同层次的特征图，使得模型既能保持高层次语义信息的表达能力，又能保留低层次的细节信息，从而在不同尺度下检测物体时都能有较好的表现。</p>
<p> FPN的工作流程主要包括以下步骤：</p>
<ol>
<li><strong>自顶向下路径（Top-Down Pathway）</strong></li>
</ol>
<ul>
<li>FPN首先通过卷积神经网络主干（如ResNet或VGG）进行前向传播，生成不同层次的特征图。较高层的特征图具有更强的语义信息，但空间分辨率较低。</li>
<li>在自顶向下路径中，FPN通过反向传播将高层的特征图进行逐层上采样（通常是2倍上采样），将高语义信息传播到较浅层次的特征图中。</li>
</ul>
<ol start="2">
<li><strong>横向连接（Lateral Connections）</strong></li>
</ol>
<ul>
<li>在自顶向下传播的过程中，每一个上采样的高层特征图与对应的浅层特征图进行横向连接（通常通过卷积操作进行融合）。这样既保留了高层的抽象语义信息，又结合了浅层的空间细节信息。</li>
<li>这个融合操作的核心在于：高层提供全局上下文信息，浅层提供更细粒度的位置信息，使得FPN对大小目标的检测更加鲁棒。</li>
</ul>
<ol start="3">
<li><strong>特征金字塔（Feature Pyramid）</strong></li>
</ol>
<ul>
<li>通过横向连接和自顶向下的传播，FPN在每一个尺度上都生成了多层次的特征图，这些特征图组合在一起形成一个特征金字塔。</li>
<li>特征金字塔中的每个层次都可以被用于不同尺度的目标检测。例如，较高分辨率的浅层特征图适合检测小物体，较低分辨率的深层特征图适合检测大物体。</li>
</ul>
</blockquote>
<h5 id="多尺度预测"><a href="#多尺度预测" class="headerlink" title="多尺度预测"></a>多尺度预测</h5><p>YOLOv3在基本特征提取器上添加几个卷积层，其中最后一个卷积层预测了一个三维张量——边界框，目标和类别预测。 </p>
<p>在COCO数据集的实验中，YOLOv3的每个网格单元预测<strong>3个边界框</strong>。每个边界框由以下内容组成：</p>
<ul>
<li><strong>4个边界框偏移量</strong>：用于调整预测框的中心坐标、宽度和高度。</li>
<li><strong>1个目标存在的预测值</strong>：表示该边界框内是否有目标（一个概率值）。</li>
<li><strong>80个类别预测</strong>：COCO数据集中有80个类别，所以每个边界框预测该目标属于这80个类别中的哪一个。</li>
</ul>
<p>因此，针对每个网格单元，预测张量的深度为：$3 \times (4+1+80)&#x3D;255$</p>
<p>这个张量的大小$N\times N\times 255$为，其中 $N$ 是特征图的宽和高，表示将输入图像划分为的网格大小（如13×13、26×26等）。</p>
<h4 id="（4）YOLOv4"><a href="#（4）YOLOv4" class="headerlink" title="（4）YOLOv4"></a>（4）YOLOv4</h4><p>YOLOv4 是 YOLO 系列的一个重要更新版本，它在保持 <strong>YOLO（You Only Look Once）</strong> 模型速度和效率的同时，进一步提升了精度。YOLOv4 集成了许多<strong>卷积神经网络（CNN）</strong>领域的最新技术，通过改进网络结构和训练策略，使得它不仅适用于学术研究，同时也适合实际应用。</p>
<h5 id="YOLOv4的创新点"><a href="#YOLOv4的创新点" class="headerlink" title="YOLOv4的创新点"></a>YOLOv4的创新点</h5><p><strong>输入端的创新点</strong>：</p>
<ol>
<li><strong>Mosaic 数据增强</strong></li>
</ol>
<p>Mosaic 数据增强是在训练时将四张图片拼接在一起，这样可以在每次迭代中提供更多的图像上下文和变化，提升模型的泛化能力，并提高对不同尺度目标的检测效果。</p>
<ol start="2">
<li><strong>cmBN（Cross Mini-Batch Normalization）</strong></li>
</ol>
<p>cmBN 解决了在小批量训练时批量归一化效果不佳的问题，通过跨多个 mini-batch 计算均值和方差，确保更稳定的训练过程。</p>
<ol start="3">
<li><strong>SAT（Self-Adversarial Training）</strong></li>
</ol>
<p>SAT 是一种自对抗训练方法，它通过让模型在生成对抗性图像时进行反向传播，使其能够抵抗输入扰动，提升模型的鲁棒性。</p>
<p><strong>BackBone主干网络</strong></p>
<ol>
<li><strong>CSPDarknet53</strong></li>
</ol>
<p>CSPDarknet53 是 YOLOv4 的骨干网络，它在 Darknet53 基础上引入了 <strong>CSPNet（Cross Stage Partial Network）</strong>，通过部分卷积层的分离与特征融合，减少计算量并提升模型的学习能力。</p>
<ol start="2">
<li><strong>Mish 激活函数</strong></li>
</ol>
<p>Mish 替代 ReLU 激活函数，具备更平滑的梯度流动，能够更好地处理负值，提升特征表达能力。</p>
<ol start="3">
<li><strong>Dropblock</strong></li>
</ol>
<p>Dropblock 是一种正则化方法，类似于 Dropout，但它通过屏蔽特征图的连续区域来增强模型的鲁棒性和泛化能力，尤其在处理高维特征时表现更好。</p>
<ul>
<li><p>Neck：目标检测网络在BackBone和最后的输出层之间往往会插入一些层，比如Yolov4中的SPP模块、FPN+PAN结构</p>
</li>
<li><p>Prediction：输出层的锚框机制和Yolov3相同，主要改进的是训练时的回归框位置损失函数CIOU_Loss，以及预测框筛选的nms变为DIOU_nms</p>
</li>
</ul>
<h5 id="YOLOv4的整体架构"><a href="#YOLOv4的整体架构" class="headerlink" title="YOLOv4的整体架构"></a><strong>YOLOv4的整体架构</strong></h5><p>YOLOv4 的架构主要可以分为三个部分：</p>
<p><strong>第一部分、Backbone（主干网络）</strong>：用于提取图像的特征。</p>
<p>YOLOv4 使用 <strong>CSPDarkNet53</strong> 作为主干网络，这是对 YOLOv3 中的 <strong>DarkNet-53</strong> 的改进。CSPDarkNet53（Cross Stage Partial DarkNet-53）采用了 <strong>CSPNet</strong> 结构，具体改进包括：</p>
<ul>
<li><strong>CSPNet（Cross Stage Partial Network）</strong>：将主干网络的输入特征图分成两部分，一部分通过网络深层提取特征，另一部分通过捷径路径直接与深层特征融合，减少了重复计算，提升了网络的学习能力，同时降低了模型复杂度。</li>
<li><strong>Mish 激活函数</strong>：YOLOv4 使用了 <strong>Mish</strong> 作为激活函数，代替了 YOLOv3 中的 Leaky ReLU。Mish 被证明在一些深度学习任务中比 ReLU 和 Leaky ReLU 更有效，特别是在训练过程中能够获得更好的梯度流动。</li>
<li><strong>CutMix 和 Mosaic 数据增强</strong>：YOLOv4 引入了两种强大的数据增强方法——<strong>CutMix</strong> 和 <strong>Mosaic</strong>，能够在数据增强过程中合成多个图像，生成新的训练样本，从而提升了模型的泛化能力。</li>
</ul>
<blockquote>
<p><strong>CSPNet</strong> 的核心思想是通过<strong>特征分流</strong>和<strong>特征融合</strong>，减少重复计算，降低计算量并提升特征表达的多样性。具体过程如下：</p>
<ol>
<li>将输入特征图 <strong>X</strong> 分为两部分：<strong>X1</strong> 和 <strong>X2</strong>。<ul>
<li><strong>X1</strong>：一部分输入特征图 <strong>X1</strong> 被送入网络的深层结构（如多个卷积层）进行进一步的特征提取。</li>
<li><strong>X2</strong>：另一部分输入特征图 <strong>X2</strong> 没有经过深层处理，而是通过<strong>捷径路径</strong>直接传递到后续层。</li>
</ul>
</li>
<li>在网络的某个位置，<strong>X1</strong> 的深层特征与 <strong>X2</strong>（捷径路径中的特征）进行融合，通常通过<strong>按元素相加</strong>或<strong>拼接</strong>的方式进行。这个融合过程帮助模型将<strong>深层特征的语义信息</strong>与<strong>浅层特征的细节信息</strong>结合在一起，提升了特征的表达能力。</li>
</ol>
<p><strong>Mish激活函数</strong>是光滑的非单调激活函数，$mish(x)&#x3D;x⋅tanh(ln(1+e^x))$</p>
</blockquote>
<p><strong>第二部分、Neck（颈部）</strong>：通过特征金字塔网络（FPN）和路径聚合网络（PAN）实现多尺度特征的融合。</p>
<p>YOLOv4 在颈部使用了两种关键技术来进行特征融合，帮助模型更好地处理多尺度目标检测：</p>
<ul>
<li><strong>FPN（特征金字塔网络）</strong>：FPN 主要用于从网络的不同层次提取特征，并通过<strong>上采样</strong>将高层次的语义信息与浅层次的高分辨率信息进行融合，从而提升多尺度物体检测的效果。</li>
<li><strong>PAN（路径聚合网络）</strong>：PAN 是 YOLOv4 中的另一个特征融合网络，它采用了一个自底向上的路径来将低层特征向上传递，进一步增强浅层特征的传播，提升对小物体的检测效果。</li>
</ul>
<p>通过这两种特征融合策略，YOLOv4 的不同尺度特征图都能够兼顾语义信息和细节信息，从而提升检测精度。</p>
<blockquote>
<p>PAN 是在 FPN 的基础上提出的，用于进一步加强特征的多层次融合，特别是为了改进网络的<strong>信息流动</strong>和<strong>特征传递</strong>。</p>
<p>FPN 采用了<strong>自顶向下</strong>的特征融合机制，但它仅从高层到低层传递信息，无法充分利用从低层到高层的反向信息流动。PAN 则通过引入<strong>自底向上（bottom-up）</strong>的特征聚合路径，使得浅层的高分辨率特征可以向上传递，并与高层次的特征相结合。通过这种双向的信息流动，PAN 提高了多尺度特征的表达能力。</p>
</blockquote>
<p><strong>第三部分、Head（头部）</strong>：用于最终的边界框预测和分类。</p>
<p>YOLOv4 在检测头部仍然采用了与 YOLOv3 类似的机制.</p>
<h2 id="4-视频关键帧处理"><a href="#4-视频关键帧处理" class="headerlink" title="4. 视频关键帧处理"></a>4. 视频关键帧处理</h2><p><strong>关键帧（I-Frame）</strong>：</p>
<ul>
<li><p>关键帧是包含该段视频中主要信息的帧</p>
</li>
<li><p>关键帧在压缩成AVI, MP4, MOV等格式时，该帧会完全保留</p>
</li>
<li><p>视频解码时只需要本帧数据，不需要从前一帧、后一帧获取数据</p>
</li>
</ul>
<p><strong>前向差别帧（P-Frame）</strong></p>
<ul>
<li><p>当前帧与前一个I-Frame或前一个P-Frame之间的差别，可以理解为与前一帧的数据偏移值</p>
</li>
<li><p>P-Frame没有完整数据画面，只有与前一帧的差别信息，解码时需要从前一帧获取数据</p>
</li>
</ul>
<p><strong>双向差别帧（B-Frame）</strong></p>
<ul>
<li>记录本帧与前一帧、后一帧的差别</li>
<li>解码时需要获取前一帧、后一帧的数据</li>
<li>压缩后的视频体积小，但编解码计算较慢</li>
</ul>
<p>可以使用<strong>FFMPEG工具</strong>提取视频中的关键帧和进行视频截取。</p>
<h2 id="5-图像标注工具"><a href="#5-图像标注工具" class="headerlink" title="5. 图像标注工具"></a>5. 图像标注工具</h2><p><strong>Labelme（指定使用）</strong><br>labelme 是一款开源的图像&#x2F;视频标注工具，标签可用于目标检测、分割和分类。灵感是来自于 MIT 开源的一款标注工具 Labelme。Labelme具有的特点是：</p>
<ul>
<li>支持图像的标注的组件有：矩形框，多边形，圆，线，点（rectangle, polygons, circle, lines, points）</li>
<li>支持视频标注</li>
<li>GUI 自定义</li>
<li>支持导出 VOC 格式用于 semantic&#x2F;instance segmentation</li>
<li>支出导出 COCO 格式用于 instance segmentation</li>
</ul>
<h1 id="三、图像质量评价"><a href="#三、图像质量评价" class="headerlink" title="三、图像质量评价"></a>三、图像质量评价</h1><h2 id="1-基本概念-1"><a href="#1-基本概念-1" class="headerlink" title="1. 基本概念"></a>1. 基本概念</h2><p>从有没有人参与的角度区分，图像质量评价方法有主观评价和客观评价两个分支。主观评价以人作为观测者，对图像进行主观评价，力求能够真实地反映人的视觉感知；客观评价方法借助于某种数学模型，反映人眼的主观感知，给出基于数字计算的结果。</p>
<h3 id="1-主观评价"><a href="#1-主观评价" class="headerlink" title="1. 主观评价"></a>1. 主观评价</h3><p><strong>主观评价</strong>是通过人类观察者直接评估图像质量。这种方法依赖于人眼对图像的感知，往往更贴近实际的视觉体验。</p>
<h4 id="（1）绝对评价"><a href="#（1）绝对评价" class="headerlink" title="（1）绝对评价"></a>（1）绝对评价</h4><p>由观察者根据自己的知识和理解，按照某些特定评价性能对图像的绝对好坏进行评价。通常，图像质量的绝对评价都是观察者参照原始图像对待定图像采用双刺激连续质量分级法，给出一个直接的质量评价值。</p>
<blockquote>
<p><strong>双刺激连续质量分级法</strong>（<strong>DSCQS</strong>，Double Stimulus Continuous Quality Scale）具体做法是将待评价图像和原始图像按一定规则交替播放持续一定时间给观察者，然后在播放后留出一定的时间间隔供观察者打分，最后将所有给出的分数取平均作为该序列的评价值，即该待评图像的评价值。</p>
</blockquote>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/764cb2e00594c5b6b2597aec07d02ef3.png" alt="img"></p>
<h4 id="（2）相对评价"><a href="#（2）相对评价" class="headerlink" title="（2）相对评价"></a>（2）相对评价</h4><p>相对评价中没有原始图像作为参考，是由观察者对一批待评价图像进行相互比较，从而判断出每个图像的优劣顺序，并给出相应的评价值。通常，相对评价采用单刺激连续质量评价方法。相对于主观绝对评价，主观相对评价也规定了相应的评分制度，称为“群优度尺度”。</p>
<blockquote>
<p><strong>单刺激连续质量评价方法（SSCQE</strong>，Single Stimulus Continuous Quality Evaluation）具体做法是，将一批待评价图像按照一定的序列播放，此时观察者在观看图像的同时给出待评图像相应的评价分值。</p>
</blockquote>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/c33f2f3bbe91dca974d5d1f43f91cfd4.png" alt="img"></p>
<h3 id="2-客观评价"><a href="#2-客观评价" class="headerlink" title="2. 客观评价"></a>2. 客观评价</h3><p>图像质量客观评价的基本目标是设计能精确和自动感知图像质量的计算模型。其终极目标是希望用计算机来代替人类视觉系统去观看和认知图像。在国际上，图像质量客观评价通常是通过测试多个影响影像质量的因素的表现，并通过计算模型获得图像质量量化值与人类主观观测值一致性的好坏来评估的。美国的Imatest和法国的DxO analyzer就是其中比较出名的图像质量客观评价系统。</p>
<img src="./基于深度学习的图像处理基本知识.assets/e06b2e09f4f2221a805e8dd373c1cfe1.jpeg" alt="img" style="zoom: 67%;" />

<p>Imatest和DxO analyzer有异曲同工之处，都是将影像质量评测拆分成多个测试项目，分别对每个项目进行测试、打分。两者相比，DxO analyzer的测试项目会稍微全面一些。</p>
<p>无论是Imatest还是DxO analyzer，两个测试系统都是通过<strong>“测试卡+光源环境+测试软件&#x3D;测试结果”</strong>的模式。通过各种各样的测试卡和光源，在实验室中模拟各种环境，再把成像结果输入软件系统，由系统自动分析，最后得出结果。</p>
<blockquote>
<p><strong>测试卡</strong>是一种专门设计的图像或图形，用于测试和校准显示设备、摄像机、图像处理系统或视频信号质量。测试卡通常具有特定的几何图形、颜色、灰度等级等元素，帮助评估显示设备或图像处理系统的性能。</p>
</blockquote>
<h2 id="2-IQA评估指标"><a href="#2-IQA评估指标" class="headerlink" title="2. IQA评估指标"></a>2. IQA评估指标</h2><h3 id="1-主观评价指标"><a href="#1-主观评价指标" class="headerlink" title="1. 主观评价指标"></a>1. 主观评价指标</h3><p>主观评价依赖于人类观察者的感知，主要通过以下指标进行评价：</p>
<ul>
<li><strong>MOS（Mean Opinion Score，平均主观得分）</strong>：观察者对图像质量打分，通常在1到5的范围内，最终取其平均值。MOS广泛用于图像、视频的质量评估。</li>
<li><strong>DMOS（Differential Mean Opinion Score，平均主观得分差异）</strong>：表示观察者对参考图像和失真图像的评分差异，用于评估失真引入的质量损失。</li>
</ul>
<p><strong>优点</strong>：与人类视觉感知高度相关，评价较为准确。<br><strong>缺点</strong>：耗费人力，费时且不易自动化。</p>
<h3 id="2-客观评价指标"><a href="#2-客观评价指标" class="headerlink" title="2. 客观评价指标"></a>2. 客观评价指标</h3><p>客观评价方法不依赖于人眼，而是通过算法对图像进行定量分析。以下是一些常见的客观评价指标：</p>
<h4 id="与整体图像差异相关的指标："><a href="#与整体图像差异相关的指标：" class="headerlink" title="与整体图像差异相关的指标："></a>与整体图像差异相关的指标：</h4><ul>
<li><strong>MSE（Mean Squared Error，均方误差）</strong>：评估失真图像和参考图像在像素级别上的差异。<br><strong>缺点</strong>：与人眼的视觉感知不符，不能反映局部的细节差异。</li>
<li><strong>RMSE（Root Mean Squared Error，均方根误差）</strong>：均方误差的平方根形式，衡量图像预测误差。<br><strong>优点</strong>：常用于计算算法预测的精度。</li>
<li><strong>PSNR（Peak Signal to Noise Ratio，峰值信噪比）</strong>：通过信号和噪声之间的比例来衡量图像质量。<br><strong>优点</strong>：计算简单，速度快。<br><strong>缺点</strong>：无法反映人类感知到的图像质量差异，特别是对细微的感知变化不敏感。</li>
<li><strong>SSIM（Structural Similarity Index，结构相似性）</strong>：衡量两幅图像在亮度、对比度和结构上的相似性，考虑了人眼对结构信息的敏感度。<br><strong>优点</strong>：比MSE和PSNR更符合人眼视觉感知。<br><strong>缺点</strong>：在位移、旋转、缩放等非结构性失真情况下效果不佳。</li>
</ul>
<h4 id="与主观视觉感知相关的指标："><a href="#与主观视觉感知相关的指标：" class="headerlink" title="与主观视觉感知相关的指标："></a>与主观视觉感知相关的指标：</h4><ul>
<li><strong>UIQI（Universal Image Quality Index，通用图像质量指数）</strong>：从亮度、对比度、结构等方面综合考虑图像质量。<br><strong>优点</strong>：综合反映了人眼感知的多个因素。</li>
<li><strong>VIF（Visual Information Fidelity，视觉信息保真度）</strong>：通过评估图像中的信息损失来衡量质量，注重高频细节。<br><strong>优点</strong>：与人眼感知的图像信息损失一致，尤其适用于图像的高频部分。</li>
<li><strong>IFC（Information Fidelity Criterion，信息保真度准则）</strong>：基于自然场景统计模型，通过高频细节评估图像信息的丢失，与人眼感知高频细节的敏感性相关。<br><strong>优点</strong>：能够很好地匹配人类视觉对高频细节的敏感度，因此在图像超分辨率质量评估中表现优秀。</li>
</ul>
<h4 id="相关性和离出率："><a href="#相关性和离出率：" class="headerlink" title="相关性和离出率："></a>相关性和离出率：</h4><ul>
<li><strong>LCC（Linear Correlation Coefficient，线性相关系数）</strong>：衡量算法评价与人眼评分的线性相关性。</li>
<li><strong>SROCC（Spearman’s Rank-Order Correlation Coefficient，Spearman 秩相关系数）</strong>：衡量评价值与主观打分的单调性。</li>
<li><strong>KROCC（Kendall’s Rank Correlation Coefficient，Kendall 秩相关系数）</strong>：也用于衡量预测结果的单调性，与SROCC类似。</li>
<li><strong>OR（Outlier Ratio，离出率）</strong>：衡量预测值与主观评分的离群点比例。</li>
</ul>
<h3 id="不同方法的优缺点比较："><a href="#不同方法的优缺点比较：" class="headerlink" title="不同方法的优缺点比较："></a>不同方法的优缺点比较：</h3><p>IFC &gt; NQM &gt; WPSNR &gt; MSSSIM &gt; SSIM &gt; UIQI &gt; PSNR &gt;VIF </p>
<ul>
<li><strong>PSNR</strong>：简单快速，但与人类视觉不完全匹配。</li>
<li><strong>SSIM</strong>：比PSNR更符合人眼感知，但对于几何失真（如旋转、缩放）不敏感。</li>
<li><strong>VIF</strong> 和 <strong>IFC</strong>：在高频细节上表现优秀，尤其适用于超分辨率等高精度重建任务。</li>
</ul>
<p>IFC被认为在超分辨率任务中的表现最佳，因为它专注于图像的高频细节，这正是人眼对超分辨率图像最敏感的部分。</p>
<h2 id="3-图像质量检测方式"><a href="#3-图像质量检测方式" class="headerlink" title="3. 图像质量检测方式"></a>3. 图像质量检测方式</h2><h3 id="1-半参考方法"><a href="#1-半参考方法" class="headerlink" title="1. 半参考方法"></a>1. 半参考方法</h3><p>图像的某些特征与原始图像的相同特征进行比较，比如小波变换系数的概率分布、综合多尺度几何分析、对比度敏感函数和可觉察灰度差异特征等。其相应的应用领域包括视频传输中的数字水印验证、利用副通道进行视频质量监控与码流率控制等。</p>
<p><strong>全参考方法</strong>是指在进行图像质量评价时，完全依赖于一张“无失真”或“原始”图像作为参考，通过比较该参考图像和失真图像来量化图像质量的下降程度。这类方法假设原始图像可以被获取，并且它代表了“最佳”质量。</p>
<p><strong>半参考方法</strong>是一种介于全参考和无参考方法之间的质量评价方式，它只需要从参考图像中提取一部分信息（特征），而不需要完整的原始图像。通过这些部分特征来估计图像质量，这在一些受限的场景中非常有用，如网络带宽有限时。</p>
<h3 id="2-盲图像质量"><a href="#2-盲图像质量" class="headerlink" title="2. 盲图像质量"></a>2. 盲图像质量</h3><p>盲图像质量(Blind image quality, BIQ)评价方法完全无需参考图像，根据失真图像的自身特征来估计图像的质量。有些方法是面向特定失真类型的，如针对模糊、噪声、块状效应的严重程度进行评价； 有些方法先进行失真原因分类， 再进行定量评价；而有些方法则试图同时评价不同失真类型的图像。无参考方法最具实用价值, 有着非常广泛的应用范围。 </p>
<p>盲图像质量评价的方法大体上可以分为以下几类：</p>
<h4 id="1-基于失真模型"><a href="#1-基于失真模型" class="headerlink" title="1. 基于失真模型"></a>1. <strong>基于失真模型</strong></h4><p>这类方法假设图像可能会遭受某种特定类型的失真（如噪声、模糊、压缩伪影等），并通过识别这些失真来评估图像质量。</p>
<ul>
<li><strong>典型模型</strong>：通过分析图像的统计特性（如噪声、边缘强度、频域特征等），检测特定类型的失真。例如，针对模糊的图像，可以通过边缘信息和梯度变化来评估质量。</li>
<li><strong>优点</strong>：这种方法适用于失真类型已知的场景，能够提供快速有效的评估。</li>
<li><strong>缺点</strong>：在复杂或混合失真条件下，这些模型可能无法准确评估质量，因为它们假定某种特定失真类型。</li>
</ul>
<h4 id="2-基于自然场景统计"><a href="#2-基于自然场景统计" class="headerlink" title="2. 基于自然场景统计"></a>2. <strong>基于自然场景统计</strong></h4><p>自然场景统计法（Natural Scene Statistics, NSS）认为，正常的自然图像在某些统计特性上具有一致性，而失真图像往往会偏离这些特性。通过评估图像偏离自然场景统计特征的程度，可以判断图像的质量。</p>
<ul>
<li><strong>典型特征</strong>：常使用小波变换、傅里叶变换等方法提取图像的统计特性，然后通过这些特性与标准自然场景的统计特性进行比较。</li>
<li><strong>BRISQUE（Blind&#x2F;Referenceless Image Spatial Quality Evaluator）</strong>：这是一个经典的基于NSS特征的盲图像质量评价模型。它通过分析图像局部对比度、亮度等特征，构建图像的统计模型，从而评估其质量。</li>
<li><strong>优点</strong>：能够应对多种类型的失真，对图像的质量评估具有较强的鲁棒性。</li>
<li><strong>缺点</strong>：计算复杂度较高，且依赖于特征提取方法的准确性。</li>
</ul>
<h4 id="3-基于机器学习"><a href="#3-基于机器学习" class="headerlink" title="3. 基于机器学习"></a>3. <strong>基于机器学习</strong></h4><p>近年来，随着机器学习和深度学习的发展，基于学习的盲图像质量评价方法变得越来越流行。通过使用大量带标签的图像训练模型，机器学习算法能够自动学习图像的质量特征。</p>
<ul>
<li><strong>方法流程</strong>：<ol>
<li><strong>特征提取</strong>：从图像中提取特征（如纹理、边缘、对比度等）。</li>
<li><strong>训练模型</strong>：使用带有质量评分的图像数据库（如LIVE、TID2013等）进行训练，使模型能够根据特征预测图像的质量。</li>
<li><strong>质量预测</strong>：训练好的模型可用于预测新图像的质量得分。</li>
</ol>
</li>
<li><strong>深度学习模型</strong>：使用卷积神经网络（CNN）等深度学习模型，不仅可以自动提取高级特征，还可以直接进行图像质量评分。近年来，基于深度学习的BIQA方法表现优异。</li>
<li><strong>优点</strong>：能够处理各种复杂和混合类型的失真，通过大规模数据训练可以获得高精度的评估。</li>
<li><strong>缺点</strong>：训练过程需要大量带标签的数据，计算资源需求较高。</li>
</ul>
<h4 id="4-基于内容"><a href="#4-基于内容" class="headerlink" title="4. 基于内容"></a>4. <strong>基于内容</strong></h4><p>这类方法根据图像内容对其质量进行评价。不同类型的图像（如自然图像、医学图像、文本图像）可能对失真的敏感性不同，因此这类方法通过识别图像内容，采用不同的策略进行质量评价。</p>
<ul>
<li><strong>场景自适应</strong>：例如，文本图像中边缘信息非常重要，模糊会显著降低质量；而在自然场景图像中，颜色和纹理的失真可能更加重要。</li>
<li><strong>优点</strong>：针对特定场景的质量评价效果好。</li>
<li><strong>缺点</strong>：不具备通用性，需要根据具体图像类型调整评估策略。</li>
</ul>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241024154152036-9755718.png" alt="image-20241024154152036"></p>
<p><img src="/./%E5%9F%BA%E4%BA%8E%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86%E5%9F%BA%E6%9C%AC%E7%9F%A5%E8%AF%86.assets/image-20241024154451934-9755894.png" alt="image-20241024154451934"></p>
<h3 id="3-基于深度学习的IQA模型"><a href="#3-基于深度学习的IQA模型" class="headerlink" title="3. 基于深度学习的IQA模型"></a>3. 基于深度学习的IQA模型</h3><p>在深度学习领域有不同类型的深度神经网络结构：卷积神经网络（Convolutional Neural Network，CNN）、循环神经网络（Recurrent Neural Network，RNN）、生成对抗网络（Generative Adversarial Network，GAN）和变换器（Transformer）等。CNN和GAN常用在图像质量评价算法中，RNN网络模型多用于对视频的质量评价，Transformer首先在自然语言处理领域取得成功，近几年，开始应用到计算机视觉领域。</p>
<h4 id="（1）基于卷积神经网络"><a href="#（1）基于卷积神经网络" class="headerlink" title="（1）基于卷积神经网络"></a>（1）基于卷积神经网络</h4><p>基于 CNN 的 IQA 方法通过深度学习网络自动学习图像的特征，进而预测图像的质量。其核心思想是训练一个神经网络模型，使其能够从输入图像中提取出与质量相关的特征，然后基于这些特征输出质量评分。这个过程通常分为以下几个步骤：</p>
<ol>
<li><strong>数据集准备</strong></li>
</ol>
<p>基于 CNN 的 IQA 方法需要大量带标签的图像数据集进行训练。通常这些数据集包含失真的图像以及相应的质量评分（如主观质量评分 MOS 或 DMOS）。</p>
<ol start="2">
<li><strong>CNN 模型设计</strong></li>
</ol>
<p>CNN 模型的设计和选择是基于卷积神经网络从图像中提取特征的能力。常用的 CNN 架构包括多层卷积层、池化层、全连接层等。常见的设计思路包括：</p>
<ul>
<li><strong>浅层网络</strong>：浅层网络结构通常只包含几层卷积层，适合特征简单的质量评价任务。</li>
<li><strong>深层网络</strong>：较深的 CNN 结构（如 ResNet、VGG 等）可以提取出更复杂的高级特征，适合处理复杂的失真类型。</li>
</ul>
<ol start="3">
<li><strong>特征提取</strong></li>
</ol>
<p>CNN 自动从图像中提取多层次的特征，低层卷积层提取简单的边缘、纹理等低级特征，高层卷积层则学习到更加复杂的形状和模式等高级特征。</p>
<ol start="4">
<li><strong>质量评分预测</strong></li>
</ol>
<p>CNN 最终会通过全连接层或回归层将提取到的特征映射为图像质量得分，通常输出一个连续值作为图像的质量分数（如 1-5 或 0-100 的范围）。这种得分可以是主观质量评分的近似值，也可以是由具体场景中定义的质量指标。</p>
<h4 id="（2）基于生成对抗网络"><a href="#（2）基于生成对抗网络" class="headerlink" title="（2）基于生成对抗网络"></a>（2）基于生成对抗网络</h4><p>基于生成对抗网络（Generative Adversarial Networks, <strong>GAN</strong>）的图像质量检测是一种新兴的、性能较好的无参考图像质量评价方法。GAN 主要由生成器（Generator）和判别器（Discriminator）两个对抗网络组成，其中判别器可以用于图像质量的预测和评价。基于 GAN 的图像质量检测利用判别器的能力来区分高质量和低质量的图像，从而有效地检测和评价图像质量。</p>
<p>在传统的 GAN 框架中，生成器旨在生成逼真的图像，而判别器负责区分真实图像和生成的假图像。在图像质量检测任务中，这一框架可以被重构用于评估图像质量：</p>
<ol>
<li><p><strong>生成器：</strong></p>
<p>生成器通常用于生成或者恢复高质量图像，但在 IQA 中，它可以用来生成不同质量的失真图像（即尝试生成可能的失真类型）。</p>
</li>
<li><p><strong>判别器：</strong></p>
<p>判别器的任务是对图像的真实性或质量进行判别。它接收真实图像和生成的图像作为输入，判断它们是否是高质量图像。判别器不仅可以区分真假图像，还可以用于输出一个质量评分（通常是基于对图像细节的理解）。</p>
</li>
</ol>
<p>在图像质量检测中，判别器经过训练后可以学习到高质量图像和低质量图像之间的差异，因此能够有效预测失真图像的质量分数。</p>
<p><strong>GAN 工作流程</strong></p>
<ol>
<li><strong>初始化</strong>：生成器和判别器网络参数随机初始化。</li>
<li><strong>生成图像</strong>：生成器接收随机噪声向量，生成一批图像。</li>
<li><strong>分类真假图像</strong>：判别器接收生成图像和真实图像，输出每张图像是真实的概率。</li>
<li><strong>损失计算</strong>：<ul>
<li>判别器的损失：正确分类真实图像和生成图像的能力。</li>
<li>生成器的损失：欺骗判别器，使生成图像被判别为真实图像的能力。</li>
</ul>
</li>
<li><strong>参数更新</strong>：<ul>
<li>生成器通过最小化损失生成更逼真的图像。</li>
<li>判别器通过最大化损失提高分类真假图像的能力。</li>
</ul>
</li>
<li><strong>重复训练</strong>：循环执行上述过程，直到生成器生成的图像足够逼真，判别器难以区分。</li>
</ol>
<h4 id="（3）基于Transformer"><a href="#（3）基于Transformer" class="headerlink" title="（3）基于Transformer"></a>（3）基于Transformer</h4><p>基于 <strong>Transformer</strong> 的图像质量评价（Image Quality Assessment, IQA）方法是近年来随着 Transformer 模型在计算机视觉任务中的成功而发展起来的一类新型方法。这些方法借助 Transformer 的强大建模能力，能够捕捉图像的全局依赖关系和复杂的特征，尤其在处理无参考图像质量评价（NR-IQA）任务中表现突出。</p>
<p>基于 Transformer 的 IQA 模型一般分为两个核心模块：</p>
<ul>
<li><strong>图像分块处理</strong>：将输入图像分成一系列小块（如 ViT 中的 Patch），每个小块作为一个 token 进入 Transformer 网络。这样可以保留图像的局部信息，同时通过 Transformer 捕捉图像块之间的全局依赖。</li>
<li><strong>全局特征提取</strong>：通过<strong>自注意力机制</strong>，Transformer 网络可以学习到图像中局部区域和全局场景之间的复杂关系。这对于衡量图像质量中的全局失真（如模糊、伪影等）非常有效。</li>
</ul>
<blockquote>
<p><strong>自注意力机制（Self-Attention Mechanism）</strong> 是神经网络中的一种关键技术，最早在自然语言处理领域的 Transformer 模型中提出，后来也被广泛应用于图像处理等领域。它通过计算输入序列中每个元素与其他元素之间的关系来提取全局信息。自注意力机制能够灵活地捕捉输入数据的全局依赖关系，这与传统的局部特征提取（如卷积操作）不同，因而在处理复杂数据时具有强大的建模能力</p>
</blockquote>
]]></content>
      <categories>
        <category>基于深度学习的图像处理</category>
      </categories>
      <tags>
        <tag>图像质量评价</tag>
        <tag>图像目标检测</tag>
        <tag>CNN</tag>
        <tag>YOLO</tag>
        <tag>GAN</tag>
        <tag>Transformer</tag>
      </tags>
  </entry>
</search>
